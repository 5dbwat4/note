---
title: "2. 定义非功能性需求 (Defining Nonfunctional Requirements)"
---
![](../map/ch01.png)
> *互联网做得如此出色，以至于大多数人将其视为一种自然资源，如同太平洋一般，而非人造之物。上一次有如此规模的技术做到如此零错误，是什么时候？*
>
> [艾伦·凯 (Alan Kay)](https://www.drdobbs.com/architecture-and-design/interview-with-alan-kay/240003442)，
> 接受 *Dr Dobb's Journal* 采访 (2012)

如果你正在构建一个应用程序，你将受到一系列需求 (requirements) 的驱动。在你的需求清单顶部，最可能的便是应用程序必须提供的功能 (functionality)：你需要哪些界面和按钮，以及每个操作应该做什么才能实现你软件的目的。这些就是你的*功能性需求 (functional requirements)*。

此外，你可能还有一些*非功能性需求 (nonfunctional requirements)*：例如，应用程序应该快速、可靠、安全、符合法律要求，并且易于维护。这些需求可能没有被明确写下来，因为它们可能看起来有些显而易见，但它们与应用程序的功能同等重要：一个慢得令人难以忍受或不可靠的应用程序，还不如不存在。

许多非功能性需求，例如安全性 (security)，超出了本书的范围。但有一些非功能性需求我们会加以考虑，本章将帮助你为自己的系统阐明这些需求：

* 如何定义和衡量系统的*性能 (performance)*（参见["描述性能"](./ch2#sec_introduction_percentiles)）；
* 服务*可靠性 (reliability)* 的含义——即在出现问题时仍能继续正确工作（参见["可靠性与容错"](./ch2#sec_introduction_reliability)）；
* 通过提供在系统负载增长时高效增加计算能力的方式，使系统具备*可扩展性 (scalability)*（参见["可扩展性"](./ch2#sec_introduction_scalability)）；以及
* 使系统在长期内更易于*维护 (maintainability)*（参见["可维护性"](./ch2#sec_introduction_maintainability)）。

本章引入的术语在后续章节中也会很有用，当我们深入探讨数据密集型系统 (data-intensive systems) 的实现细节时。然而，抽象的定义可能相当枯燥；为了让这些概念更具体，我们将以一个社交网络服务如何运作的案例研究开始本章，这将为性能和可扩展性提供实际示例。

## 案例研究：社交网络主页时间线 (Case Study: Social Network Home Timelines) 

假设你接到的任务是实现一个类似 X（前身为 Twitter）风格的社交网络，用户可以在其中发布消息并关注其他用户。这将是对此类服务实际运作方式的极大简化 [^1] [^2] [^3]，但它有助于说明大规模系统中出现的一些问题。

让我们假设用户每天发布 5 亿条帖子，即平均每秒 5,700 条帖子。偶尔，速率可能飙升至高达 150,000 条帖子/秒 [^4]。

我们还假设平均每位用户关注 200 人，同时也有 200 位关注者（尽管存在非常广泛的差异：大多数人只有少数关注者，而少数名人如巴拉克·奥巴马 (Barack Obama) 则有超过 1 亿关注者）。

### 表示用户、帖子和关注关系 (Representing Users, Posts, and Follows) 

假设我们将所有数据保存在如 [图 2-1](./ch2#fig_twitter_relational) 所示的关系型数据库 (relational database) 中。我们有一个用户表、一个帖子表，以及一个关注关系表。

<figure id="fig_twitter_relational">

![](../fig/ddia_0201.png)

图 2-1。一个简单的关系型模式 (relational schema)，用于用户可相互关注的社交网络。

</figure>

假设我们的社交网络必须支持的主要读取操作是*主页时间线 (home timeline)*，它显示你关注的用户最近发布的帖子（为简化起见，我们将忽略广告、来自非关注用户的推荐帖子以及其他扩展功能）。我们可以编写以下 SQL 查询来获取特定用户的主页时间线：

```sql
SELECT posts.*, users.* FROM posts
JOIN follows ON posts.sender_id = follows.followee_id
JOIN users ON posts.sender_id = users.id
WHERE follows.follower_id = current_user
ORDER BY posts.timestamp DESC
LIMIT 1000
```

要执行此查询，数据库将使用 `follows` 表查找 `current_user` 关注的所有用户，查找这些用户的最近帖子，并按时间戳排序以获取被关注用户中最近的 1,000 条帖子。

帖子应该是及时的，因此让我们假设在某人发布帖子后，我们希望其关注者能在 5 秒内看到它。实现这一点的一种方法是让用户的客户端每 5 秒重复执行上述查询（这称为*轮询 (polling)*）。如果我们假设同时有 1,000 万用户在线并登录，那就意味着每秒需要执行该查询 200 万次。即使你增加轮询间隔，这也是一个很大的数字。

此外，上述查询相当昂贵：如果你关注了 200 人，它需要获取这 200 人各自的最近帖子列表，并合并这些列表。那么每秒 200 万次时间线查询就意味着数据库每秒需要查找某个发送者的最近帖子 4 亿次——这是一个巨大的数字。而这还只是平均情况。有些用户关注数万个账号；对他们而言，此查询执行起来非常昂贵，且难以使其快速响应。

### 物化与更新时间线 (Materializing and Updating Timelines) 

我们如何做得更好？首先，与其轮询，不如让服务器主动向当前在线的任何关注者推送新帖子。其次，我们应该预计算上述查询的结果，以便用户请求其主页时间线时可以从缓存中提供服务。

假设我们为每个用户存储一个包含其主页时间线的数据结构，即他们关注的用户最近发布的帖子。每当用户发布帖子时，我们查找其所有关注者，并将该帖子插入到每个关注者的主页时间线中——就像将消息投递到邮箱一样。现在，当用户登录时，我们可以简单地提供这个我们预计算好的主页时间线。

此外，为了接收其时间线上新帖子的通知，用户的客户端只需订阅添加到其主页时间线的帖子流即可。

这种方法的缺点在于，现在每次用户发布帖子时，我们需要做更多的工作，因为主页时间线是派生数据 (derived data)，需要更新。该过程如 [图 2-2](./ch2#fig_twitter_timelines) 所示。当一个初始请求导致执行多个下游请求时，我们使用术语*扇出 (fan-out)* 来描述请求数量增加的倍数。

<figure id="fig_twitter_timelines">

![](../fig/ddia_0202.png)

图 2-2。扇出 (Fan-out)：将新帖子递送给发布该帖子的用户的每一位关注者。
</figure>

以每秒 5,700 条帖子的发布速率，如果平均每条帖子触达 200 位关注者（即扇出因子为 200），我们将需要每秒执行略多于 100 万次主页时间线写入。这很多，但与否则我们必须执行的每秒 4 亿次发帖者查找相比，仍然是显著的节省。

如果由于某些特殊事件导致帖子发布速率飙升，我们不必立即执行时间线递送——我们可以将它们入队 (enqueue)，并接受帖子暂时需要稍长时间才能出现在关注者的时间线中。即使在此类负载峰值期间，时间线加载仍然很快，因为我们只是从缓存中提供它们。

这种预计算并更新查询结果的过程称为*物化 (materialization)*，而时间线缓存是*物化视图 (materialized view)* 的一个示例（我们将在 [即将链接] 中进一步讨论这一概念）。物化视图加速了读取操作，但作为回报，我们必须在写入时做更多工作。对于大多数用户而言，写入成本是适度的，但社交网络还必须考虑一些极端情况：

* 如果某用户关注了非常大量的账号，且这些账号发帖频繁，那么该用户对其物化时间线的写入速率会很高。然而，在这种情况下，该用户实际上不太可能阅读其时间线中的所有帖子，因此简单地丢弃其部分时间线写入并向用户仅展示他们所关注账号帖子的样本是可以接受的 [^5]。
* 当拥有大量关注者的名人账号发布帖子时，我们需要做大量工作才能将该帖子插入到其数百万关注者的主页时间线中。在这种情况下，丢弃部分写入是不可接受的。解决此问题的一种方法是将名人帖子与其他人的帖子分开处理：我们可以省去将其添加到数百万时间线的工作，而是将名人帖子单独存储，并在读取时将其与物化时间线合并。尽管有此类优化，在社交网络上处理名人仍然可能需要大量基础设施 [^6]。

## 描述性能 (Describing Performance) 

大多数关于软件性能的讨论会考虑两种主要类型的指标：

响应时间 (Response time)
: 从用户发出请求到收到所请求答案所经过的时间。测量单位是秒（或毫秒，或微秒）。

吞吐量 (Throughput)
: 系统每秒处理的请求数量，或每秒处理的数据量。
对于给定的硬件资源分配，存在一个可处理的*最大吞吐量 (maximum throughput)*。测量单位是"某事物/秒"。

在社交网络案例研究中，"每秒帖子数"和"每秒时间线写入数"是吞吐量指标，而"加载主页时间线所需时间"或"帖子递送到关注者所需时间"是响应时间指标。

吞吐量与响应时间之间通常存在关联；[图 2-3](./ch2#fig_throughput) 概述了在线服务中此类关系的一个示例。当请求吞吐量较低时，服务的响应时间较低，但随着负载增加，响应时间也会增加。这是因为*排队 (queueing)*：当请求到达一个高负载系统时，CPU 很可能已经在处理之前的请求，因此传入的请求需要等待之前的请求完成才能被处理。随着吞吐量接近硬件所能处理的最大值，排队延迟会急剧增加。

<figure id="fig_throughput">

![](../fig/ddia_0203.png)

图 2-3。当服务的吞吐量接近其容量时，由于排队，响应时间会急剧增加。

</figure>

--------
> [!TIP] 当过载系统无法恢复时 (WHEN AN OVERLOADED SYSTEM WON'T RECOVER)
> 如果系统接近过载，吞吐量被推至极限附近，它有时会进入一个恶性循环，效率降低从而变得更加过载。例如，如果有一长串请求等待处理，响应时间可能会增加太多，以至于客户端超时并重新发送其请求。这会导致请求速率进一步增加，使问题恶化——形成*重试风暴 (retry storm)*。即使负载再次降低，此类系统也可能保持在过载状态，直到被重启或以其他方式重置。这种现象称为*亚稳态故障 (metastable failure)*，它可能导致生产系统出现严重中断 [^7] [^8]。
>
> 为避免重试使服务过载，你可以在客户端增加并随机化连续重试之间的时间间隔（*指数退避 (exponential backoff)* [^9] [^10]），并暂时停止向最近返回错误或超时的服务发送请求（使用*断路器 (circuit breaker)* [^11] [^12] 或*令牌桶 (token bucket)* 算法 [^13]）。
>
> 服务器也可以检测到自己是否接近过载，并开始主动拒绝请求（*负载削减 (load shedding)* [^14]），并返回响应要求客户端减慢速度（*背压 (backpressure)* [^1] [^15]）。
>
> 排队和负载均衡 (load-balancing) 算法的选择也可能产生影响 [^16]。
--------

就性能指标而言，响应时间通常是用户最关心的，而吞吐量则决定了所需的计算资源（例如，你需要多少台服务器），从而决定了服务特定工作负载的成本。如果吞吐量可能增加到超出当前硬件所能处理的范围，则需要扩展容量；如果通过增加计算资源可以显著提高其最大吞吐量，则称系统是*可扩展的 (scalable)*。

在本节中，我们将主要关注响应时间，我们将在 ["可扩展性"](./ch2#sec_introduction_scalability) 中再回到吞吐量和可扩展性。

### 延迟与响应时间 (Latency and Response Time) 

"延迟 (Latency)"和"响应时间 (response time)"有时可以互换使用，但在本书中，我们将以特定方式使用这些术语（如 [图 2-4](./ch2#fig_response_time) 所示）：

* *响应时间 (response time)* 是客户端所见的；它包括系统任何地方产生的所有延迟。
* *服务时间 (service time)* 是服务 actively 处理用户请求的持续时间。
* *排队延迟 (Queueing delays)* 可能出现在流程中的多个点：例如，请求被接收后，可能需要等待 CPU 可用才能被处理；如果同一机器上的其他任务正在通过出站网络接口发送大量数据，响应数据包在通过网络发送之前可能需要缓冲。
* *延迟 (Latency)* 是一个包罗万象的术语，指请求未被 actively 处理的时间，即请求处于*潜伏 (latent)* 状态的时间。特别是，*网络延迟 (network latency)* 或*网络延迟 (network delay)* 指请求和响应在网络中传输所花费的时间。

<figure id="fig_response_time">

![](../fig/ddia_0204.png)

图 2-4。响应时间、服务时间、网络延迟和排队延迟。

</figure>

在 [图 2-4](./ch2#fig_response_time) 中，时间从左向右流动，每个通信节点显示为一条水平线，请求或响应消息显示为从一个节点到另一个节点的粗对角箭头。在本书的后续内容中，你将经常遇到这种风格的图表。

即使你反复发出相同的请求，响应时间也可能从一个请求到下一个请求有显著差异。许多因素可能增加随机延迟：例如，切换到后台进程的上下文切换 (context switch)、网络数据包丢失和 TCP 重传、垃圾回收暂停 (garbage collection pause)、页错误 (page fault) 强制从磁盘读取、服务器机架中的机械振动 [^17]，或其他许多原因。我们将在 ["超时与无界延迟"](./ch9#sec_distributed_queueing) 中更详细地讨论此主题。

排队延迟通常占响应时间可变性的很大一部分。由于服务器只能并行处理少量事务（例如，受其 CPU 核心数限制），只需少量慢请求就能阻碍后续请求的处理——这种效应称为*队头阻塞 (head-of-line blocking)*。即使这些后续请求的服务时间很快，由于等待先前请求完成的时间，客户端仍会看到较慢的整体响应时间。排队延迟不是服务时间的一部分，因此出于这个原因，在客户端测量响应时间非常重要。

### 平均值、中位数和百分位数 (Average, Median, and Percentiles) 

由于响应时间从一个请求到另一个请求会有所不同，我们需要将其视为一个可测量的*分布 (distribution)* 值，而非单一数字。在 [图 2-5](./ch2#fig_lognormal) 中，每个灰色条代表对服务的一次请求，其高度显示该请求花费了多长时间。大多数请求相当快，但偶尔会有*离群值 (outliers)* 花费的时间长得多。

网络延迟的变化也称为*抖动 (jitter)*。

<figure id="fig_lognormal">

![](../fig/ddia_0205.png)

图 2-5。说明平均值和百分位数：对服务 100 个请求样本的响应时间。

</figure>

报告服务的*平均 (average)* 响应时间很常见（技术上讲，是*算术平均值 (arithmetic mean)*：即对所有响应时间求和，然后除以请求数量）。平均响应时间对于估算吞吐量限制很有用 [^18]。

然而，如果你想了解你的"典型"响应时间，平均值并不是一个很好的指标，因为它并不能告诉你有多少用户实际经历了该延迟。

通常最好使用*百分位数 (percentiles)*。如果你将响应时间列表从最快到最慢排序，那么*中位数 (median)* 就是中间点：例如，如果你的中位数响应时间是 200 毫秒，那意味着你一半的请求在 200 毫秒内返回，另一半请求花费的时间更长。这使得中位数成为了解用户通常必须等待多长时间的很好指标。中位数也称为*第 50 百分位数 (50th percentile)*，有时缩写为*p50*。

为了了解你的离群值有多糟糕，你可以查看更高的百分位数：*第 95、99 和 99.9 百分位数 (95th, 99th, and 99.9th percentiles)* 很常见（缩写为*p95*、*p99* 和*p999*）。它们是响应时间阈值，95%、99% 或 99.9% 的请求比该特定阈值更快。例如，如果第 95 百分位数响应时间是 1.5 秒，那意味着 100 个请求中有 95 个花费少于 1.5 秒，100 个请求中有 5 个花费 1.5 秒或更长时间。这在 [图 2-5](./ch2#fig_lognormal) 中有所说明。

响应时间的高百分位数，也称为*尾部延迟 (tail latencies)*，很重要，因为它们直接影响用户对服务的体验。例如，亚马逊 (Amazon) 对内部服务的响应时间要求是用第 99.9 百分位数来描述的，尽管这只影响 1/1000 的请求。这是因为请求最慢的客户往往是那些在其账户中拥有最多数据的客户，因为他们进行了许多购买——也就是说，他们是最有价值的客户 [^19]。

确保网站对这些客户快速运行，以保持他们的满意度很重要。

另一方面，优化第 99.99 百分位数（最慢的 1/10000 请求）被认为过于昂贵，且对亚马逊的目的而言收益不足。在非常高的百分位数上减少响应时间很困难，因为它们很容易受到你控制范围之外的随机事件的影响，且收益递减。

--------
> [!TIP] 响应时间对用户的影响 (THE USER IMPACT OF RESPONSE TIMES)
> 快速的服务比慢速的服务对用户更好，这似乎是直观上显而易见的 [^20]。然而，令人惊讶的是，很难获得可靠的数据来量化延迟对用户行为的影响。
>
> 一些经常被引用的统计数据并不可靠。2006 年，谷歌 (Google) 报告称，搜索结果从 400 毫秒减慢到 900 毫秒与流量和收入下降 20% 相关 [^21]。然而，谷歌 2009 年的另一项研究报告称，延迟增加 400 毫秒仅导致每天搜索次数减少 0.6% [^22]，同年必应 (Bing) 发现加载时间增加两秒使广告收入减少了 4.3% [^23]。这些公司的更新数据似乎未公开提供。
>
> 一项较新的 Akamai 研究 [^24] 声称，响应时间增加 100 毫秒会使电子商务网站的转化率降低高达 7%；然而，仔细检查后发现，同一研究揭示，*非常快* 的页面加载时间也与较低的转化率相关！这个看似矛盾的结果可以解释为，加载最快的页面往往是那些没有有用内容的页面（例如，404 错误页面）。然而，由于该研究没有努力将页面内容的影响与加载时间的影响分开，其结果可能没有意义。
>
> 雅虎 (Yahoo) 的一项研究 [^25] 比较了快速加载与慢速加载搜索结果的点击率，控制了搜索结果的质量。研究发现，当快速与慢速响应之间的差异为 1.25 秒或更多时，快速搜索的点击率高出 20–30%。
--------

### 响应时间指标的使用 (Use of Response Time Metrics) 

高百分位数在作为服务单个最终用户请求一部分而被多次调用的后端服务中尤其重要。即使你并行发出调用，最终用户请求仍然需要等待最慢的并行调用完成。只需一个慢调用就能使整个最终用户请求变慢，如 [图 2-6](./ch2#fig_tail_amplification) 所示。即使只有一小部分后端调用较慢，如果最终用户请求需要多个后端调用，获得慢调用的几率也会增加，因此更高比例的最终用户请求最终会变慢（这种效应称为*尾部延迟放大 (tail latency amplification)* [^26]）。

<figure id="fig_tail_amplification">

![](../fig/ddia_0206.png)

图 2-6。当需要多个后端调用来服务一个请求时，只需一个慢的后端请求就能拖慢整个最终用户请求。

</figure>

百分位数通常用于*服务级别目标 (service level objectives, SLOs)* 和*服务级别协议 (service level agreements, SLAs)* 中，作为定义服务预期性能和可用性的方式 [^27]。例如，SLO 可能设定一个目标，要求服务的中位数响应时间小于 200 毫秒，第 99 百分位数低于 1 秒，并且至少 99.9% 的有效请求产生非错误响应。SLA 是一份合同，规定了如果 SLO 未满足会发生什么（例如，客户可能有权获得退款）。这至少是基本思路；在实践中，为 SLO 和 SLA 定义良好的可用性指标并非易事 [^28] [^29]。

--------
> [!TIP] 计算百分位数 (COMPUTING PERCENTILES)
> 如果你想为服务的监控仪表板添加响应时间百分位数，你需要持续高效地计算它们。例如，你可能希望保留过去 10 分钟内请求响应时间的滚动窗口 (rolling window)。每分钟，你计算该窗口内值的中位数和各种百分位数，并将这些指标绘制在图表上。
>
> 最简单的实现是保留时间窗口内所有请求的响应时间列表，并每分钟对该列表排序。如果这对你来说效率太低，有一些算法可以以最小的 CPU 和内存成本计算百分位数的良好近似值。
>
> 开源百分位数估算库包括 HdrHistogram、t-digest [^30] [^31]、OpenHistogram [^32] 和 DDSketch [^33]。
>
> 注意，对百分位数取平均值，例如，为了降低时间分辨率或合并来自多台机器的数据，在数学上是毫无意义的——聚合响应时间数据的正确方法是添加直方图 (histograms) [^34]。
--------

## 可靠性与容错 (Reliability and Fault Tolerance) 

每个人对某事物可靠或不可靠的含义都有一个直观的概念。对于软件，典型的期望包括：

* 应用程序执行用户预期的功能。
* 它能够容忍用户犯错误或以意外方式使用软件。
* 在预期的负载和数据量下，其性能足以满足所需用例。
* 系统防止任何未经授权的访问和滥用。

如果所有这些加在一起意味着"正确工作"，那么我们可以将*可靠性 (reliability)* 大致理解为"即使在出现问题时也能继续正确工作"。为了更精确地描述"出现问题"，我们将区分*故障 (faults)* 和*失效 (failures)* [^35] [^36] [^37]：

故障 (Fault)
: 故障是指系统的某个*部分* 停止正常工作：例如，如果单个硬盘驱动器发生故障，或单台机器崩溃，或系统所依赖的外部服务发生中断。

失效 (Failure)
: 失效是指系统*整体* 停止向用户提供所需服务；换句话说，当它未能满足服务级别目标 (SLO) 时。

故障与失效之间的区别可能令人困惑，因为它们是同一件事，只是处于不同层面。例如，如果硬盘驱动器停止工作，我们说硬盘驱动器已失效：如果系统仅由该单个硬盘驱动器组成，它已停止提供所需服务。然而，如果你正在谈论的系统包含许多硬盘驱动器，那么单个硬盘驱动器的失效从更大系统的角度来看只是一个故障，而更大的系统可能能够通过在其他硬盘驱动器上保留数据副本来容忍该故障。

### 容错 (Fault Tolerance) 

如果一个系统在尽管某些故障发生的情况下仍能继续向用户提供所需服务，我们称该系统为*容错的 (fault-tolerant)*。如果一个系统无法容忍某个部分发生故障，我们称该部分为*单点故障 (single point of failure, SPOF)*，因为该部分的故障会升级并导致整个系统失效。

例如，在社交网络案例研究中，可能发生的故障是在扇出过程中，参与更新物化时间线的机器崩溃或变得不可用。为了使此过程具有容错性，我们需要确保另一台机器可以在不遗漏任何应递送的帖子且不重复任何帖子的情况下接管此任务。（这个想法称为*精确一次语义 (exactly-once semantics)*，我们将在 [即将链接] 中详细研究它。）

容错性总是局限于特定类型的特定数量的故障。例如，系统可能能够容忍最多两个硬盘驱动器同时发生故障，或最多三个节点中的一个崩溃。容忍任意数量的故障是没有意义的：如果所有节点都崩溃，那就无能为力了。如果整个地球（及其上的所有服务器）被黑洞吞噬，要容忍这种故障就需要在太空中托管网络服务——祝你好运，看看能否批准那个预算项目。

反直觉的是，在此类容错系统中，通过故意触发故障来*增加* 故障率可能是有意义的——例如，通过随机杀死单个进程而不发出警告。这称为*故障注入 (fault injection)*。许多关键缺陷实际上是由于错误处理不当造成的 [^38]；通过故意诱发故障，你确保容错机制不断得到练习和测试，这可以增加你对故障自然发生时能被正确处理的信心。*混沌工程 (Chaos engineering)* 是一门旨在通过故意注入故障等实验来提高对容错机制信心的学科 [^39]。

尽管我们通常倾向于容忍故障而非预防故障，但在某些情况下，预防胜于治疗（例如，因为不存在治疗方法）。安全事务就是这种情况：如果攻击者已入侵系统并获得了敏感数据的访问权限，该事件无法撤销。然而，本书主要处理如下节所述的那些可以治愈的故障类型。

### 硬件与软件故障 (Hardware and Software Faults) 

当我们想到系统失效的原因时，硬件故障会迅速浮现在脑海中：

* 大约每年有 2–5% 的机械硬盘 (magnetic hard drives) 会发生故障 [^40] [^41]；在拥有 10,000 块磁盘的存储集群中，因此我们应预期平均每天发生一次磁盘故障。
  最新数据表明磁盘正变得更可靠，但故障率仍然显著 [^42]。
* 大约每年有 0.5–1% 的固态硬盘 (solid state drives, SSDs) 会发生故障 [^43]。少量位错误会自动纠正 [^44]，但不可纠正的错误大约每年每驱动器发生一次，即使是相当新的驱动器（即磨损很少）；此错误率高于机械硬盘 [^45], [^46]。
* 其他硬件组件，如电源、RAID 控制器和内存模块也会发生故障，尽管频率低于硬盘驱动器 [^47] [^48]。
* 大约每 1,000 台机器中有一台的 CPU 核心偶尔会计算出错误结果，可能是由于制造缺陷 [^49] [^50] [^51]。在某些情况下，错误计算会导致崩溃，但在其他情况下，它会导致程序简单地返回错误结果。
* RAM 中的数据也可能损坏，要么是由于宇宙射线等随机事件，要么是由于永久性物理缺陷。即使使用带纠错码 (error-correcting codes, ECC) 的内存，每年仍有超过 1% 的机器会遇到不可纠正的错误，这通常会导致机器崩溃，并且需要更换受影响的内存模块 [^52]。
  此外，某些病态的内存访问模式可以高概率翻转比特 [^53]。
* 整个数据中心可能变得不可用（例如，由于停电或网络配置错误），甚至被永久摧毁（例如，由火灾、洪水或地震 [^54] 造成）。
  太阳风暴（当太阳喷射出大量带电粒子时，会在长距离电线中感应出大电流）可能损坏电网和海底网络电缆 [^55]。
  尽管此类大规模故障很罕见，但如果服务无法容忍数据中心的损失，其影响可能是灾难性的 [^56]。

这些事件足够罕见，以至于在处理小型系统时，只要你能够轻松更换发生故障的硬件，通常无需担心它们。然而，在大规模系统中，硬件故障发生得足够频繁，以至于它们成为正常系统操作的一部分。

#### 通过冗余容忍硬件故障 (Tolerating hardware faults through redundancy) 

我们对不可靠硬件的首要反应通常是向单个硬件组件添加冗余，以降低系统的故障率。磁盘可以配置为 RAID（将数据分散到同一台机器中的多个磁盘上，以便故障磁盘不会导致数据丢失），服务器可以配备双电源和热插拔 CPU，数据中心可以为备用电源配备电池和柴油发电机。此类冗余通常可以使机器 uninterrupted 运行数年。

当组件故障是独立的，即一个故障的发生不会改变另一个故障发生的可能性时，冗余最为有效。然而，经验表明，组件故障之间通常存在显著的相关性 [^41] [^57] [^58]；整个服务器机架或整个数据中心的不可用性仍然比我们希望的更频繁发生。

硬件冗余增加了单台机器的正常运行时间；然而，如 ["分布式与单节点系统"](./ch1#sec_introduction_distributed) 中所述，使用分布式系统具有优势，例如能够容忍一个数据中心的完全中断。

因此，云系统往往较少关注单台机器的可靠性，而是旨在通过在软件层面容忍故障节点来使服务高度可用。云提供商使用*可用区 (availability zones)* 来标识哪些资源在物理上共置；同一地点的资源比地理上分离的资源更可能同时发生故障。

本书中讨论的容错技术旨在容忍整台机器、机架或可用区的损失。它们通常通过允许一个数据中心中的机器在另一个数据中心中的机器发生故障或无法访问时接管来工作。我们将在 [第 6 章](./ch6#ch_replication)、[第 10 章](./ch10#ch_consistency) 以及本书的其他各处讨论此类容错技术。

能够容忍整台机器损失的系统也具有运维优势：单服务器系统如果需要重启机器（例如，应用操作系统安全补丁）则需要计划停机时间，而多节点容错系统可以通过一次重启一个节点来打补丁，而不会影响用户的服务。这称为*滚动升级 (rolling upgrade)*，我们将在 [第 5 章](./ch5#ch_encoding) 中进一步讨论。

#### 软件故障 (Software faults) 

尽管硬件故障可能弱相关，但它们仍然是 mostly 独立的：例如，如果一个磁盘发生故障，同一台机器中的其他磁盘很可能在另一段时间内没事。另一方面，软件故障通常高度相关，因为许多节点运行相同的软件并因此具有相同的缺陷是很常见的 [^59] [^60]。

此类故障更难预测，并且往往比不相关的硬件故障导致更多的系统失效 [^47]。例如：

* 一个软件缺陷导致所有节点在特定情况下同时发生故障。例如，2012 年 6 月 30 日，一个闰秒 (leap second) 由于 Linux 内核中的一个缺陷导致许多 Java 应用程序同时挂起，使许多互联网服务瘫痪 [^61]。
  由于固件缺陷，某些型号的所有 SSD 在精确运行 32,768 小时（不到 4 年）后突然故障，使其上的数据无法恢复 [^62]。
* 一个失控的进程消耗了某些共享的、有限的资源，如 CPU 时间、内存、磁盘空间、网络带宽或线程 [^63]。例如，处理大请求时消耗过多内存的进程可能被操作系统杀死。客户端库中的缺陷可能导致请求量远高于预期 [^64]。
* 系统所依赖的服务变慢、变得无响应，或开始返回损坏的响应。
* 不同系统之间的交互产生了在单独测试每个系统时不会出现的涌现行为 (emergent behavior) [^65]。
* 级联故障 (Cascading failures)，其中一个组件的问题导致另一个组件过载并变慢，进而导致另一个组件崩溃 [^66] [^67]。

导致此类软件故障的缺陷通常潜伏很长时间，直到被一组不寻常的情况触发。在这些情况下，揭示出软件对其环境做出了某种假设——虽然该假设通常成立，但最终由于某种原因不再成立 [^68] [^69]。

对于软件中的系统性故障问题，没有快速的解决方案。许多小事可能有所帮助：仔细思考系统中的假设和交互；彻底测试；进程隔离；允许进程崩溃和重启；避免反馈循环，如重试风暴（参见 ["当过载系统无法恢复时"](./ch2#sidebar_metastable)）；在生产环境中测量、监控和分析系统行为。

### 人类与可靠性 (Humans and Reliability) 

人类设计和构建软件系统，保持系统运行的运维人员也是人类。与机器不同，人类不只是遵循规则；他们的优势在于以创造性和适应性来完成工作。然而，这一特性也导致不可预测性，有时尽管出于好意，也会导致失效的错误。例如，一项对大型互联网服务的研究发现，运维人员的配置变更是中断的主要原因，而硬件故障（服务器或网络）仅在 10–25% 的中断中起作用 [^70]。

人们很容易将此类问题标记为"人为错误 (human error)"，并希望可以通过更严格的程序和遵守规则来更好地控制人类行为以解决这些问题。然而，指责人们犯错是适得其反的。我们所谓的"人为错误"实际上并非事件的原因，而是人们尽力完成工作的社会技术系统 (sociotechnical system) 中存在问题的症状 [^71]。

通常，复杂系统具有涌现行为，组件之间意外的交互也可能导致失效 [^72]。

各种技术措施可以帮助最大限度地减少人为错误的影响，包括彻底测试（手工编写的测试和对大量随机输入的*属性测试 (property testing)*）[38]、用于快速还原配置变更的回滚机制、新代码的渐进式推出、详细清晰的监控、用于诊断生产问题的可观测性工具（参见 ["分布式系统的问题"](./ch1#sec_introduction_dist_sys_problems)），以及鼓励"做正确的事"并阻止"做错误的事"的设计良好的接口。

然而，这些都需要投入时间和金钱，在日常业务的务实现实中，组织通常优先考虑产生收入的活动，而非增加其对错误韧性的措施。如果在更多功能和更多测试之间做出选择，许多组织 understandably 选择功能。鉴于这种选择，当可预防的错误不可避免地发生时，指责犯错的人是没有意义的——问题在于组织的优先事项。

越来越多的组织正在采用*无责备事后分析 (blameless postmortems)* 的文化：在事件发生后，鼓励相关人员分享关于发生了什么的全部细节，无需担心惩罚，因为这使组织中的其他人能够学习如何防止未来出现类似问题 [^73]。此过程可能会揭示出需要改变业务优先事项、需要投资于被忽视的领域、需要改变相关人员的激励措施，或其他需要引起管理层注意的系统性问题。

作为一般原则，在调查事件时，你应该对简单的答案保持怀疑。"鲍勃在部署该变更时应该更小心"并无成效，但"我们必须用 Haskell 重写后端"也同样如此。相反，管理层应借此机会从每天与之打交道的人的角度了解社会技术系统如何运作的细节，并基于此反馈采取措施改进它 [^71]。

--------
> [!TIP] 可靠性有多重要？(HOW IMPORTANT IS RELIABILITY?)
> 可靠性不仅适用于核电站和空中交通管制——更普通的应用程序也被期望可靠地工作。业务应用程序中的缺陷会导致生产力损失（如果数据报告错误，还存在法律风险），电子商务网站的中断可能在收入损失和声誉损害方面造成巨大成本。
>
> 在许多应用程序中，几分钟甚至几小时的临时中断是可以容忍的 [^74]，但永久性的数据丢失或损坏将是灾难性的。考虑一位将所有孩子照片和视频存储在你的照片应用程序中的家长 [^75]。如果该数据库突然损坏，他们会有什么感受？他们知道如何从备份中恢复吗？
>
> 作为不可靠软件可能伤害人们的另一个例子，考虑邮局地平线 (Post Office Horizon) 丑闻。在 1999 年至 2019 年间，英国数百名管理邮局分支机构的人因盗窃或欺诈被定罪，因为会计软件显示其账户存在短缺。最终，很明显其中许多短缺是由于软件缺陷造成的，许多定罪此后已被推翻 [^76]。
>
> 导致这一事件的，可能是英国历史上最大的司法误判，是这样一个事实：英国法律假定计算机正确运行（因此，计算机产生的证据是可靠的），除非有相反的证据 [^77]。
>
> 软件工程师可能会嘲笑软件可能永远无缺陷的想法，但这对于因不可靠计算机系统而被错误监禁、宣布破产，甚至因错误定罪而自杀的人们来说，几乎没有安慰。
>
> 在某些情况下，我们可能选择牺牲可靠性以降低开发成本（例如，在为未经证实的市场开发原型产品时）——但我们应该非常清楚地意识到何时在走捷径，并牢记潜在后果。
--------

## 可扩展性 (Scalability) 

即使一个系统今天可靠地工作，这并不意味着它将来也一定会可靠地工作。退化的一个常见原因是负载增加：也许系统已从 10,000 个并发用户增长到 100,000 个并发用户，或从 100 万增长到 1000 万。也许它正在处理比以前大得多的数据量。

*可扩展性 (Scalability)* 是我们用来描述系统应对负载增加能力的术语。有时，在讨论可扩展性时，人们会发表类似"你不是谷歌或亚马逊。别担心规模，只用关系型数据库吧"的评论。这条格言是否适用于你，取决于你正在构建的应用程序类型。

如果你正在构建一个目前只有少量用户的新产品，也许是在初创公司，压倒性的工程目标通常是尽可能保持系统简单和灵活，以便在你更多地了解客户需求时，可以轻松修改和调整产品功能 [^78]。

在这种环境中，担心未来可能需要的假设规模是适得其反的：在最好的情况下，对可扩展性的投资是浪费精力和过早优化；在最坏的情况下，它们会将你锁定在不灵活的设计中，并使演化你的应用程序变得更加困难。

原因是可扩展性不是一个一维的标签：说"X 是可扩展的"或"Y 不可扩展"是没有意义的。相反，讨论可扩展性意味着考虑如下问题：

* "如果系统以特定方式增长，我们有哪些应对增长的选项？"
* "我们如何增加计算资源来处理额外的负载？"
* "根据当前增长预测，我们何时会触及当前架构的限制？"

如果你成功地使你的应用程序流行起来，从而处理不断增长的负载，你将了解你的性能瓶颈在哪里，因此你将知道需要沿哪些维度进行扩展。在那时，是时候开始担心可扩展性技术了。

### 描述负载 (Describing Load) 

首先，我们需要简洁地描述系统当前的负载；只有这样，我们才能讨论增长问题（如果我们的负载翻倍会发生什么？）。这通常是一个吞吐量指标：例如，对服务的每秒请求数、每天到达的新数据千兆字节数，或每小时购物车结账次数。有时你关心的是某个可变数量的峰值，如 ["案例研究：社交网络主页时间线"](./ch2#sec_introduction_twitter) 中同时在线用户的数量。

通常，负载的其他统计特征也会影响访问模式，从而影响可扩展性要求。例如，你可能需要知道数据库中读取与写入的比例、缓存的命中率，或每个用户的数据项数量（例如，社交网络案例研究中的关注者数量）。也许平均情况对你很重要，也许你的瓶颈由少数极端情况主导。这完全取决于你特定应用程序的细节。

一旦你描述了系统的负载，你就可以调查负载增加时会发生什么。你可以从两个方面来看：

* 当你以某种方式增加负载并保持系统资源（CPU、内存、网络带宽等）不变时，系统的性能会受到什么影响？
* 当你以某种方式增加负载时，如果你想保持性能不变，需要增加多少资源？

通常，我们的目标是在保持系统性能在 SLA 要求范围内（参见 ["响应时间指标的使用"](./ch2#sec_introduction_slo_sla)）的同时，最小化运行系统的成本。所需的计算资源越多，成本就越高。可能某些类型的硬件比其他类型更具成本效益，并且这些因素可能会随着新型硬件的出现而随时间变化。

如果你可以加倍资源以处理两倍的负载，同时保持性能不变，我们说你具有*线性可扩展性 (linear scalability)*，这被认为是一件好事。偶尔，由于规模经济或更好的峰值负载分布，可以用少于双倍的资源处理两倍的负载 [^79] [^80]。

更可能的是，成本增长快于线性，并且可能有许多原因导致效率低下。例如，如果你有大量数据，那么处理单个写入请求可能涉及比数据量小时更多的工作，即使请求的大小相同。

### 共享内存、共享磁盘与无共享架构 (Shared-Memory, Shared-Disk, and Shared-Nothing Architecture) 

增加服务硬件资源的最简单方法是将其迁移到更强大的机器。单个 CPU 核心不再显著变快，但你可以购买（或租用云实例）具有更多 CPU 核心、更多 RAM 和更多磁盘空间的机器。这种方法称为*垂直扩展 (vertical scaling)* 或*向上扩展 (scaling up)*。

你可以通过使用多个进程或线程在单台机器上获得并行性。属于同一进程的所有线程都可以访问相同的 RAM，因此这种方法也称为*共享内存架构 (shared-memory architecture)*。共享内存方法的问题在于成本增长快于线性：具有两倍硬件资源的高端机器通常成本显著高于两倍。并且由于瓶颈，两倍大小的机器通常处理的负载少于两倍。

另一种方法是*共享磁盘架构 (shared-disk architecture)*，它使用多台具有独立 CPU 和 RAM 的机器，但将数据存储在机器之间共享的磁盘阵列上，这些机器通过快速网络连接：*网络附加存储 (Network-Attached Storage, NAS)* 或*存储区域网络 (Storage Area Network, SAN)*。这种架构传统上用于本地数据仓库工作负载，但争用和锁定开销限制了共享磁盘方法的可扩展性 [^81]。

相比之下，*无共享架构 (shared-nothing architecture)* [^82]（也称为*水平扩展 (horizontal scaling)* 或*向外扩展 (scaling out)*）获得了大量流行。在这种方法中，我们使用具有多个节点的分布式系统，每个节点都有自己的 CPU、RAM 和磁盘。节点之间的任何协调都在软件层面通过常规网络进行。

无共享的优点在于它具有线性扩展的潜力，可以使用任何提供最佳性价比的硬件（尤其是在云中），可以更轻松地随着负载增加或减少调整其硬件资源，并且可以通过跨多个数据中心和区域分布系统来实现更高的容错性。缺点是它需要显式分片 (sharding)（参见 [第 7 章](./ch7#ch_sharding)），并且会产生分布式系统的所有复杂性（[第 9 章](./ch9#ch_distributed)）。

一些云原生数据库系统使用单独的服务进行存储和事务执行（参见 ["存储与计算分离"](./ch1#sec_introduction_storage_compute)），多个计算节点共享访问同一存储服务。这种模型与共享磁盘架构有些相似，但它避免了旧系统的可扩展性问题：存储服务不是提供文件系统 (NAS) 或块设备 (SAN) 抽象，而是提供专为数据库特定需求设计的专用 API [^83]。

### 可扩展性原则 (Principles for Scalability) 

在大规模运行的系统架构通常高度特定于应用程序——不存在通用的、一刀切的可扩展架构（非正式地称为*魔法扩展秘方 (magic scaling sauce)*）。例如，设计用于处理每秒 100,000 个请求（每个 1 kB 大小）的系统，看起来与设计用于每分钟 3 个请求（每个 2 GB 大小）的系统非常不同——即使两个系统具有相同的数据吞吐量（100 MB/秒）。

此外，适用于某一负载水平的架构不太可能应对 10 倍于此的负载。如果你正在处理一个快速增长的服务，因此很可能需要在每个数量级的负载增长时重新思考你的架构。由于应用程序的需求可能会演变，通常不值得提前规划超过一个数量级的未来扩展需求。

可扩展性的一个良好通用原则是将系统分解为可以 largely 独立运行的较小组件。这是微服务（参见 ["微服务与无服务器"](./ch1#sec_introduction_microservices)）、分片（[第 7 章](./ch7#ch_sharding)）、流处理（[即将链接]）和无共享架构背后的基本原则。然而，挑战在于知道在哪里划清应该在一起的事物与应该分开的事物之间的界限。微服务的设计指南可以在其他书籍中找到 [^84]，我们将在 [第 7 章](./ch7#ch_sharding) 中讨论无共享系统的分片。

另一个良好原则是不要让事情变得比必要的更复杂。如果单台机器数据库就能完成任务，它可能比复杂的分布式设置更可取。自动扩展系统（根据需求自动添加或移除资源）很酷，但如果你的负载相当可预测，手动扩展的系统可能具有更少的运维意外（参见 ["运维：自动或手动再平衡"](./ch7#sec_sharding_operations)）。具有五个服务的系统比具有五十个服务的系统更简单。良好的架构通常涉及各种方法的务实混合。

## 可维护性 (Maintainability) 

软件不会磨损或遭受材料疲劳，因此它不会以与机械物体相同的方式损坏。但应用程序的需求经常变化，软件运行的环境会变化（例如其依赖项和底层平台），并且它有需要修复的缺陷。

人们普遍认为，软件的大部分成本不在于其初始开发，而在于其持续维护——修复缺陷、保持系统运行、调查故障、使其适应新平台、为新用例修改它、偿还技术债务以及添加新功能 [^85] [^86]。

然而，维护也很困难。如果一个系统已成功运行很长时间，它很可能使用今天没有多少工程师理解的过时技术（如大型机和 COBOL 代码）；关于系统为何以某种方式设计的制度性知识可能随着人员离开组织而丢失；可能必须修复其他人的错误。此外，计算机系统通常与其支持的人类组织交织在一起，这意味着此类*遗留 (legacy)* 系统的维护既是人的问题，也是技术问题 [^87]。

如果我们创建的每个系统如果有足够的价值能长期生存，总有一天都会成为遗留系统。为了最大限度地减少未来需要维护我们软件的那代人的痛苦，我们应该在设计时考虑维护问题。尽管我们不能总是预测哪些决定可能会在未来造成维护难题，但在本书中，我们将关注几个广泛适用的原则：

可操作性 (Operability)
: 使组织易于保持系统平稳运行。

简单性 (Simplicity)
: 通过使用易于理解的、一致的模式和结构实现系统，并避免不必要的复杂性，使新工程师易于理解系统。

可演化性 (Evolvability)
: 使工程师易于在未来对系统进行更改，随着需求变化，为其适应和扩展以应对未预期的用例。

### 可操作性：让运维工作更轻松 (Operability: Making Life Easy for Operations) 

我们之前在 ["云时代的运维"](./ch1#sec_introduction_operations) 中讨论了运维的角色，我们看到对于可靠运维而言，人类流程至少与软件工具同样重要。事实上，有人建议"良好的运维通常可以弥补不良（或不完整）软件的局限，但良好的软件无法在不良的运维下可靠运行" [^60]。

在由成千上万台机器组成的大规模系统中，手动维护会不合理地昂贵，自动化至关重要。然而，自动化可能是一把双刃剑：总会有边缘情况（如罕见的故障场景）需要运维团队手动干预。由于无法自动处理的情况是最复杂的问题，更高的自动化需要*更* 熟练的运维团队来解决这些问题 [^88]。

此外，如果自动化系统出错，通常比依赖操作员手动执行某些操作的系统更难排查。因此，并非自动化越多对可操作性就越好。然而，一定程度的自动化很重要，最佳点将取决于你特定应用程序和组织的具体情况。

良好的可操作性意味着使日常任务变得容易，让运维团队能够将其精力集中在高价值活动上。数据系统可以做各种事情使日常任务变得容易，包括 [^89]：

* 允许监控工具检查系统的关键指标，并支持可观测性工具（参见 ["分布式系统的问题"](./ch1#sec_introduction_dist_sys_problems)）以深入了解系统的运行时行为。
  各种商业和开源工具可以在此提供帮助 [^90]。
* 避免依赖单个机器（允许机器在维护期间下线，而系统整体继续 uninterrupted 运行）
* 提供良好的文档和易于理解的运维模型（"如果我做 X，Y 就会发生"）
* 提供良好的默认行为，但也允许管理员在需要时覆盖默认值
* 在适当时自愈，但也允许管理员在需要时手动控制系统状态
* 表现出可预测的行为，最大限度地减少意外

### 简单性：管理复杂性 (Simplicity: Managing Complexity) 

小型软件项目可以拥有令人愉悦的简单而富有表现力的代码，但随着项目变大，它们通常会变得非常复杂且难以理解。这种复杂性会拖慢每个需要在系统上工作的人，进一步增加维护成本。一个陷入复杂性的软件项目有时被描述为*大泥球 (big ball of mud)* [^91]。

当复杂性使维护变得困难时，预算和时间表常常会超支。在复杂软件中，进行更改时引入缺陷的风险也更大：当系统对开发人员来说更难理解和推理时，隐藏的假设、意外后果和意外交互更容易被忽视 [^69]。

相反，减少复杂性大大改善了软件的可维护性，因此简单性应成为我们构建系统的关键目标。

简单的系统更易于理解，因此我们应该尝试以最简单的方式解决给定问题。不幸的是，这说起来容易做起来难。某事物是否简单通常是主观的品味问题，因为没有客观的简单性标准 [^92]。例如，一个系统可能在简单接口后面隐藏复杂的实现，而另一个系统可能具有简单的实现，但向其用户暴露更多内部细节——哪一个更简单？

关于复杂性的一个尝试是将其分解为两类，*本质 (essential)* 和*偶然 (accidental)* 复杂性 [^93]。这个想法是，本质复杂性是应用程序问题域固有的，而偶然复杂性仅因我们工具的局限而产生。不幸的是，这种区分也有缺陷，因为本质与偶然之间的界限会随着我们工具的演变而转移 [^94]。

我们管理复杂性的最佳工具之一是*抽象 (abstraction)*。一个好的抽象可以在干净、易于理解的门面 (façade) 后面隐藏大量实现细节。一个好的抽象还可以用于各种不同的应用程序。这种重用不仅比多次重新实现类似事物更高效，而且还能带来更高质量的软件，因为抽象组件的质量改进会使使用它的所有应用程序受益。

例如，高级编程语言是隐藏机器代码、CPU 寄存器和系统调用 (syscalls) 的抽象。SQL 是隐藏复杂的磁盘上和内存中数据结构、来自其他客户端的并发请求以及崩溃后不一致性的抽象。当然，当用高级语言编程时，我们仍在使用机器代码；我们只是不*直接* 使用它，因为编程语言抽象使我们不必考虑它。

旨在减少应用程序代码复杂性的抽象，可以使用*设计模式 (design patterns)* [^95] 和*领域驱动设计 (domain-driven design, DDD)* [^96] 等方法论创建。

本书不是关于此类特定于应用程序的抽象，而是关于通用抽象，你可以在其上构建应用程序，例如数据库事务、索引和事件日志。如果你想使用 DDD 等技术，你可以在本书描述的基础之上实现它们。

### 可演化性：使变更变得容易 (Evolvability: Making Change Easy) 

你的系统需求永远保持不变的可能性极小。它们更可能处于持续变化中：你了解到新事实，出现先前未预期的用例，业务优先级变化，用户请求新功能，新平台取代旧平台，法律或监管要求变化，系统增长迫使架构变化，等等。

就组织流程而言，*敏捷 (Agile)* 工作模式为适应变化提供了框架。敏捷社区还开发了在频繁变化的环境中开发软件时有用的技术工具和流程，例如测试驱动开发 (test-driven development, TDD) 和重构 (refactoring)。在本书中，我们寻找在由具有不同特征的多个不同应用程序或服务组成的系统层面提高敏捷性的方法。

你修改数据系统并使其适应变化需求的容易程度，与其简单性和抽象密切相关：松散耦合、简单的系统通常比紧耦合、复杂的系统更容易修改。由于这是一个如此重要的想法，我们将使用不同的词来指代数据系统层面的敏捷性：*可演化性 (evolvability)* [^97]。

使大型系统中的变更变得困难的一个主要因素是当某些操作不可逆时，因此该操作需要非常谨慎地执行 [^98]。例如，假设你正在从一个数据库迁移到另一个数据库：如果在新系统出现问题时你无法切换回旧系统，那么风险要比如果可以轻松回退高得多。最小化不可逆性可以提高灵活性。

## 总结 (Summary) 

在本章中，我们研究了几个非功能性需求的示例：性能、可靠性、可扩展性和可维护性。通过这些主题，我们也遇到了贯穿本书其余部分所需的原则和术语。我们从一个关于如何在社交网络中实现主页时间线的案例研究开始，这说明了在规模上出现的一些挑战。

我们讨论了如何衡量性能（例如，使用响应时间百分位数）、系统负载（例如，使用吞吐量指标），以及它们如何在 SLA 中使用。可扩展性是一个密切相关的概念：即确保在负载增长时性能保持不变。我们看到了一些可扩展性的通用原则，例如将任务分解为可以独立运行的较小部分，我们将在后续章节中深入探讨可扩展性技术的细节。

为了实现可靠性，你可以使用容错技术，这使得系统即使某些组件（例如，磁盘、机器或其他服务）发生故障时也能继续提供服务。我们看到了可能发生的硬件故障示例，并将它们与软件故障区分开来，后者可能更难处理，因为它们通常强相关。实现可靠性的另一个方面是构建对人类犯错的韧性，我们看到了无责备事后分析作为从事件中学习的技术。

最后，我们研究了可维护性的几个方面，包括支持运维团队的工作、管理复杂性，以及使应用程序的功能随时间推移易于演化。如何实现这些目标没有简单的答案，但一件事可能有所帮助：使用提供有用抽象的、易于理解的构建块来构建应用程序。本书的其余部分将涵盖一组在实践中被证明有价值的构建块。

### 参考文献 (References)
[^1]: Mike Cvet. [How We Learned to Stop Worrying and Love Fan-In at Twitter](https://www.youtube.com/watch?v=WEgCjwyXvwc). At *QCon San Francisco*, December 2016.
[^2]: Raffi Krikorian. [Timelines at Scale](https://www.infoq.com/presentations/Twitter-Timeline-Scalability/). At *QCon San Francisco*, November 2012. Archived at [perma.cc/V9G5-KLYK](https://perma.cc/V9G5-KLYK)
[^3]: Twitter. [Twitter's Recommendation Algorithm](https://blog.twitter.com/engineering/en_us/topics/open-source/2023/twitter-recommendation-algorithm). *blog.twitter.com*, March 2023. Archived at [perma.cc/L5GT-229T](https://perma.cc/L5GT-229T)
[^4]: Raffi Krikorian. [New Tweets per second record, and how!](https://blog.twitter.com/engineering/en_us/a/2013/new-tweets-per-second-record-and-how) *blog.twitter.com*, August 2013. Archived at [perma.cc/6JZN-XJYN](https://perma.cc/6JZN-XJYN)
[^5]: Jaz Volpert. [When Imperfect Systems are Good, Actually: Bluesky's Lossy Timelines](https://jazco.dev/2025/02/19/imperfection/). *jazco.dev*, February 2025. Archived at [perma.cc/2PVE-L2MX](https://perma.cc/2PVE-L2MX)
[^6]: Samuel Axon. [3% of Twitter's Servers Dedicated to Justin Bieber](https://mashable.com/archive/justin-bieber-twitter). *mashable.com*, September 2010. Archived at [perma.cc/F35N-CGVX](https://perma.cc/F35N-CGVX)
[^7]: Nathan Bronson, Abutalib Aghayev, Aleksey Charapko, and Timothy Zhu. [Metastable Failures in Distributed Systems](https://sigops.org/s/conferences/hotos/2021/papers/hotos21-s11-bronson.pdf). At *Workshop on Hot Topics in Operating Systems* (HotOS), May 2021. [doi:10.1145/3458336.3465286](https://doi.org/10.1145/3458336.3465286)
[^8]: Marc Brooker. [Metastability and Distributed Systems](https://brooker.co.za/blog/2021/05/24/metastable.html). *brooker.co.za*, May 2021. Archived at [perma.cc/7FGJ-7XRK](https://perma.cc/7FGJ-7XRK)
[^9]: Marc Brooker. [Exponential Backoff And Jitter](https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/). *aws.amazon.com*, March 2015. Archived at [perma.cc/R6MS-AZKH](https://perma.cc/R6MS-AZKH)
[^10]: Marc Brooker. [What is Backoff For?](https://brooker.co.za/blog/2022/08/11/backoff.html) *brooker.co.za*, August 2022. Archived at [perma.cc/PW9N-55Q5](https://perma.cc/PW9N-55Q5)
[^11]: Michael T. Nygard. [*Release It!*](https://learning.oreilly.com/library/view/release-it-2nd/9781680504552/), 2nd Edition. Pragmatic Bookshelf, January 2018. ISBN: 9781680502398
[^12]: Frank Chen. [Slowing Down to Speed Up – Circuit Breakers for Slack's CI/CD](https://slack.engineering/circuit-breakers/). *slack.engineering*, August 2022. Archived at [perma.cc/5FGS-ZPH3](https://perma.cc/5FGS-ZPH3)
[^13]: Marc Brooker. [Fixing retries with token buckets and circuit breakers](https://brooker.co.za/blog/2022/02/28/retries.html). *brooker.co.za*, February 2022. Archived at [perma.cc/MD6N-GW26](https://perma.cc/MD6N-GW26)
[^14]: David Yanacek. [Using load shedding to avoid overload](https://aws.amazon.com/builders-library/using-load-shedding-to-avoid-overload/). Amazon Builders' Library, *aws.amazon.com*. Archived at [perma.cc/9SAW-68MP](https://perma.cc/9SAW-68MP)
[^15]: Matthew Sackman. [Pushing Back](https://wellquite.org/posts/lshift/pushing_back/). *wellquite.org*, May 2016. Archived at [perma.cc/3KCZ-RUFY](https://perma.cc/3KCZ-RUFY)
[^16]: Dmitry Kopytkov and Patrick Lee. [Meet Bandaid, the Dropbox service proxy](https://dropbox.tech/infrastructure/meet-bandaid-the-dropbox-service-proxy). *dropbox.tech*, March 2018. Archived at [perma.cc/KUU6-YG4S](https://perma.cc/KUU6-YG4S)
[^17]: Haryadi S. Gunawi, Riza O. Suminto, Russell Sears, Casey Golliher, Swaminathan Sundararaman, Xing Lin, Tim Emami, Weiguang Sheng, Nematollah Bidokhti, Caitie McCaffrey, Gary Grider, Parks M. Fields, Kevin Harms, Robert B. Ross, Andree Jacobson, Robert Ricci, Kirk Webb, Peter Alvaro, H. Birali Runesha, Mingzhe Hao, and Huaicheng Li. [Fail-Slow at Scale: Evidence of Hardware Performance Faults in Large Production Systems](https://www.usenix.org/system/files/conference/fast18/fast18-gunawi.pdf). At *16th USENIX Conference on File and Storage Technologies*, February 2018.
[^18]: Marc Brooker. [Is the Mean Really Useless?](https://brooker.co.za/blog/2017/12/28/mean.html) *brooker.co.za*, December 2017. Archived at [perma.cc/U5AE-CVEM](https://perma.cc/U5AE-CVEM)
[^19]: Giuseppe DeCandia, Deniz Hastorun, Madan Jampani, Gunavardhan Kakulapati, Avinash Lakshman, Alex Pilchin, Swaminathan Sivasubramanian, Peter Vosshall, and Werner Vogels. [Dynamo: Amazon's Highly Available Key-Value Store](https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf). At *21st ACM Symposium on Operating Systems Principles* (SOSP), October 2007. [doi:10.1145/1294261.1294281](https://doi.org/10.1145/1294261.1294281)
[^20]: Kathryn Whitenton. [The Need for Speed, 23 Years Later](https://www.nngroup.com/articles/the-need-for-speed/). *nngroup.com*, May 2020. Archived at [perma.cc/C4ER-LZYA](https://perma.cc/C4ER-LZYA)
[^21]: Greg Linden. [Marissa Mayer at Web 2.0](https://glinden.blogspot.com/2006/11/marissa-mayer-at-web-20.html). *glinden.blogspot.com*, November 2005. Archived at [perma.cc/V7EA-3VXB](https://perma.cc/V7EA-3VXB)
[^22]: Jake Brutlag. [Speed Matters for Google Web Search](https://services.google.com/fh/files/blogs/google_delayexp.pdf). *services.google.com*, June 2009. Archived at [perma.cc/BK7R-X7M2](https://perma.cc/BK7R-X7M2)
[^23]: Eric Schurman and Jake Brutlag. [Performance Related Changes and their User Impact](https://www.youtube.com/watch?v=bQSE51-gr2s). Talk at *Velocity 2009*.
[^24]: Akamai Technologies, Inc. [The State of Online Retail Performance](https://web.archive.org/web/20210729180749/https%3A//www.akamai.com/us./multimedia/documents/report/akamai-state-of-online-retail-performance-spring-2017.pdf). *akamai.com*, April 2017. Archived at [perma.cc/UEK2-HYCS](https://perma.cc/UEK2-HYCS)
[^25]: Xiao Bai, Ioannis Arapakis, B. Barla Cambazoglu, and Ana Freire. [Understanding and Leveraging the Impact of Response Latency on User Behaviour in Web Search](https://iarapakis.github.io/papers/TOIS17.pdf). *ACM Transactions on Information Systems*, volume 36, issue 2, article 21, April 2018. [doi:10.1145/3106372](https://doi.org/10.1145/3106372)
[^26]: Jeffrey Dean and Luiz André Barroso. [The Tail at Scale](https://cacm.acm.org/research/the-tail-at-scale/). *Communications of the ACM*, volume 56, issue 2, pages 74–80, February 2013. [doi:10.1145/2408776.2408794](https://doi.org/10.1145/2408776.2408794)
[^27]: Alex Hidalgo. [*Implementing Service Level Objectives: A Practical Guide to SLIs, SLOs, and Error Budgets*](https://www.oreilly.com/library/view/implementing-service-level/9781492076803/). O'Reilly Media, September 2020. ISBN: 1492076813
[^28]: Jeffrey C. Mogul and John Wilkes. [Nines are Not Enough: Meaningful Metrics for Clouds](https://research.google/pubs/pub48033/). At *17th Workshop on Hot Topics in Operating Systems* (HotOS), May 2019. [doi:10.1145/3317550.3321432](https://doi.org/10.1145/3317550.3321432)
[^29]: Tamás Hauer, Philipp Hoffmann, John Lunney, Dan Ardelean, and Amer Diwan. [Meaningful Availability](https://www.usenix.org/conference/nsdi20/presentation/hauer). At *17th USENIX Symposium on Networked Systems Design and Implementation* (NSDI), February 2020.
[^30]: Ted Dunning. [The t-digest: Efficient estimates of distributions](https://www.sciencedirect.com/science/article/pii/S2665963820300403). *Software Impacts*, volume 7, article 100049, February 2021. [doi:10.1016/j.simpa.2020.100049](https://doi.org/10.1016/j.simpa.2020.100049)
[^31]: David Kohn. [How percentile approximation works (and why it's more useful than averages)](https://www.timescale.com/blog/how-percentile-approximation-works-and-why-its-more-useful-than-averages/). *timescale.com*, September 2021. Archived at [perma.cc/3PDP-NR8B](https://perma.cc/3PDP-NR8B)
[^32]: Heinrich Hartmann and Theo Schlossnagle. [Circllhist — A Log-Linear Histogram Data Structure for IT Infrastructure Monitoring](https://arxiv.org/pdf/2001.06561.pdf). *arxiv.org*, January 2020.
[^33]: Charles Masson, Jee E. Rim, and Homin K. Lee. [DDSketch: A Fast and Fully-Mergeable Quantile Sketch with Relative-Error Guarantees](https://www.vldb.org/pvldb/vol12/p2195-masson.pdf). *Proceedings of the VLDB Endowment*, volume 12, issue 12, pages 2195–2205, August 2019. [doi:10.14778/3352063.3352135](https://doi.org/10.14778/3352063.3352135)
[^34]: Baron Schwartz. [Why Percentiles Don't Work the Way You Think](https://orangematter.solarwinds.com/2016/11/18/why-percentiles-dont-work-the-way-you-think/). *solarwinds.com*, November 2016. Archived at [perma.cc/469T-6UGB](https://perma.cc/469T-6UGB)
[^35]: Walter L. Heimerdinger and Charles B. Weinstock. [A Conceptual Framework for System Fault Tolerance](https://resources.sei.cmu.edu/asset_files/TechnicalReport/1992_005_001_16112.pdf). Technical Report CMU/SEI-92-TR-033, Software Engineering Institute, Carnegie Mellon University, October 1992. Archived at [perma.cc/GD2V-DMJW](https://perma.cc/GD2V-DMJW)
[^36]: Felix C. Gärtner. [Fundamentals of fault-tolerant distributed computing in asynchronous environments](https://dl.acm.org/doi/pdf/10.1145/311531.311532). *ACM Computing Surveys*, volume 31, issue 1, pages 1–26, March 1999. [doi:10.1145/311531.311532](https://doi.org/10.1145/311531.311532)
[^37]: Algirdas Avižienis, Jean-Claude Laprie, Brian Randell, and Carl Landwehr. [Basic Concepts and Taxonomy of Dependable and Secure Computing](https://hdl.handle.net/1903/6459). *IEEE Transactions on Dependable and Secure Computing*, volume 1, issue 1, January 2004. [doi:10.1109/TDSC.2004.2](https://doi.org/10.1109/TDSC.2004.2)
[^38]: Ding Yuan, Yu Luo, Xin Zhuang, Guilherme Renna Rodrigues, Xu Zhao, Yongle Zhang, Pranay U. Jain, and Michael Stumm. [Simple Testing Can Prevent Most Critical Failures: An Analysis of Production Failures in Distributed Data-Intensive Systems](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-yuan.pdf). At *11th USENIX Symposium on Operating Systems Design and Implementation* (OSDI), October 2014.
[^39]: Casey Rosenthal and Nora Jones. [*Chaos Engineering*](https://learning.oreilly.com/library/view/chaos-engineering/9781492043850/). O'Reilly Media, April 2020. ISBN: 9781492043867
[^40]: Eduardo Pinheiro, Wolf-Dietrich Weber, and Luiz Andre Barroso. [Failure Trends in a Large Disk Drive Population](https://www.usenix.org/legacy/events/fast07/tech/full_papers/pinheiro/pinheiro_old.pdf). At *5th USENIX Conference on File and Storage Technologies* (FAST), February 2007.
[^41]: Bianca Schroeder and Garth A. Gibson. [Disk failures in the real world: What does an MTTF of 1,000,000 hours mean to you?](https://www.usenix.org/legacy/events/fast07/tech/schroeder/schroeder.pdf) At *5th USENIX Conference on File and Storage Technologies* (FAST), February 2007.
[^42]: Andy Klein. [Backblaze Drive Stats for Q2 2021](https://www.backblaze.com/blog/backblaze-drive-stats-for-q2-2021/). *backblaze.com*, August 2021. Archived at [perma.cc/2943-UD5E](https://perma.cc/2943-UD5E)
[^43]: Iyswarya Narayanan, Di Wang, Myeongjae Jeon, Bikash Sharma, Laura Caulfield, Anand Sivasubramaniam, Ben Cutler, Jie Liu, Badriddine Khessib, and Kushagra Vaid. [SSD Failures in Datacenters: What? When? and Why?](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/08/a7-narayanan.pdf) At *9th ACM International on Systems and Storage Conference* (SYSTOR), June 2016. [doi:10.1145/2928275.2928278](https://doi.org/10.1145/2928275.2928278)
[^44]: Alibaba Cloud Storage Team. [Storage System Design Analysis: Factors Affecting NVMe SSD Performance (1)](https://www.alibabacloud.com/blog/594375). *alibabacloud.com*, January 2019. Archived at [archive.org](https://web.archive.org/web/20230522005034/https%3A//www.alibabacloud.com/blog/594375)
[^45]: Bianca Schroeder, Raghav Lagisetty, and Arif Merchant. [Flash Reliability in Production: The Expected and the Unexpected](https://www.usenix.org/system/files/conference/fast16/fast16-papers-schroeder.pdf). At *14th USENIX Conference on File and Storage Technologies* (FAST), February 2016.
[^46]: Jacob Alter, Ji Xue, Alma Dimnaku, and Evgenia Smirni. [SSD failures in the field: symptoms, causes, and prediction models](https://dl.acm.org/doi/pdf/10.1145/3295500.3356172). At *International Conference for High Performance Computing, Networking, Storage and Analysis* (SC), November 2019. [doi:10.1145/3295500.3356172](https://doi.org/10.1145/3295500.3356172)
[^47]: Daniel Ford, François Labelle, Florentina I. Popovici, Murray Stokely, Van-Anh Truong, Luiz Barroso, Carrie Grimes, and Sean Quinlan. [Availability in Globally Distributed Storage Systems](https://www.usenix.org/legacy/event/osdi10/tech/full_papers/Ford.pdf). At *9th USENIX Symposium on Operating Systems Design and Implementation* (OSDI), October 2010.
[^48]: Kashi Venkatesh Vishwanath and Nachiappan Nagappan. [Characterizing Cloud Computing Hardware Reliability](https://www.microsoft.com/en-us/research/wp-content/uploads/2010/06/socc088-vishwanath.pdf). At *1st ACM Symposium on Cloud Computing* (SoCC), June 2010. [doi:10.1145/1807128.1807161](https://doi.org/10.1145/1807128.1807161)
[^49]: Peter H. Hochschild, Paul Turner, Jeffrey C. Mogul, Rama Govindaraju, Parthasarathy Ranganathan, David E. Culler, and Amin Vahdat. [Cores that don't count](https://sigops.org/s/conferences/hotos/2021/papers/hotos21-s01-hochschild.pdf). At *Workshop on Hot Topics in Operating Systems* (HotOS), June 2021. [doi:10.1145/3458336.3465297](https://doi.org/10.1145/3458336.3465297)
[^50]: Harish Dattatraya Dixit, Sneha Pendharkar, Matt Beadon, Chris Mason, Tejasvi Chakravarthy, Bharath Muthiah, and Sriram Sankar. [Silent Data Corruptions at Scale](https://arxiv.org/abs/2102.11245). *arXiv:2102.11245*, February 2021.
[^51]: Diogo Behrens, Marco Serafini, Sergei Arnautov, Flavio P. Junqueira, and Christof Fetzer. [Scalable Error Isolation for Distributed Systems](https://www.usenix.org/conference/nsdi15/technical-sessions/presentation/behrens). At *12th USENIX Symposium on Networked Systems Design and Implementation* (NSDI), May 2015.
[^52]: Bianca Schroeder, Eduardo Pinheiro, and Wolf-Dietrich Weber. [DRAM Errors in the Wild: A Large-Scale Field Study](https://static.googleusercontent.com/media/research.google.com.//pubs/archive/35162.pdf). At *11th International Joint Conference on Measurement and Modeling of Computer Systems* (SIGMETRICS), June 2009. [doi:10.1145/1555349.1555372](https://doi.org/10.1145/1555349.1555372)
[^53]: Yoongu Kim, Ross Daly, Jeremie Kim, Chris Fallin, Ji Hye Lee, Donghyuk Lee, Chris Wilkerson, Konrad Lai, and Onur Mutlu. [Flipping Bits in Memory Without Accessing Them: An Experimental Study of DRAM Disturbance Errors](https://users.ece.cmu.edu/~yoonguk/papers/kim-isca14.pdf). At *41st Annual International Symposium on Computer Architecture* (ISCA), June 2014. [doi:10.5555/2665671.2665726](https://doi.org/10.5555/2665671.2665726)
[^54]: Tim Bray. [Worst Case](https://www.tbray.org/ongoing/When/202x/2021/10/08/The-WOrst-Case). *tbray.org*, October 2021. Archived at [perma.cc/4QQM-RTHN](https://perma.cc/4QQM-RTHN)
[^55]: Sangeetha Abdu Jyothi. [Solar Superstorms: Planning for an Internet Apocalypse](https://ics.uci.edu/~sabdujyo/papers/sigcomm21-cme.pdf). At *ACM SIGCOMM Conferene*, August 2021. [doi:10.1145/3452296.3472916](https://doi.org/10.1145/3452296.3472916)
[^56]: Adrian Cockcroft. [Failure Modes and Continuous Resilience](https://adrianco.medium.com/failure-modes-and-continuous-resilience-6553078caad5). *adrianco.medium.com*, November 2019. Archived at [perma.cc/7SYS-BVJP](https://perma.cc/7SYS-BVJP)
[^57]: Shujie Han, Patrick P. C. Lee, Fan Xu, Yi Liu, Cheng He, and Jiongzhou Liu. [An In-Depth Study of Correlated Failures in Production SSD-Based Data Centers](https://www.usenix.org/conference/fast21/presentation/han). At *19th USENIX Conference on File and Storage Technologies* (FAST), February 2021.
[^58]: Edmund B. Nightingale, John R. Douceur, and Vince Orgovan. [Cycles, Cells and Platters: An Empirical Analysis of Hardware Failures on a Million Consumer PCs](https://eurosys2011.cs.uni-salzburg.at/pdf/eurosys2011-nightingale.pdf). At *6th European Conference on Computer Systems* (EuroSys), April 2011. [doi:10.1145/1966445.1966477](https://doi.org/10.1145/1966445.1966477)
[^59]: Haryadi S. Gunawi, Mingzhe Hao, Tanakorn Leesatapornwongsa, Tiratat Patana-anake, Thanh Do, Jeffry Adityatama, Kurnia J. Eliazar, Agung Laksono, Jeffrey F. Lukman, Vincentius Martin, and Anang D. Satria. [What Bugs Live in the Cloud?](https://ucare.cs.uchicago.edu/pdf/socc14-cbs.pdf) At *5th ACM Symposium on Cloud Computing* (SoCC), November 2014. [doi:10.1145/2670979.2670986](https://doi.org/10.1145/2670979.2670986)
[^60]: Jay Kreps. [Getting Real About Distributed System Reliability](https://blog.empathybox.com/post/19574936361/getting-real-about-distributed-system-reliability). *blog.empathybox.com*, March 2012. Archived at [perma.cc/9B5Q-AEBW](https://perma.cc/9B5Q-AEBW)
[^61]: Nelson Minar. [Leap Second Crashes Half the Internet](https://www.somebits.com/weblog/tech/bad/leap-second-2012.html). *somebits.com*, July 2012. Archived at [perma.cc/2WB8-D6EU](https://perma.cc/2WB8-D6EU)
[^62]: Hewlett Packard Enterprise. [Support Alerts – Customer Bulletin a00092491en\_us](https://support.hpe.com/hpesc/public/docDisplay?docId=emr_na-a00092491en_us). *support.hpe.com*, November 2019. Archived at [perma.cc/S5F6-7ZAC](https://perma.cc/S5F6-7ZAC)
[^63]: Lorin Hochstein. [awesome limits](https://github.com/lorin/awesome-limits). *github.com*, November 2020. Archived at [perma.cc/3R5M-E5Q4](https://perma.cc/3R5M-E5Q4)
[^64]: Caitie McCaffrey. [Clients Are Jerks: AKA How Halo 4 DoSed the Services at Launch & How We Survived](https://www.caitiem.com/2015/06/23/clients-are-jerks-aka-how-halo-4-dosed-the-services-at-launch-how-we-survived/). *caitiem.com*, June 2015. Archived at [perma.cc/MXX4-W373](https://perma.cc/MXX4-W373)
[^65]: Lilia Tang, Chaitanya Bhandari, Yongle Zhang, Anna Karanika, Shuyang Ji, Indranil Gupta, and Tianyin Xu. [Fail through the Cracks: Cross-System Interaction Failures in Modern Cloud Systems](https://tianyin.github.io/pub/csi-failures.pdf). At *18th European Conference on Computer Systems* (EuroSys), May 2023. [doi:10.1145/3552326.3587448](https://doi.org/10.1145/3552326.3587448)
[^66]: Mike Ulrich. [Addressing Cascading Failures](https://sre.google/sre-book/addressing-cascading-failures/). In Betsy Beyer, Jennifer Petoff, Chris Jones, and Niall Richard Murphy (ed). [*Site Reliability Engineering: How Google Runs Production Systems*](https://www.oreilly.com/library/view/site-reliability-engineering/9781491929117/). O'Reilly Media, 2016. ISBN: 9781491929124
[^67]: Harri Faßbender. [Cascading failures in large-scale distributed systems](https://blog.mi.hdm-stuttgart.de/index.php/2022/03/03/cascading-failures-in-large-scale-distributed-systems/). *blog.mi.hdm-stuttgart.de*, March 2022. Archived at [perma.cc/K7VY-YJRX](https://perma.cc/K7VY-YJRX)
[^68]: Richard I. Cook. [How Complex Systems Fail](https://www.adaptivecapacitylabs.com/HowComplexSystemsFail.pdf). Cognitive Technologies Laboratory, April 2000. Archived at [perma.cc/RDS6-2YVA](https://perma.cc/RDS6-2YVA)
[^69]: David D. Woods. [STELLA: Report from the SNAFUcatchers Workshop on Coping With Complexity](https://snafucatchers.github.io/). *snafucatchers.github.io*, March 2017. Archived at [archive.org](https://web.archive.org/web/20230306130131/https%3A//snafucatchers.github.io/)
[^70]: David Oppenheimer, Archana Ganapathi, and David A. Patterson. [Why Do Internet Services Fail, and What Can Be Done About It?](https://static.usenix.org/events/usits03/tech/full_papers/oppenheimer/oppenheimer.pdf) At *4th USENIX Symposium on Internet Technologies and Systems* (USITS), March 2003.
[^71]: Sidney Dekker. [*The Field Guide to Understanding 'Human Error', 3rd Edition*](https://learning.oreilly.com/library/view/the-field-guide/9781317031833/). CRC Press, November 2017. ISBN: 9781472439055
[^72]: Sidney Dekker. [*Drift into Failure: From Hunting Broken Components to Understanding Complex Systems*](https://www.taylorfrancis.com/books/mono/10.1201/9781315257396/drift-failure-sidney-dekker). CRC Press, 2011. ISBN: 9781315257396
[^73]: John Allspaw. [Blameless PostMortems and a Just Culture](https://www.etsy.com/codeascraft/blameless-postmortems/). *etsy.com*, May 2012. Archived at [perma.cc/YMJ7-NTAP](https://perma.cc/YMJ7-NTAP)
[^74]: Itzy Sabo. [Uptime Guarantees — A Pragmatic Perspective](https://world.hey.com/itzy/uptime-guarantees-a-pragmatic-perspective-736d7ea4). *world.hey.com*, March 2023. Archived at [perma.cc/F7TU-78JB](https://perma.cc/F7TU-78JB)
[^75]: Michael Jurewitz. [The Human Impact of Bugs](http://jury.me/blog/2013/3/14/the-human-impact-of-bugs). *jury.me*, March 2013. Archived at [perma.cc/5KQ4-VDYL](https://perma.cc/5KQ4-VDYL)
[^76]: Mark Halper. [How Software Bugs led to 'One of the Greatest Miscarriages of Justice' in British History](https://cacm.acm.org/news/how-software-bugs-led-to-one-of-the-greatest-miscarriages-of-justice-in-british-history/). *Communications of the ACM*, January 2025. [doi:10.1145/3703779](https://doi.org/10.1145/3703779)
[^77]: Nicholas Bohm, James Christie, Peter Bernard Ladkin, Bev Littlewood, Paul Marshall, Stephen Mason, Martin Newby, Steven J. Murdoch, Harold Thimbleby, and Martyn Thomas. [The legal rule that computers are presumed to be operating correctly – unforeseen and unjust consequences](https://www.benthamsgaze.org/wp-content/uploads/2022/06/briefing-presumption-that-computers-are-reliable.pdf). Briefing note, *benthamsgaze.org*, June 2022. Archived at [perma.cc/WQ6X-TMW4](https://perma.cc/WQ6X-TMW4)
[^78]: Dan McKinley. [Choose Boring Technology](https://mcfunley.com/choose-boring-technology). *mcfunley.com*, March 2015. Archived at [perma.cc/7QW7-J4YP](https://perma.cc/7QW7-J4YP)
[^79]: Andy Warfield. [Building and operating a pretty big storage system called S3](https://www.allthingsdistributed.com/2023/07/building-and-operating-a-pretty-big-storage-system.html). *allthingsdistributed.com*, July 2023. Archived at [perma.cc/7LPK-TP7V](https://perma.cc/7LPK-TP7V)
[^80]: Marc Brooker. [Surprising Scalability of Multitenancy](https://brooker.co.za/blog/2023/03/23/economics.html). *brooker.co.za*, March 2023. Archived at [perma.cc/ZZD9-VV8T](https://perma.cc/ZZD9-VV8T)
[^81]: Ben Stopford. [Shared Nothing vs. Shared Disk Architectures: An Independent View](http://www.benstopford.com/2009/11/24/understanding-the-shared-nothing-architecture/). *benstopford.com*, November 2009. Archived at [perma.cc/7BXH-EDUR](https://perma.cc/7BXH-EDUR)
[^82]: Michael Stonebraker. [The Case for Shared Nothing](https://dsf.berkeley.edu/papers/hpts85-nothing.pdf). *IEEE Database Engineering Bulletin*, volume 9, issue 1, pages 4–9, March 1986.
[^83]: Panagiotis Antonopoulos, Alex Budovski, Cristian Diaconu, Alejandro Hernandez Saenz, Jack Hu, Hanuma Kodavalla, Donald Kossmann, Sandeep Lingam, Umar Farooq Minhas, Naveen Prakash, Vijendra Purohit, Hugh Qu, Chaitanya Sreenivas Ravella, Krystyna Reisteter, Sheetal Shrotri, Dixin Tang, and Vikram Wakade. [Socrates: The New SQL Server in the Cloud](https://www.microsoft.com/en-us/research/uploads/prod/2019/05/socrates.pdf). At *ACM International Conference on Management of Data* (SIGMOD), pages 1743–1756, June 2019. [doi:10.1145/3299869.3314047](https://doi.org/10.1145/3299869.3314047)
[^84]: Sam Newman. [*Building Microservices*, second edition](https://www.oreilly.com/library/view/building-microservices-2nd/9781492034018/). O'Reilly Media, 2021. ISBN: 9781492034025
[^85]: Nathan Ensmenger. [When Good Software Goes Bad: The Surprising Durability of an Ephemeral Technology](https://themaintainers.wpengine.com/wp-content/uploads/2021/04/ensmenger-maintainers-v2.pdf). At *The Maintainers Conference*, April 2016. Archived at [perma.cc/ZXT4-HGZB](https://perma.cc/ZXT4-HGZB)
[^86]: Robert L. Glass. [*Facts and Fallacies of Software Engineering*](https://learning.oreilly.com/library/view/facts-and-fallacies/0321117425/). Addison-Wesley Professional, October 2002. ISBN: 9780321117427
[^87]: Marianne Bellotti. [*Kill It with Fire*](https://learning.oreilly.com/library/view/kill-it-with/9781098128883/). No Starch Press, April 2021. ISBN: 9781718501188
[^88]: Lisanne Bainbridge. [Ironies of automation](https://www.adaptivecapacitylabs.com/IroniesOfAutomation-Bainbridge83.pdf). *Automatica*, volume 19, issue 6, pages 775–779, November 1983. [doi:10.1016/0005-1098(83)90046-8](https://doi.org/10.1016/0005-1098%2883%2990046-8)
[^89]: James Hamilton. [On Designing and Deploying Internet-Scale Services](https://www.usenix.org/legacy/events/lisa07/tech/full_papers/hamilton/hamilton.pdf). At *21st Large Installation System Administration Conference* (LISA), November 2007.
[^90]: Dotan Horovits. [Open Source for Better Observability](https://horovits.medium.com/open-source-for-better-observability-8c65b5630561). *horovits.medium.com*, October 2021. Archived at [perma.cc/R2HD-U2ZT](https://perma.cc/R2HD-U2ZT)
[^91]: Brian Foote and Joseph Yoder. [Big Ball of Mud](http://www.laputan.org/pub/foote/mud.pdf). At *4th Conference on Pattern Languages of Programs* (PLoP), September 1997. Archived at [perma.cc/4GUP-2PBV](https://perma.cc/4GUP-2PBV)
[^92]: Marc Brooker. [What is a simple system?](https://brooker.co.za/blog/2022/05/03/simplicity.html) *brooker.co.za*, May 2022. Archived at [perma.cc/U72T-BFVE](https://perma.cc/U72T-BFVE)
[^93]: Frederick P. Brooks. [No Silver Bullet – Essence and Accident in Software Engineering](https://worrydream.com/refs/Brooks_1986_-_No_Silver_Bullet.pdf). In [*The Mythical Man-Month*](https://www.oreilly.com/library/view/mythical-man-month-the/0201835959/), Anniversary edition, Addison-Wesley, 1995. ISBN: 9780201835953
[^94]: Dan Luu. [Against essential and accidental complexity](https://danluu.com/essential-complexity/). *danluu.com*, December 2020. Archived at [perma.cc/H5ES-69KC](https://perma.cc/H5ES-69KC)
[^95]: Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides. [*Design Patterns: Elements of Reusable Object-Oriented Software*](https://learning.oreilly.com/library/view/design-patterns-elements/0201633612/). Addison-Wesley Professional, October 1994. ISBN: 9780201633610
[^96]: Eric Evans. [*Domain-Driven Design: Tackling Complexity in the Heart of Software*](https://learning.oreilly.com/library/view/domain-driven-design-tackling/0321125215/). Addison-Wesley Professional, August 2003. ISBN: 9780321125217
[^97]: Hongyu Pei Breivold, Ivica Crnkovic, and Peter J. Eriksson. [Analyzing Software Evolvability](https://www.es.mdh.se/pdf_publications/1251.pdf). at *32nd Annual IEEE International Computer Software and Applications Conference* (COMPSAC), July 2008. [doi:10.1109/COMPSAC.2008.50](https://doi.org/10.1109/COMPSAC.2008.50)
[^98]: Enrico Zaninotto. [From X programming to the X organisation](https://martinfowler.com/articles/zaninotto.pdf). At *XP Conference*, May 2002. Archived at [perma.cc/R9AR-QCKZ](https://perma.cc/R9AR-QCKZ)