# 人工智能的发展现状等基本概论

# 知识表达与推理
## 逻辑

什么是**逻辑：进行正确推理和充分论证的研究**（the study of correct reasoning and good arguments）

**命题：确定为真或为假的陈述句**

命题通常用小写符号来表示，如p或者 q。命题总是具有一个“真值”，它只有两种可能性：真或假，分别用符号T（True）和F（False）表示。只有具有确定真值的陈述句才是命题，无法判断真或假的描述性句子不能作为命题 。
- 北京是中国的首都  （真命题）
- 请出去  （祈使句）
- 您去开会吗?  （疑问句）
- 13能被6整除 （假命题）
- 这座山真高啊！（感叹句）
- 我正在说谎  （悖论）

**逻辑等价：具有相同的真假结果，一般用$\equiv$来表示**

给定命题p和命题q，如果p和q在所有情况下都具有相同的真假结果，那么p和q在逻辑上等价，一般用≡来表示，即p≡q。
逻辑等价为命题进行形式转换带来了可能。基于命题转换，我们不再需要逐一列出 p和q的真值表来判断两者是否在逻辑上等价，而是可直接根据已有逻辑等价公式来判断p和q在逻辑上是否等价。


### 逻辑，与($\land$)、或($\lor$)、异或($\oplus$)


### 全称量词($\forall$)、存在量词($\exists$)的消去、引入
### 原子命题，复合命题
### 推理手段，推导过程，谓词逻辑（关系）
## 知识图谱推理
### 三元组，两个对象 + 关系
### FOIL 算法
### 路径排序算法
## 概率图
### 贝叶斯有向图
### 马尔可夫无向图
### 局部马尔可夫性，概率计算
## 因果
### 混淆偏差（前因）、选择偏差（后果）
### 干预
### 反事实

# 搜索求解
## 基本概念
### 状态、动作、状态转移、路径/代价
### 搜索算法的时空复杂度、完备性、最优性
### 搜索算法的启发函数、评价函数
## 搜索算法
### 贪婪搜索
### A*搜索，可容性、一致性
## 对抗搜索
### minmax 对抗
### $\alpha$-$\beta$ 剪枝
## 蒙特卡洛方法
### 蒙特卡洛树搜索
### $\epsilon$-贪心
### UCB1，上限置信区间算法

# 机器学习
## 基本概念
### 有监督、无监督学习的区别
### 三个集的区别：训练集、验证集、测试集
### 三种损失函数：均方差，交叉熵，0-1 损失
### 两种风险：经验风险，期望风险
## 评估指标
### 准确率，错误率
### 精确率
### 召回率
### F1值
## 模型与算法
### logistic 非线性回归的计算
### $\sigma(x)$(sigmoid), $\tanh(x)$, $\text{ReLU}(x)$ 的区别、特点
### $k$ 均值聚类的步骤
### LDA 与 PCA 的区别与对比
### eigenface 的流程

# 深度学习
## 神经网络基础
### 感知机与深度神经网络
### 全连接、反向传播、梯度下降
### $\text{ReLU}$, $\sigma$(sigmoid), $\text{softmax}$
### 反推某个变量的梯度，要会推，反传，链式法则
## 卷积神经网络
### 池化层，会计算池化后向量大小
### 局部感知，参数共享，选择性感受野
## 循环神经网络
### 长短时记忆模型 LSTM
## 注意力机制
### 自注意力机制
## 训练与优化
### 梯度消失、梯度爆炸
### 正则化、过拟合、dropout
## 自然语言处理基础
### 词向量的生成

# 强化学习
## 基本概念
### 价值($V$)和奖励($R$)的区别辨析
### 监督学习、无监督学习、强化学习的横向对比
### 单步 vs 序贯性
### 离散马尔科夫链
### 奖励机制，折扣因子($\gamma$)
### MDP 马尔可夫决策过程
### 轨迹、片段、持续问题
## 核心函数与方程
### 策略函数($\pi$)、价值函数($V$)、动作-价值函数($Q$)
### 贝尔曼方程
## 方法分类
### 基于策略的和基于价值的强化学习区别
## 经典算法
### DP 算法
### 蒙特卡洛算法
### TD 算法
### $Q$-学习，怎样更新 $Q$ 值，探索和利用
### DQN 引入的新机制
### actor-critic 算法

# 人工智能博弈
## 基本模型
### 囚徒困境、最优解
### 纳什均衡。期望收益相同
## 算法
### 虚拟遗憾最小化算法，例子：石头剪刀布
## 匹配理论
### 双边匹配
### 单边匹配