# 第5章 大数定律及中心极限定理

从前面四章的介绍中，我们知道随机现象的规律性要在大量试验中重复考察才能体现出来，“大量”这一特点就意味着对极限定理研究的必要性。极限定理是概率论的重要内容，也是数理统计学的基石之一。长期以来，对极限定理的研究所形成的概率论分析方法影响着概率论的发展。同时，新的极限理论问题也在实际研究和应用中不断产生和解决。极限定理主要包括随机变量及其分布的极限性质和收敛性的一些结果，其中大数定律及中心极限定理这两类是极限定理中的基本理论。大数定律主要探讨随机变量序列的平均在一定条件下的稳定性规律；大量的随机变量之和的分布在一定条件下可以用正态分布去逼近，这就是中心极限定理的主要研究内容。我们将在本章介绍这两类极限定理。



## 5.1 大数定律

在给出大数定律之前，我们先介绍一下用数学语言表述大数定律时所用到的概率意义下的极限定义以及在证明大数定律时所涉及的两个概率不等式。

### （一）依概率收敛

**定义 5.1.1** 设 $\{Y_n, n \geq 1\}$ 为一随机变量序列，$c$ 为一常数。若对任意的 $\varepsilon > 0$，都有

$$
\lim_{n \to +\infty} P\{|Y_n - c| \geq \varepsilon\} = 0 \tag{5.1.1}
$$

成立，则称 $\{Y_n, n \geq 1\}$ **依概率收敛**（convergence in probability）于 $c$，记为

$$
Y_n \xrightarrow{P} c, \, n \to +\infty.
$$

显然，(5.1.1) 可以等价表示为

$$
\lim_{n \to +\infty} P\{|Y_n - c| < \varepsilon\} = 1. \tag{5.1.2}
$$

一般地，称概率接近 1 的事件为大概率事件，称概率接近 0 的事件为小概率事件。由此可见，$\{Y_n, n \geq 1\}$ 依概率收敛于 $c$ 意味着：当 $n$ 很大时，$Y_n$ 十分接近 $c$，两者的偏差小于任意给定的正数 $\varepsilon$ 这一事件发生的概率趋于 1，为一大概率事件。请注意，这种收敛性是在概率意义下的一种收敛，而不是数学意义下的一般收敛。

下面不加证明地给出依概率收敛的一个重要性质：

设 $X_n \xrightarrow{P} a, Y_n \xrightarrow{P} b, \, n \to +\infty$，其中 $a, b$ 为两个常数，若二元函数 $g(x, y)$ 在点 $(a, b)$ 处连续，则有

$$
g(X_n, Y_n) \xrightarrow{P} g(a, b), \, n \to +\infty. \tag{5.1.3}
$$



### （二）马尔可夫不等式和切比雪夫不等式

**定理 5.1.1** （马尔可夫（Markov）不等式）若随机变量 $Y$ 的 $k$ 阶（原点）矩存在 ($k \geq 1$)，则对任意的 $\varepsilon > 0$，有

$$
P\{|Y| \geq \varepsilon\} \leq \frac{E(|Y|^k)}{\varepsilon^k} \quad \left(\text{或写为 } P\{|Y| < \varepsilon\} \geq 1 - \frac{E(|Y|^k)}{\varepsilon^k}\right). \tag{5.1.4}
$$

特别地，当 $Y$ 为取非负值的随机变量且它的 $k$ 阶矩存在时，则有

$$
P\{Y \geq \varepsilon\} \leq \frac{E(Y^k)}{\varepsilon^k}. \tag{5.1.5}
$$

**证明** 令

$$
Z = 
\begin{cases}
\varepsilon, & |Y| \geq \varepsilon, \\
0, & |Y| < \varepsilon,
\end{cases}
$$

则 $Z^k \leq |Y|^k$，故 $E(Z^k) \leq E(|Y|^k)$。对任意的 $k \geq 1$，注意到 $E(Z^k) = \varepsilon^k \cdot P\{|Y| \geq \varepsilon\}$，所以

$$
P\{|Y| \geq \varepsilon\} = \frac{E(Z^k)}{\varepsilon^k} \leq \frac{E(|Y|^k)}{\varepsilon^k}.
$$

**例 5.1.1** 某城市一周内发生交通事故的次数记为随机变量 $X$，显然 $P\{X \geq 0\} = 1$。已知 $E(X) = 75$，求一周内发生事故的次数不少于 100 的概率上界。

由于 $P\{X \geq 0\} = 1$，且 $X$ 的数学期望存在，取 $k = 1$，利用马尔可夫不等式，有

$$
P\{X \geq 100\} \leq \frac{E(X)}{100} = 75\%.
$$

即一周内发生事故的次数不少于 100 的概率上界为 75%。

作为马尔可夫不等式的推论，可得：

**定理 5.1.2** （切比雪夫（Chebyshev）不等式）设随机变量 $X$ 的数学期望和方差存在，分别记为 $\mu, \sigma^2$，则对任意的 $\varepsilon > 0$，有

$$
P\{|X - \mu| \geq \varepsilon\} \leq \frac{\sigma^2}{\varepsilon^2} \quad \left(\text{或写为 } P\{|X - \mu| < \varepsilon\} \geq 1 - \frac{\sigma^2}{\varepsilon^2}\right). \tag{5.1.6}
$$

**证明** 在定理 5.1.1 中，取 $Y = X - \mu, k = 2$ 即可。

切比雪夫不等式的重要性在于：不管随机变量的分布类型是什么，只要已知它的数学期望和方差，就可以对随机变量落入数学期望附近的区域 $(\mu - \varepsilon, \mu + \varepsilon)$ 内或外的概率给出一个界的估计。

从 (5.1.6) 可以看出，$X$ 的方差越小，对于同一个 $\varepsilon > 0$，$P\{|X - \mu| \geq \varepsilon\}$ 的上界就越小，即 $X$ 落入区域 $(-\infty, \mu - \varepsilon] \cup [\mu + \varepsilon, +\infty)$ 的可能性就越小，落入 $(\mu - \varepsilon, \mu + \varepsilon)$ 这个 $\mu$ 附近区域的可能性就越大。这也进一步说明了方差这个数字特征的确刻画了 $X$ 的概率分布偏离其中心位置（数学期望）的离散程度。

**例 5.1.2** 证明：设随机变量 $X$ 的方差存在，若 $\text{Var}(X) = 0$，则 $P\{X = c\} = 1$，其中 $c = E(X)$。

**证明** 由于 $X$ 的方差存在，故其数学期望也存在。如果 $|X - E(X)| > 0$，那么必存在某正整数 $n$，使得 $|X - E(X)| \geq \frac{1}{n}$，反之亦然。故

$$
\{ |X - E(X)| > 0 \} = \bigcup_{n=1}^{+\infty} \left\{ |X - E(X)| \geq \frac{1}{n} \right\},
$$

于是有

$$
0 \leq P\{ |X - E(X)| > 0 \} = P \left\{ \bigcup_{n=1}^{+\infty} \left( |X - E(X)| \geq \frac{1}{n} \right) \right\}
\leq \sum_{n=1}^{+\infty} P \left\{ |X - E(X)| \geq \frac{1}{n} \right\}
\leq \sum_{n=1}^{+\infty} \frac{\text{Var}(X)}{(1/n)^2} = 0,
$$

其中最后一个不等式用了切比雪夫不等式。这样就得到了 $P\{ |X - E(X)| > 0 \} = 0$，即 $P\{ X = E(X) \} = 1$。

这一结论具体可理解为：当随机变量 $X$ 的方差为 0，即没有波动时，也就意味着 $X$ 集中地取一值，这一数值就是它的平均值。这是一个十分直观而又明显的结论。

**例 5.1.3** 某天文机构为了得到宇宙中两颗行星的距离，进行了 $n$ 次独立的观测，第 $i$ 次的观测值为 $X_i$ 光年，$i = 1, 2, \cdots , n$。$E(X_i) = \mu$，$\text{Var}(X_i) = 5$，其中 $\mu$ 是两颗行星的真实距离（未知）。现取 $n$ 次观测值的平均值作为真实距离的估计。

(1) 如果测量次数 $n = 100$，问估计值与真实值之间的误差在 ±0.5 光年之内的概率至少有多大？

(2) 如果要以不低于 95% 的把握控制估计值与真实值之间的误差在 ±0.5 光年之内，问观测次数至少为多少？

**解** 由于对任意的 $i = 1, 2, \cdots , n$，$E(X_i) = \mu$，$\text{Var}(X_i) = 5$，且每次观测独立，故

$$
E \left( \frac{1}{n} \sum_{i=1}^{n} X_i \right) = \mu, \quad \text{Var} \left( \frac{1}{n} \sum_{i=1}^{n} X_i \right) = \frac{5}{n}.
$$

(1) 当 $n = 100$ 时，由切比雪夫不等式知

$$
P \left\{ \left| \frac{1}{100} \sum_{i=1}^{100} X_i - \mu \right| < 0.5 \right\} \geq 1 - \frac{5/100}{0.5^2} = 0.8,
$$

故当测量 100 次时，估计值与真实值之间的误差在 ±0.5 光年之内的概率至少有 80%。

(2) 同样利用切比雪夫不等式，可得

$$
P \left\{ \left| \frac{1}{n} \sum_{i=1}^{n} X_i - \mu \right| < 0.5 \right\} \geq 1 - \frac{5/n}{0.5^2} \geq 95\%,
$$

从而得到 $n \geq 400$，即至少要观测 400 次才能保证以不低于 95% 的把握控制估计值与真实值之间的误差在 ±0.5 光年之内。



### （三）两个大数定律

**定义 5.1.2** 设 $\{X_i, i \geq 1\}$ 为一随机变量序列，若存在常数序列 $\{c_n, n \geq 1\}$，使得对任意的 $\varepsilon > 0$，有

$$
\lim_{n \to +\infty} P \left\{ \left| \frac{1}{n} \sum_{i=1}^{n} X_i - c_n \right| \geq \varepsilon \right\} = 0 \quad \text{或} \quad \lim_{n \to +\infty} P \left\{ \left| \frac{1}{n} \sum_{i=1}^{n} X_i - c_n \right| < \varepsilon \right\} = 1 \tag{5.1.7}
$$

成立，即当 $n \to +\infty$ 时，有

$$
\frac{1}{n} \sum_{i=1}^{n} X_i - c_n \xrightarrow{P} 0,
$$

则称随机变量序列 $\{X_i, i \geq 1\}$ **服从弱大数定律**（weak law of large numbers），简称服从大数定律。特别地，当 $c_n = c(n = 1, 2, \cdots )$ 时，可写为

$$
\frac{1}{n} \sum_{i=1}^{n} X_i \xrightarrow{P} c, \quad n \to +\infty.
$$

在实践中，人们发现大量测量值的算术平均具有一定的稳定性，这一稳定性其实就是大数定律的客观背景。最早的大数定律是著名的伯努利大数定律。

**定理 5.1.3** （伯努利（Bernoulli）大数定律）设 $n_A$ 为 $n$ 重伯努利试验中事件 $A$ 发生的次数，$p(0 < p < 1)$ 为事件 $A$ 在每次试验中发生的概率，即 $P(A) = p$，则对任意的 $\varepsilon > 0$，有

$$
\lim_{n \to +\infty} P \left\{ \left| \frac{n_A}{n} - p \right| \geq \varepsilon \right\} = 0.
$$

**证明** 引入随机变量

$$
X_i = 
\begin{cases} 
1, & \text{第 } i \text{ 次试验中事件 } A \text{ 发生}, \\ 
0, & \text{第 } i \text{ 次试验中事件 } A \text{ 不发生},
\end{cases} \quad i = 1, 2, \cdots, n.
$$

易见 $n_A = \sum_{i=1}^n X_i$，$X_1, X_2, \cdots, X_n$ 相互独立，且都服从参数为 $p$ 的 $0-1$ 分布。从而

$$
E(X_i) = p, \quad \text{Var}(X_i) = p(1-p), \quad i = 1, 2, \cdots, n.
$$

故 $E\left(\frac{n_A}{n}\right) = p$，$\text{Var}\left(\frac{n_A}{n}\right) = \frac{p(1-p)}{n}$。利用切比雪夫不等式，可得

$$
P\left\{\left|\frac{n_A}{n}-p\right| \geq \varepsilon\right\} = P\left\{\left|\frac{n_A}{n}-E\left(\frac{n_A}{n}\right)\right| \geq \varepsilon\right\} \leq \frac{\text{Var}\left(\frac{n_A}{n}\right)}{\varepsilon^2} = \frac{p(1-p)}{n\varepsilon^2} \to 0, \quad n \to +\infty.
$$

再结合 $P\left\{\left|\frac{n_A}{n}-p\right| \geq \varepsilon\right\} \geq 0$，定理得证。

伯努利大数定律提供了用频率的极限值来定义概率的理论依据。事实上，在本书的第 1 章就曾提及在重复试验中某一事件发生的频率具有一定的稳定性，即当试验次数增加时，事件发生的频率稳定于某一确定的常数附近。这一发现启发人们用一个确定的数去表征某事件发生的可能性大小，进而有了“概率”一词的说法。概率论的研究至今约有 300 多年的历史，作为这门学科的基础，“概率”定义的合理性和严密性这一根本问题在此学科的起始阶段一直困扰着研究者，一直到 1713 年，伯努利在其名著《猜度术》中提出了上述大数定律，才从数学上严格证明了频率的稳定值即为概率的结论。因此，伯努利的这篇文章有时也被称为概率论中的第一篇论文，该结果也为概率论的公理化体系奠定了扎实的理论基础。

**例 5.1.4** （用蒙特卡罗方法（也称随机投点法）计算定积分）设 $f(x)$ 为定义在 $[0,1]$ 上的连续函数，且 $0 \leq f(x) \leq 1$，求定积分 $I = \int_0^1 f(x)dx$ 的近似值。

**解** 设 $(X,Y)$ 服从正方形 $\{(x,y): 0 \leq x \leq 1, 0 \leq y \leq 1\}$ 上的均匀分布，则 $X$ 与 $Y$ 相互独立，且都服从 $[0,1]$ 上的均匀分布。令事件 $A = \{Y \leq f(X)\}$，则 $A$ 发生的概率为

$$
p = P(A) = P\{Y \leq f(X)\} = \iint\limits_{y \leq f(x)} f(x,y)dydx
$$

$$
= \int_{0}^{1} \int_{0}^{f(x)} 1dy dx = \int_{0}^{1} f(x)dx = I,
$$

即定积分 $I$ 的值就是事件 $A$ 发生的概率 $p$。那么根据伯努利大数定律，我们可以通过做大量重复独立试验，以试验中事件 $A$ 发生的频率来作为定积分 $I$ 的近似值。下面用蒙特卡罗方法来得到事件 $A$ 发生的频率：

(1) 用计算机随机产生 $[0,1]$ 上均匀分布的 $2n$ 个随机数 $x_i, y_i, i = 1, 2, \cdots, n$，一般这里的 $n$ 是比较大的数，如 $10^4, 10^5$ 等；

(2) 考察这 $n$ 对数据 $(x_i, y_i), i = 1, 2, \cdots, n$，记录满足不等式 $y_i \leq f(x_i)$ 的次数 $\mu_n$，那么 $\frac{\mu_n}{n}$ 即为事件 $A$ 发生的频率，于是 $I \approx \frac{\mu_n}{n}$（如图 5.1.1 所示）。

这种做法其实就是将 $(X,Y)$ 看成正方形 $\{(x,y): 0 \leq x \leq 1, 0 \leq y \leq 1\}$ 的随机点，用随机点落在区域 $\{(x,y): y \leq f(x)\}$ 中的频率作为定积分 $I = \int_{0}^{1} f(x)dx$ 的近似值，所以此法也称为随机投点法。

在现实生活中，人们常常用多个观测值的平均来作为某个考察指标的一个估计。譬如：考察某大学学生的平均身高，一般的做法是随机抽取一些学生，比方说 1000 个学生，以这 1000 个学生的平均身高作为该大学学生平均身高的一个近似，这样做的理论依据其实就是辛钦大数定律。

**定理 5.1.4** （辛钦（Khinchin）大数定律）设 $\{X_i, i \geq 1\}$ 为独立同分布的随机变量序列，且数学期望存在，记为 $\mu$，则对任意的 $\varepsilon > 0$，有

$$
\lim_{n \to +\infty} P \left\{ \left| \frac{1}{n} \sum_{i=1}^{n} X_i - \mu \right| \geq \varepsilon \right\} = 0,
$$

即 $\frac{1}{n} \sum_{i=1}^{n} X_i \overset{P}{\longrightarrow} \mu (n \to +\infty)$，并认为此时随机变量序列 $\{X_i, i \geq 1\}$ 服从大数定律。

该定理的证明需要用到随机变量的特征函数，这里就不介绍了（参阅参考文献 [2]）。

辛钦大数定律不要求随机变量的方差存在，在实际应用相当广泛。事实上，它为寻求随机变量数学期望的近似提供了一个理论保证。因为若对随机变量 $X$ 独立重复观察 $n$ 次，则这 $n$ 次的结果 $X_1, X_2, \cdots, X_n$ 应该相互独立，而且都与 $X$ 同分布。那么不管 $X$ 的分布是什么，只要 $X$ 的数学期望存在，利用辛钦大数定律，就可以用平均观察结果 $\frac{1}{n} \sum_{i=1}^n X_i$ 来近似表示 $E(X)$，而且辛钦大数定律也为数理统计中的矩估计提供了一定的理论依据。

注意到当 $\{X_i, i \geq 1\}$ 为独立同分布的随机变量序列时，若 $h(x)$ 为一连续函数，则 $\{h(X_i), i \geq 1\}$ 也是独立同分布的。因此由辛钦大数定律可以得到以下推论。

**推论 5.1.1** 设 $\{X_i, i \geq 1\}$ 为独立同分布的随机变量序列，若 $h(x)$ 为一连续函数，且 $E(|h(X_1)|) < +\infty$，则对任意的 $\varepsilon > 0$，有

$$
\lim_{n \to +\infty} P \left\{ \left| \frac{1}{n} \sum_{i=1}^n h(X_i) - a \right| \geq \varepsilon \right\} = 0,
$$

其中 $a = E(h(X_1))$，即 $\frac{1}{n} \sum_{i=1}^n h(X_i) \xrightarrow{P} a, n \to +\infty$。

**例 5.1.5** 设随机变量序列 $\{X_i, i \geq 1\}$ 独立同分布，且对某 $k \geq 1$，有 $E(|X_1|^k) < +\infty$，证明：

$$
\frac{1}{n} \sum_{i=1}^n X_i^k \xrightarrow{P} E(X_1^k), \quad n \to +\infty.
$$

**证明** 由于对某 $k \geq 1$，有 $E(|X_1|^k) < +\infty$，故 $E(X_1^k)$ 存在。而 $\{X_i, i \geq 1\}$ 同分布，所以对于任意的 $i \geq 1$，$E(X_i^k) = E(X_1^k)$。取 $h(x) = x^k$，则此函数为一连续函数。由推论 5.1.1，可知

$$
\frac{1}{n} \sum_{i=1}^n X_i^k \xrightarrow{P} E(h(X_1)) = E(X_1^k), \quad n \to +\infty.
$$

**例 5.1.6** 设随机变量序列 $\{X_i, i \geq 1\}$ 独立同分布，且 $\text{Var}(X_1) = \sigma^2$ 存在。令

$$
\overline{X}_n = \frac{1}{n} \sum_{i=1}^n X_i, \quad S_n^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X}_n)^2,
$$

证明：

$$
S_n^2 \xrightarrow{P} \sigma^2, \quad n \to +\infty.
$$

**证明** 由于 $\text{Var}(X_1) = \sigma^2$ 存在，故 $E(X_1)$ 也存在，记其为 $\mu$。注意到

$$
S_n^2 = \frac{1}{n-1} \left( \sum_{i=1}^n X_i^2 - n \overline{X}_n^2 \right),
$$

在例 5.1.5 中取 $k = 1$，则 $\overline{X}_n \overset{P}{\longrightarrow} \mu, n \to +\infty$。

由 (5.1.3) 式知，$\overline{X}_n^2 \overset{P}{\longrightarrow} \mu^2, n \to +\infty$。再一次利用例 5.1.5，取 $k = 2$，则

$$
\frac{1}{n} \sum_{i=1}^n X_i^2 \overset{P}{\longrightarrow} E(X_1^2), n \to +\infty.
$$

而 $E(X_1^2) - \mu^2 = \text{Var}(X_1) = \sigma^2$，所以

$$
S_n^2 = \frac{n}{n-1} \left( \frac{1}{n} \sum_{i=1}^n X_i^2 - \overline{X}_n^2 \right) \overset{P}{\longrightarrow} \sigma^2, n \to +\infty.
$$

**例 5.1.7** 设 $\{X_i, i \geq 1\}$ 为一独立同分布的随机变量序列，$X_1 \sim U(0,1)$。

(1) 对任意的 $k = 1, 2, \cdots$，$\{X_i^k\}$ 满足大数定律吗？若满足，求出其极限值；若不满足，说明理由；

(2) 当 $n \to +\infty$ 时，$\sqrt[n]{X_1 X_2 \cdots X_n}$ 依概率收敛吗？若收敛，给出收敛的极限值，否则请说明理由。

**解** (1) 由于 $X_1, X_2, \cdots$ 独立同分布，故对任意的 $k = 1, 2, \cdots$，$X_1^k, X_2^k, \cdots$ 也相互独立，而且分布相同。又 $X_1 \sim U(0,1)$，故 $E(X_1^k)$ 存在，且

$$
E(X_1^k) = \int_0^1 x^k \cdot 1 dx = \frac{1}{k+1}.
$$

根据辛钦大数定律，对任意的 $\varepsilon > 0$，

$$
\lim_{n \to +\infty} P \left\{ \left| \frac{1}{n} \sum_{i=1}^n X_i^k - \frac{1}{k+1} \right| \geq \varepsilon \right\} = 0,
$$

即 $\{X_i^k\}$ 满足大数定律，且

$$
\frac{1}{n} \sum_{i=1}^n X_i^k \overset{P}{\longrightarrow} \frac{1}{k+1}, n \to +\infty.
$$

事实上，若取 $h(x) = x^k$，直接利用推论 5.1.1 也可得到结论。

(2) 记 $Y_n = \sqrt[n]{X_1 X_2 \cdots X_n}$，$Z_n = \ln Y_n = \frac{1}{n} (\ln X_1 + \ln X_2 + \cdots + \ln X_n)$。

由于 $X_1, X_2, \cdots, X_n$ 独立同分布，故 $\ln X_1, \ln X_2, \cdots, \ln X_n$ 也独立同分布，且

$$
E(\ln X_1) = \int_0^1 \ln x dx = -1.
$$

根据辛钦大数定律，得

$$
Z_n \overset{P}{\longrightarrow} -1, n \to +\infty.
$$

取 $g(x) = e^x$，利用 (5.1.3) 式，有

$$
Y_n = e^{Z_n} \overset{P}{\longrightarrow} e^{-1}, \quad n \to +\infty.
$$



## 5.2 中心极限定理

自从高斯在研究测量误差时导出了正态分布，人们在以后的生活和实践中越来越意识到正态分布的常见性和重要性。这不仅因为很多随机变量的分布是正态分布，还由于现实世界中许多研究对象是受大量的相互独立的随机因素影响着，而其中每一个个别因素在总的影响中所起的作用都微乎其微，这样的对象往往就近似地服从正态分布，这就是中心极限定理的客观背景。粗略而言，中心极限定理主要描述了大量的随机变量之和的分布可用正态分布来逼近。

最早的中心极限定理是关于 $n$ 重伯努利试验的。早在 18 世纪初期，棣莫弗就事件发生的概率 $p = \frac{1}{2}$ 时证明了二项分布的极限分布为正态分布。此后，拉普拉斯和李雅普诺夫等人改进了他的证明，并把二项分布推广到更为一般的分布。到了 20 世纪二三十年代，林德伯格条件和费勒条件的提出及特征函数理论的系统化，更是促进了中心极限定理的蓬勃发展。

中心极限定理（central limit theorem, 常简写为 CLT）这一名称是 1920 年由波利亚给出的。至今，学者们已得到了多种情形下的中心极限定理，在本节中我们仅仅列出其中几个最基本的结果。

### （一）独立同分布情形

**定理 5.2.1** （林德伯格（Lindeberg）-莱维（Levy）中心极限定理）设 $\{X_i, i \geq 1\}$ 为独立同分布的随机变量序列，且数学期望 $E(X_i) = \mu$ 和方差 $\text{Var}(X_i) = \sigma^2$ 均存在 ($\sigma > 0$)，则对任意的 $x \in \mathbb{R}$，有

$$
\lim_{n \to +\infty} P \left\{ \frac{\sum_{i=1}^n X_i - n\mu}{\sigma\sqrt{n}} \leq x \right\} = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{x} e^{-\frac{t^2}{2}} dt = \Phi(x). \tag{5.2.1}
$$

定理 5.2.1 也称为独立同分布的中心极限定理。这也就是说，数学期望为 $\mu$，方差为 $\sigma^2$ 的独立同分布的随机变量的部分和 $\sum_{i=1}^n X_i$ 的标准化变量 $\frac{\sum_{i=1}^n X_i - n\mu}{\sigma\sqrt{n}}$，当 $n$ 充分大时，近似地服从标准正态分布 $N(0,1)$，即

$$
\frac{\sum_{i=1}^n X_i - n\mu}{\sigma\sqrt{n}} \ \text{近似地} \ N(0,1), \ \text{当} \ n \ \text{充分大时}. \tag{5.2.2}
$$

显然，上式也可以表示为

$$
\frac{\frac{1}{n} \sum_{i=1}^n X_i - \mu}{\sigma/\sqrt{n}} \ \text{近似地} \ N(0,1), \ \text{当} \ n \ \text{充分大时}. \tag{5.2.3}
$$

**例 5.2.1** 某宴会提供一瓶 6 000 mL 的法国红酒，假定与会者每次所倒的红酒量服从同一分布，数学期望为 100 mL，标准差为 32 mL。若每次所倒的红酒量是相互独立的，问倒了 55 次后该瓶红酒仍有剩余的概率为多少？

设 $X_i$ 为第 $i$ 次所倒的红酒量（单位：mL），$i = 1, 2, \cdots, 55$，则 $X_1, X_2, \cdots, X_{55}$ 相互独立。对任意的 $i = 1, 2, \cdots, 55$，$X_i$ 的分布相同，且 $E(X_i) = 100$，$\text{Var}(X_i) = 32^2$。由定理 5.2.1，知

$$
\frac{\sum_{i=1}^{55} X_i - 55 \times 100}{32\sqrt{55}} \ \text{近似地} \ N(0,1).
$$

所以

$$
P\{\text{倒了 55 次后该瓶红酒仍有剩余}\} = P\left\{ \sum_{i=1}^{55} X_i < 6 000 \right\} = P\left\{ \frac{\sum_{i=1}^{55} X_i - 55 \times 100}{32\sqrt{55}} < \frac{6 000 - 55 \times 100}{32\sqrt{55}} \right\} \approx \Phi \left( \frac{6 000 - 55 \times 100}{32\sqrt{55}} \right) = \Phi(2.11) = 0.9826.
$$

**例 5.2.2** 某福利彩票每周开奖三次，每次奖金金额 $X$（单位：万元）随机产生，其概率分布律为

| $X$/万元 | 5    | 10   | 20   | 50   | 100  |
|||||||
| $p$      | 0.35 | 0.3  | 0.2  | 0.1  | 0.05 |

彩票收入用于福利事业，不作为奖金，故为了开奖，需储备一定的奖金。问一年（52周）需要储备多少奖金总额，才能至少有95%的把握发放奖金。

由题意知，一年共发放奖金156次，设 $X_i$ 为第 $i$ 次发放的奖金金额，则 $\{X_i, 1 \leq i \leq 156\}$ 独立同分布，且

$$
E(X_1) = 18.75, \quad E(X_1^2) = 868.75, \quad \text{Var}(X_1) = 517.1875.
$$

设一年储备奖金总额为 $m$ 万元，则

$$
P\left\{\sum_{i=1}^{156} X_i \leq m\right\} \geq 95\%.
$$

由定理 5.2.1 知

$$
P\left\{\sum_{i=1}^{156} X_i \leq m\right\} \approx \Phi\left(\frac{m - 156 \times 18.75}{\sqrt{156 \times 517.1875}}\right) \geq 95\%.
$$

而 $\Phi(1.645) = 0.95$，故只需

$$
\frac{m - 156 \times 18.75}{\sqrt{156 \times 517.1875}} \geq 1.645,
$$

解得 $m \geq 3 \, 392.26$。因此，一年约需储备 3 393 万元，才能有 95% 的把握发放奖金。

将定理 5.2.1 应用到 $n$ 重伯努利试验中，可得如下推论。

**推论 5.2.1** （棣莫弗（De Moivre）-拉普拉斯（Laplace）中心极限定理）设 $n_A$ 为在 $n$ 重伯努利试验中事件 $A$ 发生的次数，$p$ 为事件 $A$ 在每次试验中发生的概率，即 $P(A) = p(0 < p < 1)$，则对任意的 $x \in \mathbb{R}$，有

$$
\lim_{n \to +\infty} P\left\{\frac{n_A - np}{\sqrt{np(1-p)}} \leq x\right\} = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^x e^{-\frac{t^2}{2}} dt = \Phi(x).
$$

**证明** 引入随机变量

$$
X_i = 
\begin{cases} 
1, & \text{第 } i \text{ 次试验中事件 } A \text{ 发生}, \\ 
0, & \text{第 } i \text{ 次试验中事件 } A \text{ 不发生},
\end{cases} \quad i = 1, 2, \cdots, n,
$$

易见 $n_A = \sum_{i=1}^n X_i$，并且 $X_1, X_2, \cdots, X_n$ 相互独立，都服从参数为 $p$ 的 $0-1$ 分布。从而

$$
E(X_i) = p, \quad \text{Var}(X_i) = p(1-p), \quad i = 1, 2, \cdots, n.
$$

由定理 5.2.1 知结论成立。

棣莫弗-拉普拉斯中心极限定理表明，当 $n$ 充分大时，二项分布 $B(n, p)$ 可用正态分布 $N(np, np(1-p))$ 来逼近。

**例 5.2.3** 将一颗骰子抛掷 11 520 次，共出现了 2 160 次“1 点”，由此可否断言此骰子不均匀？

记 $n_A$ 为骰子抛掷 11 520 次时出现“1 点”的次数。假设骰子是均匀的，则 $n_A \sim B \left( 11 \, 520, \frac{1}{6} \right)$。由于抛掷次数 $n = 11 \, 520$ 足够多，由棣莫弗-拉普拉斯中心极限定理可知

$$
\frac{n_A - 11 \, 520 \times \frac{1}{6}}{\sqrt{11 \, 520 \times \frac{1}{6} \times \frac{5}{6}}} = \frac{n_A - 1 \, 920}{40} \ \text{近似地} \ N(0,1).
$$

于是

$$
P\{n_A \geq 2 \, 160\} \approx 1 - \Phi \left( \frac{2 \, 160 - 1 \, 920}{40} \right) = 1 - \Phi(6) \approx 0.
$$

也就是说，若骰子是均匀的，则几乎不可能出现“1 点”高达 2 160 次。根据实际推断原理可以判断骰子是不均匀的。

**例 5.2.4** 某校 1 500 名学生选修“概率论与数理统计”课程，共有 10 名教师主讲此课，假定每位学生可以随机选择一位教师（即选择任意一位教师的可能性均为 $\frac{1}{10}$），而且学生之间的选择是相互独立的。问：每位教师的上课教室应该设多少座位才能保证该教室因没有座位而使学生离开的概率小于 5%。

由于每位学生可以随机选择一位教师，所以我们只需要考虑某个教师甲的上课教室的座位数即可。引入随机变量

$$
X_i = 
\begin{cases} 
1, & \text{第 } i \text{ 个学生选择教师甲}, \\ 
0, & \text{其他},
\end{cases}
\quad i = 1, 2, \cdots, 1 \, 500,
$$

则 $X_i$ 独立同分布，均服从参数为 $\frac{1}{10}$ 的 $0-1$ 分布。记 $Y = \sum_{i=1}^{1 \, 500} X_i$，则 $Y$ 为选择教师甲的学生数，且 $Y \sim B \left( 1 \, 500, \frac{1}{10} \right)$。利用棣莫弗-拉普拉斯中心极限定理，知

$$
\frac{Y - \frac{1 \, 500}{10}}{\sqrt{\frac{9}{100}} \times \sqrt{1 \, 500}} \ \text{近似地} \ N(0,1).
$$

如果教室需设 $a$ 个座位，为使学生不因没有座位而离开教室，就需 $Y \leq a$。由题意知需要满足

$$
95\% \leq P\{Y \leq a\} \approx \Phi \left( \frac{a - \frac{1 \, 500}{10}}{\sqrt{\frac{9}{100}} \times \sqrt{1 \, 500}} \right).
$$

查附表2得 $\Phi(1.645) = 0.95$，故需 $\frac{a - 150}{\sqrt{135}} \geq 1.645$，即 $a \geq 169.11$。故每位教师的上课教室应该至少设 170 个座位才能保证因没有座位而使学生离开的概率小于 5%。

**例 5.2.5** 某市为了了解市民对公共自行车服务的满意率 $p, 0 < p < 1$，特意委托某调查公司进行调查。该调查公司随机抽取调查对象，并将调查对象中对该服务满意的频率作为 $p$ 的估计 $\hat{p}$。现要保证至少有 95% 的把握使真实满意率 $p$ 与调查所得的满意率估计 $\hat{p}$ 之间的差异小于 10%，问至少需要多少个调查对象？

设随机调查了 $n$ 个对象，记

$$
X_i = 
\begin{cases}
1, & \text{第 } i \text{ 个调查对象对该服务满意}, \\
0, & \text{第 } i \text{ 个调查对象对该服务不满意},
\end{cases}
\quad i = 1, 2, \cdots, n,
$$

则 $X_i$ 独立同分布，均服从参数为 $p$ 的 $0-1$ 分布，$E(X_i) = p$，$\text{Var}(X_i) = p(1-p)$，$i = 1, 2, \cdots, n$。记 $Y_n$ 为 $n$ 个调查对象中对该服务满意的人数，则

$$
Y_n = \sum_{i=1}^{n} X_i, \quad \hat{p} = \frac{Y_n}{n} = \frac{\sum_{i=1}^{n} X_i}{n}.
$$

而利用 (5.2.3) 或棣莫弗-拉普拉斯中心极限定理，可以得到

$$
\frac{\frac{1}{n} \sum_{i=1}^{n} X_i - p}{\sqrt{p(1-p)/n}} \ \text{近似地} \ N(0,1).
$$

由题意知需满足

$$
95\% \leq P \left\{ \left| \frac{1}{n} \sum_{i=1}^{n} X_i - p \right| < 10\% \right\} \approx 2\Phi \left( \frac{0.1\sqrt{n}}{\sqrt{p(1-p)}} \right) - 1,
$$

即

$$
\Phi \left( \frac{0.1\sqrt{n}}{\sqrt{p(1-p)}} \right) \geq 0.975.
$$

查附表2得 $\Phi(1.96) = 0.975$，从而需

$$
n \geq p(1-p) \frac{1.96^2}{0.1^2} = 384.16p(1-p).
$$

由于对任意的 $0 < p < 1$，有 $0 < p(1-p) \leq \frac{1}{4}$，所以 $n \geq 96.04$，即至少需要 97 个调查对象。

此题如果用切比雪夫不等式来解答，结果又会怎样呢？

由于

$$
E \left( \frac{1}{n} \sum_{i=1}^{n} X_i \right) = p, \quad \text{Var} \left( \frac{1}{n} \sum_{i=1}^{n} X_i \right) = \frac{p(1-p)}{n},
$$

故由切比雪夫不等式可知

$$
P \left\{ \left| \frac{1}{n} \sum_{i=1}^{n} X_i - p \right| < 10\% \right\} \geq 1 - \frac{p(1-p)/n}{0.1^2}.
$$

若

$$
1 - \frac{p(1-p)/n}{0.1^2} \geq 95\% \tag{5.2.5}
$$

成立，则一定可以保证

$$
P \left\{ \left| \frac{1}{n} \sum_{i=1}^{n} X_i - p \right| < 10\% \right\} \geq 95\%.
$$

而要使 (5.2.5) 成立，需有

$$
n \geq \frac{p(1-p)}{0.05 \cdot 0.1^2} = 2000p(1-p).
$$

同样由于对任意的 $0 < p < 1$，有 $0 < p(1-p) \leq \frac{1}{4}$，所以 $n \geq 500$。即利用切比雪夫不等式，我们得到的解答是：至少需要 500 个调查对象。在实际应用中，计算事件的概率值最好是精确结果，其次是近似结果，最后才考虑用上（下）界。



### *（二）独立不同分布情形

**定理 5.2.2** （李雅普诺夫（Lyapunov）中心极限定理）设 $\{X_i, i \geq 1\}$ 为相互独立的随机变量序列，其数学期望 $E(X_i) = \mu_i$，方差 $\text{Var}(X_i) = \sigma_i^2 (\sigma_i > 0)$，$i = 1, 2, \cdots$。如果存在 $\varepsilon > 0$，使

$$
\lim_{n \to +\infty} \frac{1}{B_n^{2+\varepsilon}} \sum_{i=1}^n E|X_i - \mu_i|^{2+\varepsilon} = 0, \tag{5.2.6}
$$

其中 $B_n^2 = \sum_{i=1}^n \sigma_i^2$，那么对于任意的 $x \in \mathbb{R}$，有

$$
\lim_{n \to +\infty} P\left\{ \frac{1}{B_n} \sum_{i=1}^n (X_i - \mu_i) \leq x \right\} = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{x} e^{-\frac{t^2}{2}} dt = \Phi(x).
$$





## **思考题五**

1. 依概率收敛与高等数学中的收敛含义有何区别？  
2. 马尔可夫不等式与切比雪夫不等式分别适用于哪些随机变量？  
3. 说明大数定律与中心极限定理的联系与区别。  
4. 对例 5.2.5 而言，为什么利用中心极限定理与切比雪夫不等式得到的结论有所差异？


## **习题五（A类）**

**A1.**  
设 $X_1, X_2, \cdots, X_n, \cdots$ 相互独立，均服从参数为 2 的指数分布。问当 $n \to +\infty$ 时，  
$$
\frac{1}{n} \sum_{i=1}^n X_i^2
$$  
依概率收敛于何值？

**A2.**  
设随机变量序列 \(\{X_n, n \geq 1\}\)，当 $n \to +\infty$ 时，$X_n$ 依概率收敛于 3。问当 $n \to +\infty$ 时，下列随机变量序列依概率收敛于何值：  
(1) $X_n^2$；  
(2) $2X_n - 3$。

**A3.**  
设 $X_1$ 与 $X_2$ 相互独立，均值都为 2，方差都为 4，用切比雪夫不等式求  
$$
P\{|X_1 - X_2| \geq 4\}
$$  
的上界。

**A4.**  
设 $X_1, X_2, \cdots, X_{315}$ 独立同分布，$X_1$ 的密度函数为  
$$
f(x) = 
\begin{cases} 
\frac{2}{3}x, & 1 < x < 2, \\
0, & \text{其他}.
\end{cases}
$$  
Y 表示 \(\{X_i < 1.5\}\)（$i = 1, 2, \cdots, 315$）出现的个数，求 $P\{Y < 140\}$ 的近似值。

**A5.**  
设 $X_1, X_2, \cdots, X_{240}$ 独立同分布，  
$$
P\{X_1 = -2\} = 0.3, \quad P\{X_1 = 0\} = 0.4, \quad P\{X_1 = 2\} = 0.3。
$$  
令 $Y = X_1 + X_2 + \cdots + X_{240}$，求 $P\{|Y| > 24\}$ 的近似值。


## **习题五（B类）**

**B1.**  
某种类的昆虫每周产卵数为随机变量 $X$（以个计），若已知其平均周产卵数为 36 个。  
(1) 用马尔可夫不等式求一周内该昆虫产卵数不少于 50 个的概率的上界；  
(2) 若又已知该昆虫每周产卵数的标准差为 2 个，用切比雪夫不等式求一周内产卵数在 \((32,40)\) 内的概率的下界。

**B2.**  
一种遗传病的隔代发病率为 10%，在得病家庭中选取 500 户进行研究，试用切比雪夫不等式估计这 500 户中隔代发病的比例与发病率之差的绝对值小于 5% 的概率下界。

**B3.**  
设随机变量 $X_i$ 的密度函数为  
$$
f_i(x) = 
\begin{cases} 
\frac{i|x|^{i-1}}{2}, & |x| \leq 1, \\ 
0, & \text{其他},
\end{cases}
\quad i=1,2,\cdots,n。
$$  
且 $X_1, X_2, \cdots, X_n$ 相互独立。令 $Y_n = X_1 X_2 \cdots X_n$，用切比雪夫不等式求使  
$$
P\left\{|Y_n| \geq \frac{1}{2}\right\} \leq \frac{1}{9}
$$  
成立的最小的 $n$。

**B4.**  
设随机变量序列 \(\{X_n, n \geq 1\}\) 独立同分布，都服从 $U(0,a)$，其中 $a > 0$。令  
$$
X_{(n)} = \max_{1 \leq i \leq n} X_i，
$$  
证明：  
$$
X_{(n)} \overset{P}{\longrightarrow} a, \quad n \to +\infty。
$$

**B5.**  
设随机变量序列 \(\{X_i, i \geq 1\}\) 独立同分布，数学期望与方差均存在，证明：  
$$
\frac{2}{n(n+1)} \sum_{i=1}^{n} i \cdot X_i \overset{P}{\longrightarrow} E(X_1), \quad n \to +\infty。
$$

**B6.**  
设 \(\{X_i, i \geq 1\}\) 为独立同分布的正态随机变量序列，若 $X_1 \sim N(\mu, \sigma^2)$，其中 $\sigma > 0$。问以下的随机变量序列当 $n \to +\infty$ 时依概率收敛吗？若收敛，请给出收敛的极限值，否则请说明理由：  
$$
(1) \frac{1}{n} \sum_{i=1}^{n} X_i^2; \quad (2) \frac{1}{n} \sum_{i=1}^{n} (X_i - \mu)^2;
$$  
$$
(3) \frac{X_1 + X_2 + \cdots + X_n}{X_1^2 + X_2^2 + \cdots + X_n^2}; \quad (4) \frac{X_1 + X_2 + \cdots + X_n}{\sqrt{n} \sum_{i=1}^{n} (X_i - \mu)^2}。
$$

**B7.**  
设随机变量序列 \(\{X_i, i \geq 1\}\) 独立同分布，都服从期望为 $\frac{1}{\lambda}$ 的指数分布，其中 $\lambda > 0$。  
(1) 若对任意的 $\varepsilon > 0$，均有  
$$
\lim_{n \to +\infty} P\left\{ \left| \frac{X_1^2 + X_2^2 + \cdots + X_n^2}{n} - a \right| < \varepsilon \right\} = 1
$$  
成立，求 $a$；  
(2) 给出  
$$
\frac{1}{50} \sum_{i=1}^{100} X_i
$$  
的近似分布；  
(3) 求  
$$
P\left\{ \frac{1}{100} \sum_{i=1}^{100} X_i^2 \leq \frac{2}{\lambda^2} \right\}
$$  
的近似值。

**B8.**  
抛掷一枚硬币 10 000 次，出现了 5 325 次“正面”，是否可以断言此硬币是不均匀的？

**B9.**  
设随机变量 $X$ 服从辛普森分布（三角分布），密度函数为  
$$
f(x) = 
\begin{cases} 
x, & 0 \leq x < 1, \\
2 - x, & 1 \leq x < 2, \\
0, & \text{其他}。
\end{cases}
$$  
(1) 对 $X$ 进行 100 次独立观察，事件 \(\{0.95 < X < 1.05\}\) 出现的次数记为 $Y$，试用三种方法（\(Y\) 的精确分布、用泊松分布来作为 \(Y\) 的近似分布、中心极限定理）分别求 $P(Y > 2)$；  
(2) 要保证至少有 95% 的把握使事件 \(\left\{ \frac{1}{2} < X < \frac{3}{2} \right\}\) 出现的次数不少于 80，问至少需要进行多少次观察？

**B10.**  
某企业庆祝百年华诞，邀请了一些社会名流及企业的相关人士来参加庆典，被邀请人独自一人或携伴（一位同伴）出席，也有可能因故缺席，这三种情况出现的概率分别为 0.3, 0.5, 0.2。若此次庆典事先发出了 800 份邀请函，若每位被邀请人参加庆典的行为相互独立，问有超过十人出席该庆典的可能性有多大？

**B11.**  
某次“知识竞赛”规则如下：参赛选手最多可抽取 3 个相互独立的问题一一回答；如果答错就被淘汰，进而失去回答下一题的资格；每答对一题得 1 分，若 3 题都对则再加 1 分（即共得 4 分）。现有 100 名参赛选手，每人独立答题。  
(1) 若每人至少答对一题的概率为 0.7，用中心极限定理计算“最多有 35 人得 0 分”的概率；  
(2) 若题目的难易程度类似，每人答对每题的概率均为 0.8，求这 100 名参赛选手的总分超过 220 分的概率。
