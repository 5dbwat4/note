---
title: "Lab 2: 向量化计算"
---
import {NImage} from "naive-ui";

# 使用 AVX 指令集实现整数矩阵乘法

Naive version:

```cpp
void naive_gemm(uint8_t* A, int8_t* B, uint32_t* C, int m, int n, int k) {
    for (int i = 0; i < m; i++) {
        for (int j = 0; j < n; j++) {
            for (int kk = 0; kk < k * 4; kk++) {
                C[i * k + j] += A[i * k * 4 + kk] * B[j * k * 4 + kk];
            }
        }
    }
}
```

参考课上所讲，我们首先实现了一个基本的做了分块操作的矩阵乘法：

```cpp
// 67mm
for (int i = 0; i < M; i++) {
    for (int j = 0; j < N; j += 16) {
        __m512i acc = _mm512_setzero_epi32();

        for (int k = 0; k < K; k++) {
            uint32_t a_val = *reinterpret_cast<uint32_t*>(A + i * K * 4 + k * 4);
            __m512i a_vec = _mm512_set1_epi32(a_val);

            __m512i b_vec = _mm512_loadu_si512(
                reinterpret_cast<const __m512i*>(B_reshape + k * N * 4 + j * 4)
            );

            acc = _mm512_dpbusd_epi32(acc, a_vec, b_vec);
        }

        _mm512_storeu_si512(reinterpret_cast<__m512i*>(C + i * N + j), acc);
    }
}
```

大致思路是：  
1. **分块处理**：将矩阵分成小块（`M x 16`），每次处理一行和多列。
2. **使用 AVX 指令**：利用 `_mm512_setzero_epi32` 初始化累加器，使用 `_mm512_set1_epi32` 和 `_mm512_loadu_si512` 加载数据。
3. **点积操作**：使用 `_mm512_dpbusd_epi32` 进行点积计算，将结果累加到累加器中。



存在的主要问题是：  
1. **指令级并行度不足**：
   - 单累加器导致严重的指令依赖链
   - CPU流水线无法充分利用

2. **内存访问效率低**：
   - A矩阵元素重复广播（每次K迭代广播1次）
   - B矩阵访问跨度大（k * N * 4），缓存局部性差

3. **循环开销大**：
   - K维循环无展开，分支预测开销高
   - 地址计算未优化（每次k迭代重复计算）

进一步优化的核心就是**减少内存访问，以及增加指令的并行程度**

所以我们有了这个版本：

```cpp
// 46mm
int8_t* B_ptr0 = B_reshape;
int8_t* B_ptr1 = B_reshape + 64;
for (int i = 0; i < M; i++) {
     
    __m512i acc0 = _mm512_setzero_epi32();
    __m512i acc1 = _mm512_setzero_epi32();
    __m512i acc2 = _mm512_setzero_epi32();
    __m512i acc3 = _mm512_setzero_epi32();
    __m512i acc4 = _mm512_setzero_epi32();
    __m512i acc5 = _mm512_setzero_epi32();
    __m512i acc6 = _mm512_setzero_epi32();
    __m512i acc7 = _mm512_setzero_epi32();
    
    // 指针预计算减少地址计算开销
    uint8_t* A_ptr = A + i * K * 4;

    
    // 展开K循环4次
    for (int k = 0; k < K; k += 4) {
        // 使用广播指令加载A向量
        __m128i a_pack = _mm_loadu_si128((const __m128i*)(A_ptr + k * 4));
        __m512i a_vec0 = _mm512_broadcastd_epi32(_mm512_castsi512_si128(_mm512_broadcastd_epi32(a_pack)));
        __m512i a_vec1 = _mm512_broadcastd_epi32(_mm512_castsi512_si128(_mm512_broadcastd_epi32(_mm_srli_si128(a_pack, 4))));
        __m512i a_vec2 = _mm512_broadcastd_epi32(_mm512_castsi512_si128(_mm512_broadcastd_epi32(_mm_srli_si128(a_pack, 8))));
        __m512i a_vec3 = _mm512_broadcastd_epi32(_mm512_castsi512_si128(_mm512_broadcastd_epi32(_mm_srli_si128(a_pack, 12))));
        
        // 加载B矩阵数据
        __m512i b_vec00 = _mm512_loadu_si512((const __m512i*)(B_ptr0 + k * N * 4));
        __m512i b_vec01 = _mm512_loadu_si512((const __m512i*)(B_ptr0 + (k+1) * N * 4));
        __m512i b_vec02 = _mm512_loadu_si512((const __m512i*)(B_ptr0 + (k+2) * N * 4));
        __m512i b_vec03 = _mm512_loadu_si512((const __m512i*)(B_ptr0 + (k+3) * N * 4));
        
        __m512i b_vec10 = _mm512_loadu_si512((const __m512i*)(B_ptr1 + k * N * 4));
        __m512i b_vec11 = _mm512_loadu_si512((const __m512i*)(B_ptr1 + (k+1) * N * 4));
        __m512i b_vec12 = _mm512_loadu_si512((const __m512i*)(B_ptr1 + (k+2) * N * 4));
        __m512i b_vec13 = _mm512_loadu_si512((const __m512i*)(B_ptr1 + (k+3) * N * 4));
        
        // 使用点积指令进行累加
        acc0 = _mm512_dpbusd_epi32(acc0, a_vec0, b_vec00);
        acc1 = _mm512_dpbusd_epi32(acc1, a_vec1, b_vec01);
        acc2 = _mm512_dpbusd_epi32(acc2, a_vec2, b_vec02);
        acc3 = _mm512_dpbusd_epi32(acc3, a_vec3, b_vec03);
        
        acc4 = _mm512_dpbusd_epi32(acc4, a_vec0, b_vec10);
        acc5 = _mm512_dpbusd_epi32(acc5, a_vec1, b_vec11);
        acc6 = _mm512_dpbusd_epi32(acc6, a_vec2, b_vec12);
        acc7 = _mm512_dpbusd_epi32(acc7, a_vec3, b_vec13);
    }
    
    // 合并累加器结果
    acc0 = _mm512_add_epi32(acc0, acc1);
    acc2 = _mm512_add_epi32(acc2, acc3);
    acc4 = _mm512_add_epi32(acc4, acc5);
    acc6 = _mm512_add_epi32(acc6, acc7);
    
    acc0 = _mm512_add_epi32(acc0, acc2);
    acc4 = _mm512_add_epi32(acc4, acc6);
    
    // 存储结果
    _mm512_storeu_si512((__m512i*)(C + i * N), acc0);
    _mm512_storeu_si512((__m512i*)(C + i * N + 16), acc4);
}
```

这个版本的主要改进点是：
1. **指针预计算**：将 A 和 B 的指针提前计算，减少每次循环中的地址计算开销。
2. **使用广播指令**：使用 `_mm512_broadcastd_epi32` 将 A 的数据广播到 AVX 寄存器中，而不是每一次都要读取 A 的数据，减少了内存访问次数。
3. **展开 K 循环**：将 K 循环展开 4 次，每次处理 4 个元素，减少了循环次数和依赖链，提高指令级并行度。

import ImageP1 from "./assets/lab2-p1.png?url";

<center>
<NImage src={ImageP1} alt="Picture" />

还是相当成功的

</center>

# 使用 AMX 实现整数矩阵乘法

参考[intel/AMX-TMUL-Code-Samples: Code samples related to Intel(R) AMX](https://github.com/intel/AMX-TMUL-Code-Samples)

以及官方文档

import ImageI1 from "./assets/lab2-index-1.svg?url";
import ImageI2 from "./assets/lab2-index-2.svg?url";

<NImage src={ImageI1} alt="Picture" />

<NImage src={ImageI2} alt="Picture" />

我们将使用*TDPBSSD/TDPBSUD/TDPBUSD/TDPBUUD—Dot Product of Signed/Unsigned Bytes with Dword Accumulation*，具体的说是其中的TDPBUSD来实现矩阵乘法。

`tile.h`里面把tile管理写好了，我们可以直接调用：

```cpp
set_tiledata_use ();
init_tile_config (M);
```

在main函数里的核心代码：

```cpp
for (int j = 0; j < N; j += 16) {
    _tile_zero(0);
    for (int k = 0; k < K * 4; k += 64) {
        _tile_loadd(1, A + k, K * 4);
        _tile_loadd(2, B_reshape + j * 4 + k * N, K * 4);
        _tile_dpbusd(0, 1, 2);
    }
    _tile_stored(0, C + j, N * sizeof(uint32_t));
}
```

import ImageP2 from "./assets/lab2-p2.png?url";

<center>
<NImage src={ImageP2} alt="Picture" />

跑出来的结果也是相当领先

</center>

# 思考题

## 双线性插值向量化版本 `vectorized.py`

1. 代码第 21 行 - 第 23 行中的 None 起到什么作用？可以删掉吗，请说明理由？

   **作用：**
   
   在 `numpy` 中，`None` 是 `np.newaxis` 的一个别名(参见文档： [Constants — NumPy v2.3 Manual](https://numpy.org/doc/stable/reference/constants.html#numpy.newaxis))。它的作用是**增加一个新的维度（轴）**。当在数组索引中使用 `None` 时，实际会在那个位置插入一个长度为 1 的新轴。
   
   1.  `x_mul = (x - x_idx)[:,:,:,None]`
       * `x` 和 `x_idx` 的 `shape` 都是 `(N, H2, W2)`。
       * `(x - x_idx)` 的 `shape` 也是 `(N, H2, W2)`。
       * 通过在最后添加 `[:,:,:,None]`，`x_mul` 的 `shape` 变成了 `(N, H2, W2, 1)`。
   
   2.  `y_mul = (y - y_idx)[:,:,:,None]`
       * 与 `x_mul` 同理，`y_mul` 的 `shape` 也从 `(N, H2, W2)` 变成了 `(N, H2, W2, 1)`。
   
   3.  `n_idx = np.arange(N)[:,None,None]`
       * `np.arange(N)` 生成一个 `shape` 为 `(N,)` 的一维数组。
       * 第一个 `[:,None]` 将其 `shape` 变为 `(N, 1)`。
       * 第二个 `[None]`（这里是 `[:,None,None]` 的一部分）将其 `shape` 变为 `(N, 1, 1)`。

   **不可以删掉。**

   ```plain
   Traceback (most recent call last):
     File "/home/hpc101/*[Redacted]*/lab2/numpy_example/main.py", line 31, in <module>
       vectorized_result, vectorized_time = time_function(bilinear_interp_vectorized, a, b)
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
     File "/home/hpc101/*[Redacted]*/lab2/numpy_example/utils/timer.py", line 9, in time_function
       res = f(*args)
             ^^^^^^^^
     File "/home/hpc101/*[Redacted]*/lab2/numpy_example/bilinear_interp/vectorized.py", line 24, in bilinear_interp_vectorized
       res[:] = a[n_idx, x_idx, y_idx] * (1 - x_mul) * (1 - y_mul) + \
                ~^^^^^^^^^^^^^^^^^^^^^
   IndexError: shape mismatch: indexing arrays could not be broadcast together with shapes (8,) (8,720,1280) (8,720,1280)
   ```
   
   ~~理由在于，实践出真知~~

   理由在于，增加这些新维度的目的是为了在后续的计算中（第 24-27 行）能够正确地利用 `numpy` 的**广播（Broadcasting）机制**。
   
   * 原始输入图像 `a` 的 `shape` 是 `[N, H1, W1, C]`。我们最终希望计算出的每个像素值都保留 `C` 这个通道维度。
   * 通过将 `x_mul` 和 `y_mul` 的 `shape` 变为 `(N, H2, W2, 1)`，它们就可以和从 `a` 中索引出的、`shape` 为 `(N, H2, W2, C)` 的数组进行元素级（element-wise）的乘法运算。广播机制会自动将 `shape` 为 `(N, H2, W2, 1)` 的数组“扩展”到 `(N, H2, W2, C)`，以便进行计算。
   * `n_idx` 的 `shape` 变为 `(N, 1, 1)` 是为了配合 `x_idx` 和 `y_idx`（`shape` 均为 `(N, H2, W2)`）进行高级索引（Advanced Indexing），具体将在下一个问题中解释。
   
   如果删掉 `None`，`x_mul` 和 `y_mul` 的 `shape` 将是 `(N, H2, W2)`，在与 `shape` 为 `(N, H2, W2, C)` 的数组相乘时，会因为维度不匹配而出错。

2. 代码第 24 行中 `a[n_idx, x_idx, y_idx]` 的 shape 是什么？这些 idx 在这里是怎么作用的？

   **Shape:**
   
   `a[n_idx, x_idx, y_idx]` 的 `shape` 是 **`(N, H2, W2, C)`**。
   
   **`idx` 的作用（高级索引）：**
   
   这里应用了 `numpy` 的**高级索引（Advanced Indexing）**和**广播（Broadcasting）**相结合的机制。
   
   1.  **索引数组的广播**：在进行索引之前，`numpy` 会首先对传入的索引数组（`n_idx`, `x_idx`, `y_idx`）进行广播。
       * `n_idx`: `shape` 是 `(N, 1, 1)`
       * `x_idx`: `shape` 是 `(N, H2, W2)`
       * `y_idx`: `shape` 是 `(N, H2, W2)`
   
       根据广播规则，`n_idx` 会被扩展成 `(N, H2, W2)`。因此，这三个索引数组共同作用时，等效于提供了 `N * H2 * W2` 组索引坐标。
   
   2.  **元素选取**：`numpy` 会从 `a` 数组中选取元素。对于输出数组中的每一个位置 `(i, j, k)`（其中 `0 <= i < N`, `0 <= j < H2`, `0 <= k < W2`），它会找到对应的索引值：
       * `n = n_idx[i, 0, 0]` (广播后等效于 `n_idx[i, j, k]`)
       * `x = x_idx[i, j, k]`
       * `y = y_idx[i, j, k]`
   
       然后，它会从 `a` 中取出 `a[n, x, y]` 这个位置的元素。由于我们只对前三个维度（N, H, W）进行了索引，所以第四个维度 `C` (通道)被完整地保留了下来。
   
   3.  **构建输出**：最终，`numpy` 将所有取出的 `(C,)` 向量组合起来，形成一个 `shape` 为 `(N, H2, W2, C)` 的新数组。这个新数组的 `[i, j, k, :]` 位置的值，就是从 `a[n_idx[i,j,k], x_idx[i,j,k], y_idx[i,j,k], :]` 取出的值。
   
   简单来说，`n_idx`, `x_idx`, `y_idx` 共同指定了要从原始图像 `a` 的哪个批次（N）、哪个 x 坐标、哪个 y 坐标去拾取像素值（一个包含 `C` 个通道值的向量）。

3. 代码第 24 行中的向量乘法中这三个向量的维度相同吗？如果不同的话，是怎么广播的？

   我们以第 24 行的第一个乘法项 `a[n_idx, x_idx, y_idx] * (1 - x_mul) * (1 - y_mul)` 为例来分析。
   
   **维度不同。**
   
   这三个数组（或称为张量/向量）的 `shape` 分别是：
   
   1.  `a[n_idx, x_idx, y_idx]`: `shape` 是 `(N, H2, W2, C)`
   2.  `(1 - x_mul)`: `shape` 是 `(N, H2, W2, 1)`
   3.  `(1 - y_mul)`: `shape` 是 `(N, H2, W2, 1)`
   
   **广播机制：**

   参考文档：[Broadcasting — NumPy v2.3 Manual](https://numpy.org/doc/stable/user/basics.broadcasting.html#broadcasting)
   
   `numpy` 的广播规则允许在执行元素级运算时，自动扩展维度较小的数组，使其维度与维度较大的数组相匹配。规则如下（从尾部维度开始比较）：
   
   1.  如果维度大小相同，则继续比较下一个维度。
   2.  如果其中一个数组的维度大小为 1，则将其“拉伸”或“复制”以匹配另一个数组的维度大小。
   3.  如果维度大小不同且不为 1，则会出错。
   
   例如， `a[...] * (1 - x_mul)` 的广播过程：
   
   * 比较最后一个维度：`C` vs `1`。` (1 - x_mul)` 的最后一个维度是 1，所以它会被扩展 `C` 次，以匹配 `a[...]` 的 `shape`。
   * 比较其他维度：`W2` vs `W2`, `H2` vs `H2`, `N` vs `N`。它们都相同。
   
   因此，` (1 - x_mul)`（`shape` 为 `(N, H2, W2, 1)`）被广播成了 `(N, H2, W2, C)` 的 `shape`。同理，` (1 - y_mul)` 也被广播成了 `(N, H2, W2, C)` 的 `shape`。
   
   最终，这个乘法运算实际上是三个 `shape` 均为 `(N, H2, W2, C)` 的数组在进行元素级别的相乘。这套广播机制同样适用于第 25-27 行的所有其他乘法运算。




## 矩阵转置算法 `reshape.cpp`

```cpp 
void reshape_block(uint8_t* matrix, int rows, int colsb, int cur_row, int start_col, uint8_t* matrix_o) {
    int trans_rows = start_col / 4;
    int new_col = 4 * rows;
    int new_start_col = cur_row * 4;
    __mmask64 mask = 0xFFFFFFFFFFFFFFFFULL;
    __m512i r0 = _mm512_loadu_epi8(matrix + start_col + 0 * colsb);
    __m512i r1 = cur_row + 1 < rows ? _mm512_loadu_epi8(matrix + start_col + 1 * colsb) : _mm512_setzero_si512();
    __m512i r2 = cur_row + 2 < rows ? _mm512_loadu_epi8(matrix + start_col + 2 * colsb) : _mm512_setzero_si512();
    __m512i r3 = cur_row + 3 < rows ? _mm512_loadu_epi8(matrix + start_col + 3 * colsb) : _mm512_setzero_si512();
    __m512i r4 = cur_row + 4 < rows ? _mm512_loadu_epi8(matrix + start_col + 4 * colsb) : _mm512_setzero_si512();
    __m512i r5 = cur_row + 5 < rows ? _mm512_loadu_epi8(matrix + start_col + 5 * colsb) : _mm512_setzero_si512();
    __m512i r6 = cur_row + 6 < rows ? _mm512_loadu_epi8(matrix + start_col + 6 * colsb) : _mm512_setzero_si512();
    __m512i r7 = cur_row + 7 < rows ? _mm512_loadu_epi8(matrix + start_col + 7 * colsb) : _mm512_setzero_si512();
    __m512i r8 = cur_row + 8 < rows ? _mm512_loadu_epi8(matrix + start_col + 8 * colsb) : _mm512_setzero_si512();
    __m512i r9 = cur_row + 9 < rows ? _mm512_loadu_epi8(matrix + start_col + 9 * colsb) : _mm512_setzero_si512();
    __m512i ra = cur_row + 10 < rows ? _mm512_loadu_epi8(matrix + start_col + 10 * colsb) : _mm512_setzero_si512();
    __m512i rb = cur_row + 11 < rows ? _mm512_loadu_epi8(matrix + start_col + 11 * colsb) : _mm512_setzero_si512();
    __m512i rc = cur_row + 12 < rows ? _mm512_loadu_epi8(matrix + start_col + 12 * colsb) : _mm512_setzero_si512();
    __m512i rd = cur_row + 13 < rows ? _mm512_loadu_epi8(matrix + start_col + 13 * colsb) : _mm512_setzero_si512();
    __m512i re = cur_row + 14 < rows ? _mm512_loadu_epi8(matrix + start_col + 14 * colsb) : _mm512_setzero_si512();
    __m512i rf = cur_row + 15 < rows ? _mm512_loadu_epi8(matrix + start_col + 15 * colsb) : _mm512_setzero_si512();

    __m512i t0, t1, t2, t3, t4, t5, t6, t7, t8, t9, ta, tb, tc, td, te, tf;

    t0 = _mm512_unpacklo_epi32(r0, r1);
    t1 = _mm512_unpackhi_epi32(r0, r1);
    t2 = _mm512_unpacklo_epi32(r2, r3);
    t3 = _mm512_unpackhi_epi32(r2, r3);
    t4 = _mm512_unpacklo_epi32(r4, r5);
    t5 = _mm512_unpackhi_epi32(r4, r5);
    t6 = _mm512_unpacklo_epi32(r6, r7);
    t7 = _mm512_unpackhi_epi32(r6, r7);
    t8 = _mm512_unpacklo_epi32(r8, r9);
    t9 = _mm512_unpackhi_epi32(r8, r9);
    ta = _mm512_unpacklo_epi32(ra, rb);
    tb = _mm512_unpackhi_epi32(ra, rb);
    tc = _mm512_unpacklo_epi32(rc, rd);
    td = _mm512_unpackhi_epi32(rc, rd);
    te = _mm512_unpacklo_epi32(re, rf);
    tf = _mm512_unpackhi_epi32(re, rf);

    r0 = _mm512_unpacklo_epi64(t0, t2);
    r1 = _mm512_unpackhi_epi64(t0, t2);
    r2 = _mm512_unpacklo_epi64(t1, t3);
    r3 = _mm512_unpackhi_epi64(t1, t3);
    r4 = _mm512_unpacklo_epi64(t4, t6);
    r5 = _mm512_unpackhi_epi64(t4, t6);
    r6 = _mm512_unpacklo_epi64(t5, t7);
    r7 = _mm512_unpackhi_epi64(t5, t7);
    r8 = _mm512_unpacklo_epi64(t8, ta);
    r9 = _mm512_unpackhi_epi64(t8, ta);
    ra = _mm512_unpacklo_epi64(t9, tb);
    rb = _mm512_unpackhi_epi64(t9, tb);
    rc = _mm512_unpacklo_epi64(tc, te);
    rd = _mm512_unpackhi_epi64(tc, te);
    re = _mm512_unpacklo_epi64(td, tf);
    rf = _mm512_unpackhi_epi64(td, tf);

    t0 = _mm512_shuffle_i32x4(r0, r4, 0x88);
    t1 = _mm512_shuffle_i32x4(r1, r5, 0x88);
    t2 = _mm512_shuffle_i32x4(r2, r6, 0x88);
    t3 = _mm512_shuffle_i32x4(r3, r7, 0x88);
    t4 = _mm512_shuffle_i32x4(r0, r4, 0xdd);
    t5 = _mm512_shuffle_i32x4(r1, r5, 0xdd);
    t6 = _mm512_shuffle_i32x4(r2, r6, 0xdd);
    t7 = _mm512_shuffle_i32x4(r3, r7, 0xdd);
    t8 = _mm512_shuffle_i32x4(r8, rc, 0x88);
    t9 = _mm512_shuffle_i32x4(r9, rd, 0x88);
    ta = _mm512_shuffle_i32x4(ra, re, 0x88);
    tb = _mm512_shuffle_i32x4(rb, rf, 0x88);
    tc = _mm512_shuffle_i32x4(r8, rc, 0xdd);
    td = _mm512_shuffle_i32x4(r9, rd, 0xdd);
    te = _mm512_shuffle_i32x4(ra, re, 0xdd);
    tf = _mm512_shuffle_i32x4(rb, rf, 0xdd);

    r0 = _mm512_shuffle_i32x4(t0, t8, 0x88);
    r1 = _mm512_shuffle_i32x4(t1, t9, 0x88);
    r2 = _mm512_shuffle_i32x4(t2, ta, 0x88);
    r3 = _mm512_shuffle_i32x4(t3, tb, 0x88);
    r4 = _mm512_shuffle_i32x4(t4, tc, 0x88);
    r5 = _mm512_shuffle_i32x4(t5, td, 0x88);
    r6 = _mm512_shuffle_i32x4(t6, te, 0x88);
    r7 = _mm512_shuffle_i32x4(t7, tf, 0x88);
    r8 = _mm512_shuffle_i32x4(t0, t8, 0xdd);
    r9 = _mm512_shuffle_i32x4(t1, t9, 0xdd);
    ra = _mm512_shuffle_i32x4(t2, ta, 0xdd);
    rb = _mm512_shuffle_i32x4(t3, tb, 0xdd);
    rc = _mm512_shuffle_i32x4(t4, tc, 0xdd);
    rd = _mm512_shuffle_i32x4(t5, td, 0xdd);
    re = _mm512_shuffle_i32x4(t6, te, 0xdd);
    rf = _mm512_shuffle_i32x4(t7, tf, 0xdd);

    _mm512_storeu_epi64(matrix_o + (trans_rows) * new_col + new_start_col, r0);
    _mm512_storeu_epi64(matrix_o + (trans_rows + 1) * new_col + new_start_col, r1);
    _mm512_storeu_epi64(matrix_o + (trans_rows + 2) * new_col + new_start_col, r2);
    _mm512_storeu_epi64(matrix_o + (trans_rows + 3) * new_col + new_start_col, r3);
    _mm512_storeu_epi64(matrix_o + (trans_rows + 4) * new_col + new_start_col, r4);
    _mm512_storeu_epi64(matrix_o + (trans_rows + 5) * new_col + new_start_col, r5);
    _mm512_storeu_epi64(matrix_o + (trans_rows + 6) * new_col + new_start_col, r6);
    _mm512_storeu_epi64(matrix_o + (trans_rows + 7) * new_col + new_start_col, r7);
    _mm512_storeu_epi64(matrix_o + (trans_rows + 8) * new_col + new_start_col, r8);
    _mm512_storeu_epi64(matrix_o + (trans_rows + 9) * new_col + new_start_col, r9);
    _mm512_storeu_epi64(matrix_o + (trans_rows + 10) * new_col + new_start_col, ra);
    _mm512_storeu_epi64(matrix_o + (trans_rows + 11) * new_col + new_start_col, rb);
    _mm512_storeu_epi64(matrix_o + (trans_rows + 12) * new_col + new_start_col, rc);
    _mm512_storeu_epi64(matrix_o + (trans_rows + 13) * new_col + new_start_col, rd);
    _mm512_storeu_epi64(matrix_o + (trans_rows + 14) * new_col + new_start_col, re);
    _mm512_storeu_epi64(matrix_o + (trans_rows + 15) * new_col + new_start_col, rf);
}
```

该函数 `reshape_block` 使用 Intel AVX-512 指令集实现高效的以 32-bit 为单位进行的矩阵转置。以下是详细分析：


### 加载输入数据

```cpp
__m512i r0 = _mm512_loadu_epi8(matrix + start_col + 0 * colsb);
__m512i r1 = cur_row + 1 < rows ? _mm512_loadu_epi8(matrix + start_col + 1 * colsb) : _mm512_setzero_si512();
__m512i r2 = cur_row + 2 < rows ? _mm512_loadu_epi8(matrix + start_col + 2 * colsb) : _mm512_setzero_si512();
__m512i r3 = cur_row + 3 < rows ? _mm512_loadu_epi8(matrix + start_col + 3 * colsb) : _mm512_setzero_si512();
__m512i r4 = cur_row + 4 < rows ? _mm512_loadu_epi8(matrix + start_col + 4 * colsb) : _mm512_setzero_si512();
__m512i r5 = cur_row + 5 < rows ? _mm512_loadu_epi8(matrix + start_col + 5 * colsb) : _mm512_setzero_si512();
__m512i r6 = cur_row + 6 < rows ? _mm512_loadu_epi8(matrix + start_col + 6 * colsb) : _mm512_setzero_si512();
__m512i r7 = cur_row + 7 < rows ? _mm512_loadu_epi8(matrix + start_col + 7 * colsb) : _mm512_setzero_si512();
__m512i r8 = cur_row + 8 < rows ? _mm512_loadu_epi8(matrix + start_col + 8 * colsb) : _mm512_setzero_si512();
__m512i r9 = cur_row + 9 < rows ? _mm512_loadu_epi8(matrix + start_col + 9 * colsb) : _mm512_setzero_si512();
__m512i ra = cur_row + 10 < rows ? _mm512_loadu_epi8(matrix + start_col + 10 * colsb) : _mm512_setzero_si512();
__m512i rb = cur_row + 11 < rows ? _mm512_loadu_epi8(matrix + start_col + 11 * colsb) : _mm512_setzero_si512();
__m512i rc = cur_row + 12 < rows ? _mm512_loadu_epi8(matrix + start_col + 12 * colsb) : _mm512_setzero_si512();
__m512i rd = cur_row + 13 < rows ? _mm512_loadu_epi8(matrix + start_col + 13 * colsb) : _mm512_setzero_si512();
__m512i re = cur_row + 14 < rows ? _mm512_loadu_epi8(matrix + start_col + 14 * colsb) : _mm512_setzero_si512();
__m512i rf = cur_row + 15 < rows ? _mm512_loadu_epi8(matrix + start_col + 15 * colsb) : _mm512_setzero_si512();
```

- [_mm512_loadu_epi8](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm512_loadu_epi8&ig_expand=4082)  

  `__m512i _mm512_loadu_epi8 (void const* mem_addr)`

  从内存中（即参数`mem_addr`）加载 512 位（由 64 个打包的 8 位整数组成）到 `dst` 中。`mem_addr` 不需要对齐到任何特定边界。

- [_mm512_setzero_si512](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm512_setzero_si512&ig_expand=5871)  

  `__m512i _mm512_setzero_si512 ()`

  返回一个类型为 `__m512i` 的向量，其中所有元素都被设置为零。

从输入矩阵加载连续的 16 行数据到`r0`~`rf`中。若行数不足（`cur_row + i >= rows`），用 0 填充寄存器避免条件分支。

### 转置操作

1. 32位Unpack

    - [_mm512_unpacklo_epi32](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm512_unpacklo_epi32&ig_expand=7018)  

       `__m512i _mm512_unpacklo_epi32 (__m512i a, __m512i b)`

       将 `a` 和 `b` 中的低 32 位元素解包（unpack）并交错（interleave）到一个新的 512 位向量中。

    - [__mm512_unpackhi_epi32](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm512_unpackhi_epi32&ig_expand=7019)  

       `__m512i _mm512_unpackhi_epi32 (__m512i a, __m512i b)`

       将 `a` 和 `b` 中的高 32 位元素解包并交错到一个新的 512 位向量中。

    ```plain
    DEFINE INTERLEAVE_DWORDS(src1[127:0], src2[127:0]) {
    	dst[31:0] := src1[31:0] 
    	dst[63:32] := src2[31:0] 
    	dst[95:64] := src1[63:32] 
    	dst[127:96] := src2[63:32] 
    	RETURN dst[127:0]	
    }
    ```


2. 64位Unpack

   - [_mm512_unpacklo_epi64](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm512_unpacklo_epi64&ig_expand=7027)  

     `__m512i _mm512_unpacklo_epi64 (__m512i a, __m512i b)`

     将 `a` 和 `b` 中的低 64 位元素解包并交错到一个新的 512 位向量中。

   - [_mm512_unpackhi_epi64](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm512_unpackhi_epi64&ig_expand=6970)  

      `__m512i _mm512_unpackhi_epi64 (__m512i a, __m512i b)`

      将 `a` 和 `b` 中的高 64 位元素解包并交错到一个新的 512 位向量中。


3. 128位Shuffle

    - [_mm512_shuffle_i32x4](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm512_shuffle_i32x4&ig_expand=7020)  
    
      `__m512i _mm512_shuffle_i32x4 (__m512i a, __m512i b, const int imm)`
    
      根据 `imm` 参数指定的掩码对 `a` 和 `b` 中的 128 位元素进行洗牌（shuffle）。

        - `0x88`：取两个寄存器的低 128 位（`[a_low, b_low]`）。
        - `0xdd`：取两个寄存器的高 128 位（`[a_high, b_high]`）。

通过这样的过程，最终实现了将 16 行 64 列的矩阵block转置为 64 行 16 列的矩阵。

### 存储转置结果

```cpp
_mm512_storeu_epi64(matrix_o + (trans_rows) * new_col + new_start_col, r0);
_mm512_storeu_epi64(matrix_o + (trans_rows + 1) * new_col + new_start_col, r1);
_mm512_storeu_epi64(matrix_o + (trans_rows + 2) * new_col + new_start_col, r2);
_mm512_storeu_epi64(matrix_o + (trans_rows + 3) * new_col + new_start_col, r3);
_mm512_storeu_epi64(matrix_o + (trans_rows + 4) * new_col + new_start_col, r4);
_mm512_storeu_epi64(matrix_o + (trans_rows + 5) * new_col + new_start_col, r5);
_mm512_storeu_epi64(matrix_o + (trans_rows + 6) * new_col + new_start_col, r6);
_mm512_storeu_epi64(matrix_o + (trans_rows + 7) * new_col + new_start_col, r7);
_mm512_storeu_epi64(matrix_o + (trans_rows + 8) * new_col + new_start_col, r8);
_mm512_storeu_epi64(matrix_o + (trans_rows + 9) * new_col + new_start_col, r9);
_mm512_storeu_epi64(matrix_o + (trans_rows + 10) * new_col + new_start_col, ra);
_mm512_storeu_epi64(matrix_o + (trans_rows + 11) * new_col + new_start_col, rb);
_mm512_storeu_epi64(matrix_o + (trans_rows + 12) * new_col + new_start_col, rc);
_mm512_storeu_epi64(matrix_o + (trans_rows + 13) * new_col + new_start_col, rd);
_mm512_storeu_epi64(matrix_o + (trans_rows + 14) * new_col + new_start_col, re);
_mm512_storeu_epi64(matrix_o + (trans_rows + 15) * new_col + new_start_col, rf);
```

- [_mm512_storeu_epi64](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm512_storeu_epi64&ig_expand=6524)  

  `void _mm512_storeu_epi64 (void* mem_addr, __m512i a)`

  将 `a` 中的 64 位整数存储到内存地址 `mem_addr` 中。`mem_addr` 不需要对齐到任何特定边界。