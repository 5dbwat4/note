---
title: "Lab 4: Solver Challenge"
---

实验文档： https://hpc101.zjusct.io/lab/Lab4-Solver-Challenge

import Card from "@md-components/Card.vue"
import Asciinema from "@md-components/AsciinemaWrapper.vue"
import {NImage} from "naive-ui";

# 实验目标

本实验将利用OpenMP和MPI等并行优化技术，对BiCGSTAB迭代求解线性方程组的算法进行性能优化，提升其在大规模矩阵上的求解效率。

# 优化过程

## 基准代码

使用基准代码，编译运行，对于数据`case_2001.bin`，总计用时`427.23 s`

## Profile 性能分析（优化程序之前）

<Card title="该性能分析运行于M602节点上。" no-expansion>
</Card>

<Card title="顺便一提" type="thinking">
不想在本地装VTune，也不想用X11 Forwarding，还有别的轻量级方法吗？

有的有的

VTune原生支持WebUI，只需要先`ssh -L 0.0.0.0:8080:0.0.0.0:8080 hpc101`再`vtune-backend --web-port=8080 --data-directory=.`即可在`localhost:8080`享受等同于本地安装VTune的体验。

BTW，VTune是electron应用，所以这个WebUI真的可以说是媲美原生的体验，甚至还不需要手动处理report上传和下载
</Card>

### Screenshots

**Hotspots**

*Summary*

import Image1 from "./assets/lab4-prep-pg1.jpeg?url"

<NImage src={Image1} />


### 并在报告中展示……

耗时最长的函数：`gemv`

import Image3 from "./assets/lab4-prep-pg3.png?url"

<NImage src={Image3} />

程序运行的 Flame Graph：如下图

import Image2 from "./assets/lab4-prep-pg2.png?url"

<NImage src={Image2} />

## OpenMP 线程级并行

```diff
#include <math.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
+ #include <omp.h>  // Include OpenMP header

void gemv(double* __restrict y, double* __restrict A, double* __restrict x, int N) {
    // y = A * x
+   #pragma omp parallel for
    for (int i = 0; i < N; i++) {
        double temp = 0.0;
        for (int j = 0; j < N; j++) {
            temp += A[i * N + j] * x[j];
        }
        y[i] = temp;
    }
}

double dot_product(double* __restrict x, double* __restrict y, int N) {
    // dot product of x and y
    double result = 0.0;
+   #pragma omp parallel for reduction(+:result)
    for (int i = 0; i < N; i++) {
        result += x[i] * y[i];
    }
    return result;
}

void precondition(double* __restrict A, double* __restrict K2_inv, int N) {
    // K2_inv = 1 / diag(A)
+   #pragma omp parallel for
    for (int i = 0; i < N; i++) {
        K2_inv[i] = 1.0 / A[i * N + i];
    }
}

void precondition_apply(double* __restrict z, double* __restrict K2_inv, double* __restrict r, int N) {
    // z = K2_inv * r
+   #pragma omp parallel for
    for (int i = 0; i < N; i++) {
        z[i] = K2_inv[i] * r[i];
    }
}

int bicgstab(int N, double* A, double* b, double* x, int max_iter, double tol) {
    double* r      = (double*)calloc(N, sizeof(double));
    double* r_hat  = (double*)calloc(N, sizeof(double));
    double* p      = (double*)calloc(N, sizeof(double));
    double* v      = (double*)calloc(N, sizeof(double));
    double* s      = (double*)calloc(N, sizeof(double));
    double* h      = (double*)calloc(N, sizeof(double));
    double* t      = (double*)calloc(N, sizeof(double));
    double* y      = (double*)calloc(N, sizeof(double));
    double* z      = (double*)calloc(N, sizeof(double));
    double* K2_inv = (double*)calloc(N, sizeof(double));

    double rho_old = 1, alpha = 1, omega = 1;
    double rho = 1, beta = 1;
    double tol_squared = tol * tol;

    precondition(A, K2_inv, N);

    gemv(r, A, x, N);
+   #pragma omp parallel for
    for (int i = 0; i < N; i++) {
        r[i] = b[i] - r[i];
    }

+   #pragma omp parallel for
    for (int i = 0; i < N; i++) {
        r_hat[i] = r[i];
    }

    rho = dot_product(r_hat, r, N);

+   #pragma omp parallel for
    for (int i = 0; i < N; i++) {
        p[i] = r[i];
    }

    int iter;
    for (iter = 1; iter <= max_iter; iter++) {
        if (iter % 1000 == 0) {
            printf("Iteration %d, residul = %e\n", iter, sqrt(dot_product(r, r, N)));
        }

        precondition_apply(y, K2_inv, p, N);
        gemv(v, A, y, N);
        alpha = rho / dot_product(r_hat, v, N);

+       #pragma omp parallel for
        for (int i = 0; i < N; i++) {
            h[i] = x[i] + alpha * y[i];
        }

+       #pragma omp parallel for
        for (int i = 0; i < N; i++) {
            s[i] = r[i] - alpha * v[i];
        }

        if (dot_product(s, s, N) < tol_squared) {
+           #pragma omp parallel for
            for (int i = 0; i < N; i++) {
                x[i] = h[i];
            }
            break;
        }

        precondition_apply(z, K2_inv, s, N);
        gemv(t, A, z, N);
        omega = dot_product(t, s, N) / dot_product(t, t, N);

+       #pragma omp parallel for
        for (int i = 0; i < N; i++) {
            x[i] = h[i] + omega * z[i];
        }

+       #pragma omp parallel for
        for (int i = 0; i < N; i++) {
            r[i] = s[i] - omega * t[i];
        }

        if (dot_product(r, r, N) < tol_squared) break;

        rho_old = rho;
        rho = dot_product(r_hat, r, N);
        beta = (rho / rho_old) * (alpha / omega);

+       #pragma omp parallel for
        for (int i = 0; i < N; i++) {
            p[i] = r[i] + beta * (p[i] - omega * v[i]);
        }
    }

    free(r);
    free(r_hat);
    free(p);
    free(v);
    free(s);
    free(h);
    free(t);
    free(y);
    free(z);
    free(K2_inv);

    if (iter >= max_iter)
        return -1;
    else
        return iter;
}
```

直接在所有for循环前添加`#pragma omp parallel for`指令，以实现并行化。

特别的，对于
```c
#pragma omp parallel for reduction(+:result)
for (int i = 0; i < N; i++) {
    result += x[i] * y[i];
}
```

由于存在数据竞争，必须使用`reduction`子句来确保正确性。

除此以外，编译时需要添加`-fopenmp`选项。（懂不懂忘了加选项然后努力半天都是~200s的痛感）


import Cast1 from "./casts/lab4-ooamad8-a1.cast?raw"

<Asciinema cast={Cast1} />

<Card type="task" title="思考题">

在使用 OpenMP 线程级并行时，请在报告中记录使用的**线程数**以及**加速比**，思考: **加速比能随着线程数的增加而线性增长吗？**

</Card>

运行程序前通过环境变量 `OMP_NUM_THREADS` 来设置使用的线程数

例如

```bash
export OMP_NUM_THREADS=4
```

实际测试中，我们配置了`OMP_NUM_THREADS`分别为4,8,16,52,104，以下为结果：

| 线程数 | 运行时间（秒） | 加速比 |
|--------|----------------|--------|
| *baseline* |    427.23     |  1   |
|   4    |        63.1132       |   6.77    |
|   8    |       26.3015       |  16.24  |
|  16    |        18.7415       |   22.80  |
|  52    |        8.68367       |   49.20 |
|  104   |       12.0822       |   35.36 |

加速比能随着线程数的增加而线性增长吗？

**答案是：不能。**

import Image4 from "./assets/lab4-dp1.svg?url"

<NImage src={Image4} />

使用 Profiler 的结果进行分析，可以看出，随着线程数的增加，

  * 看到 OpenMP 运行时库 (`libgomp`) 相关的函数开销（用于线程同步和调度）在总运行时间中的占比显著增加，异军突起。
  * 随着线程数的增加，在并行化的 `for` 循环（如 `gemv`, `dot_product`）上花费的总时间（Wall Time）理论上会显著减少 ~~但实际上没有显著减少啊\流汗~~。
  * 但是，程序中串行部分所花费的时间基本保持不变。
  * 综合这些因素，总的加速比曲线会开始趋于平缓，甚至在核心数非常多时可能出现性能下降的情况。

### 探究调度方式和调度粒度对并行效果的影响

<Card type="task" title="任务">

探究调度方式和调度粒度对并行效果的影响

比如，从访存连续性的角度考虑，一个线程执行 for 循环的连续数次迭代可能会比跳跃执行同样次数的迭代要更高效，这与我们在课程中提到的“伪共享”问题是一致的。 可以使用 OpenMP 的 schedule 子句和 size 参数来调整调度方式和调度粒度，也可以手动分配任务。

</Card>


`schedule` 子句用于指定如何将循环的迭代次数分配给并行区域中的线程。这对于负载均衡和减少开销至关重要。

其基本语法为：`#pragma omp parallel for schedule(kind, [chunk_size])`

主要有三种 `kind`：

1.  **`static`**:

      * **工作方式**: 在循环开始前，迭代任务就被静态地分配给线程。
      * **`schedule(static)`**: 如果不指定 `chunk_size`，循环迭代会大致被均等地分成N块（N为线程数），每个线程分到一块。
      * **`schedule(static, chunk_size)`**: 迭代任务被划分为大小为 `chunk_size` 的连续块，然后以轮询(round-robin)的方式静态分配给线程。
      * **优点**: 调度开销最小，因为分配在运行时之前就已确定。
      * **缺点**: 如果每次迭代的计算量差异巨大，容易导致负载不均。
      * **适用场景**: 每次迭代计算量都相近的循环。

2.  **`dynamic`**:

      * **工作方式**: 任务是动态分配的。线程完成当前块后，会向运行时系统请求下一个块。
      * **`schedule(dynamic)`**: 默认 `chunk_size` 为 1。
      * **`schedule(dynamic, chunk_size)`**: 线程每次请求 `chunk_size` 个迭代任务。
      * **优点**: 天然支持负载均衡，非常适合迭代计算量不均的场景。
      * **缺点**: 调度开销较大，因为线程需要频繁地与主线程或运行时同步以获取新任务。
      * **适用场景**: 每次迭代计算量未知或差异很大的循环（例如，循环中包含 `if` 分支，导致路径不同）。

3.  **`guided`**:

      * **工作方式**: `dynamic` 的一种变体和优化。开始时分配较大的任务块，随着剩余任务的减少，分配的任务块也逐渐变小。
      * **优点**: 结合了 `static` 的低开销（初始大块）和 `dynamic` 的负载均衡（后期小块）的优点。
      * **缺点**: 仍然比 `static` 有更高的开销。
      * **适用场景**: 迭代计算量不均，但又希望减少 `dynamic` 频繁调度开销的场景。

由于 BiCGSTAB 的运算时间实在是有点过于玄学（波动太大（倒也正常）），总之我们把 BiCGSTAB 中对性能影响最大的`gemv`函数单独拿出来测试。（控制变量（确信）

```c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <omp.h>
#include <math.h>

typedef enum {
    STATIC,
    DYNAMIC,
    GUIDED
} schedule_type;

void gemv_scheduled(double* y, double* A, double* x, int N, schedule_type sched_type, int chunk_size) {
    switch (sched_type) {
        case STATIC:
            #pragma omp parallel for schedule(static, chunk_size)
            for (int i = 0; i < N; i++) {
                y[i] = 0.0;
                for (int j = 0; j < N; j++) {
                    y[i] += A[i * N + j] * x[j];
                }
            }
            break;
        case DYNAMIC:
            #pragma omp parallel for schedule(dynamic, chunk_size)
            for (int i = 0; i < N; i++) {
                y[i] = 0.0;
                for (int j = 0; j < N; j++) {
                    y[i] += A[i * N + j] * x[j];
                }
            }
            break;
        case GUIDED:
            #pragma omp parallel for schedule(guided, chunk_size)
            for (int i = 0; i < N; i++) {
                y[i] = 0.0;
                for (int j = 0; j < N; j++) {
                    y[i] += A[i * N + j] * x[j];
                }
            }
            break;
    }
}


int main() {
    const int N = 32768; 
    const int N_TRIALS = 10; // 多次运行取平均值

    // 分配内存
    double* A = (double*)malloc(N * N * sizeof(double));
    double* x = (double*)malloc(N * sizeof(double));
    double* y = (double*)malloc(N * sizeof(double));

    if (!A || !x || !y) {
        fprintf(stderr, "Memory allocation failed\n");
        return 1;
    }

    // 初始化数据
    for (int i = 0; i < N * N; i++) A[i] = (double)rand() / RAND_MAX;
    for (int i = 0; i < N; i++) x[i] = (double)rand() / RAND_MAX;

    int num_threads = omp_get_max_threads();
    printf("Matrix size N = %d, Using %d threads\n", N, num_threads);
    printf("=================================================================\n");
    printf("%-10s | %-12s | %-15s | %-10s\n", "Schedule", "Chunk Size", "Avg Time (ms)", "Note");
    printf("-----------------------------------------------------------------\n");

    // 定义要测试的调度策略和 chunk size
    schedule_type schedules[] = {STATIC, DYNAMIC, GUIDED};
    const char* schedule_names[] = {"static", "dynamic", "guided"};
    int chunk_sizes[] = {1, 4, 16, 32, 64, 128, 256, 512};

    for (int s = 0; s < 3; s++) {
        for (int c = 0; c < 8; c++) {
            int chunk = chunk_sizes[c];
            double total_time = 0.0;

            // 预热
            gemv_scheduled(y, A, x, N, schedules[s], chunk);

            for (int t = 0; t < N_TRIALS; t++) {
                double start_time = omp_get_wtime();
                gemv_scheduled(y, A, x, N, schedules[s], chunk);
                double end_time = omp_get_wtime();
                total_time += (end_time - start_time);
            }

            double avg_time_ms = (total_time / N_TRIALS) * 1000.0;
        }
        printf("-----------------------------------------------------------------\n");
    }

    free(A);
    free(x);
    free(y);

    return 0;
}
```

在 M602 节点上运行结果为：

```
Matrix size N = 32768, Using 104 threads
=================================================================
Schedule   | Chunk Size   | Avg Time (ms)   | Note
-----------------------------------------------------------------
static     | 1            | 85.1242         |
static     | 4            | 80.7229         |
static     | 16           | 78.7635         |
static     | 32           | 78.0869         |
static     | 64           | 75.2856         |
static     | 128          | 73.9751         |
static     | 256          | 76.6910         |
static     | 512          | 69.3671         |
-----------------------------------------------------------------
dynamic    | 1            | 81.8968         |
dynamic    | 4            | 68.8556         |
dynamic    | 16           | 63.6352         |
dynamic    | 32           | 59.4023         |
dynamic    | 64           | 57.0717         |
dynamic    | 128          | 58.0172         |
dynamic    | 256          | 67.0884         |
dynamic    | 512          | 65.6648         |
-----------------------------------------------------------------
guided     | 1            | 61.7485         |
guided     | 4            | 60.6535         |
guided     | 16           | 57.2916         |
guided     | 32           | 64.5815         |
guided     | 64           | 56.8399         |
guided     | 128          | 60.1174         |
guided     | 256          | 62.5511         |
guided     | 512          | 61.3749         |
-----------------------------------------------------------------
```

**分析：**

对于 `gemv`这种任务：

1.  **`static` 调度性能最好**: 对于 `gemv` 这种负载完全均衡的循环，`static` 调度的性能是最好的。因为它几乎没有运行时调度开销，所有任务分配在循环开始前一次性完成。

2.  **`dynamic` 调度性能最差**: `dynamic` 在这里表现最差，因为它带来了巨大的运行时开销。每个线程每完成一个（或一批）迭代，就要去请求新任务，这在负载均衡的情况下是完全没有必要的开销。

3.  **`guided` 调度居中**: `guided` 的性能介于两者之间。它比 `dynamic` 好，因为它初始分配的任务块较大，减少了调度次数。但它仍然不如 `static`，因为它依然存在不必要的动态调度开销。

4.  **`chunk_size` 的影响**:

      * 对于 `static`：
          * 当 `chunk_size` 很小（例如 1）时，性能可能不是最优的。虽然任务分配是静态的，但一个线程处理的迭代任务在内存上不连续（线程0处理i=0, 8, 16...），形成了实验文档中*从访存连续性的角度考虑，一个线程执行 `for` 循环的连续数次迭代可能会比跳跃执行同样次数的迭代要更高效，这与我们在课程中提到的“伪共享”问题是一致的*。
          * 随着 `chunk_size` 增大，性能会提升并趋于稳定。因为每个线程现在处理一个连续的内存块（例如线程0处理i=0到63），这最大化了缓存的利用率。
      * 对于 `dynamic` 和 `guided`：`chunk_size`较小时，增大 `chunk_size` 对性能有显著的积极影响。因为它减少了调度的次数，降低了线程在请求新任务时的开销。

### 这一步优化后的性能提升情况

## 编译优化

此处编译优化在*OpenMP 线程级并行*实现后进行。

此处优化在M600集群上运行。

测试使用的数据为：`data/case_2001.bin`

### 对比不同编译器编译出来的程序的运行时间

测试中的
- `g++`版本为：`g++ (Debian 12.2.0-14+deb12u1) 12.2.0`
- `icpx`版本为：`Intel(R) oneAPI DPC++/C++ Compiler 2025.0.4 (2025.0.4.20241205)`
- `clang++`版本为：`Debian clang version 14.0.6`

| 编译器  | 运行时间 |
| ----- | ------ |
| g++    | 10.3849 s |
| icpx   | 6.9736 s |
| clang++ | 20.5139 s |

**分析**：`icpx` 编译器的性能优于 `g++` 和 `clang++`，这可能与其对现代硬件的优化有关。`icpx` 在 SIMD 和多线程方面的优化使其在处理大规模并行计算时表现更佳。而 `clang++` 的性能最差，可能是由于其对某些优化的支持不如前两者。

### 对比不添加额外选项，与添加了指导优化的编译选项

将使用`icpx`和`icx`作为编译器。

| 编译选项  | 运行时间 | 迭代次数 | 最终结果的误差 |
| --- | --- | --- | --- |
| `-O0` | 33.2252 s | 22208 | 6.339233e-15 |
| `-Og` | 8.78027 s | 17424 | 9.172063e-15 |
| `-O2` | 7.78544 s | 20386 | 6.749136e-15 |
| `-O3` | 7.25592 s | 20386 | 6.749136e-15 |
| `-Ofast` | 5.95681 s | 20386 | 6.749136e-15 |
| `-Ofast -mavx512f -ffast-math` | 5.30115 s | 16848 | 5.164212e-16 |

运行时间方面，`-O0`&lt;`-Og`&lt;`-O2`&lt;`-O3`&lt;`-Ofast`。这是符合预期的，因为随着优化级别的提高，编译器会进行更多的代码优化，从而提高程序的运行效率。




### 优化结果

import Cast5 from "./casts/lab4-running-fuck2.cast?url"

<Asciinema url={Cast5} />

## MPI 进程级并行



## Profile 性能分析（优化程序之后）




# 提交文件说明

