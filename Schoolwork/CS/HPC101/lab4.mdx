---
title: "Lab 4: Solver Challenge"
---

实验文档： https://hpc101.zjusct.io/lab/Lab4-Solver-Challenge

import Card from "@md-components/Card.vue"
import Asciinema from "@md-components/AsciinemaWrapper.vue"
import {NImage} from "naive-ui";

# 实验目标

本实验将利用OpenMP和MPI等并行优化技术，对BiCGSTAB迭代求解线性方程组的算法进行性能优化，提升其在大规模矩阵上的求解效率。

# 优化过程

## 基准代码

使用基准代码，编译运行，对于数据`case_2001.bin`，总计用时`427.23 s`

## Profile 性能分析（优化程序之前）

<Card title="该性能分析运行于M602节点上。" no-expansion>
</Card>

<Card title="顺便一提" type="thinking">
不想在本地装VTune，也不想用X11 Forwarding，还有别的轻量级方法吗？

有的有的

VTune原生支持WebUI，只需要先`ssh -L 0.0.0.0:8080:0.0.0.0:8080 hpc101`再`vtune-backend --web-port=8080 --data-directory=.`即可在`localhost:8080`享受等同于本地安装VTune的体验。

BTW，VTune是electron应用，所以这个WebUI真的可以说是媲美原生的体验，甚至还不需要手动处理report上传和下载
</Card>

### Screenshots

**Hotspots**

*Summary*

import Image1 from "./assets/lab4-prep-pg1.jpeg?url"

<NImage src={Image1} />


### 并在报告中展示……

耗时最长的函数：`gemv`

import Image3 from "./assets/lab4-prep-pg3.png?url"

<NImage src={Image3} />

程序运行的 Flame Graph：如下图

import Image2 from "./assets/lab4-prep-pg2.png?url"

<NImage src={Image2} />

## OpenMP 线程级并行

```diff
#include <math.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
+ #include <omp.h>  // Include OpenMP header

void gemv(double* __restrict y, double* __restrict A, double* __restrict x, int N) {
    // y = A * x
+   #pragma omp parallel for
    for (int i = 0; i < N; i++) {
        double temp = 0.0;
        for (int j = 0; j < N; j++) {
            temp += A[i * N + j] * x[j];
        }
        y[i] = temp;
    }
}

double dot_product(double* __restrict x, double* __restrict y, int N) {
    // dot product of x and y
    double result = 0.0;
+   #pragma omp parallel for reduction(+:result)
    for (int i = 0; i < N; i++) {
        result += x[i] * y[i];
    }
    return result;
}

void precondition(double* __restrict A, double* __restrict K2_inv, int N) {
    // K2_inv = 1 / diag(A)
+   #pragma omp parallel for
    for (int i = 0; i < N; i++) {
        K2_inv[i] = 1.0 / A[i * N + i];
    }
}

void precondition_apply(double* __restrict z, double* __restrict K2_inv, double* __restrict r, int N) {
    // z = K2_inv * r
+   #pragma omp parallel for
    for (int i = 0; i < N; i++) {
        z[i] = K2_inv[i] * r[i];
    }
}

int bicgstab(int N, double* A, double* b, double* x, int max_iter, double tol) {
    double* r      = (double*)calloc(N, sizeof(double));
    double* r_hat  = (double*)calloc(N, sizeof(double));
    double* p      = (double*)calloc(N, sizeof(double));
    double* v      = (double*)calloc(N, sizeof(double));
    double* s      = (double*)calloc(N, sizeof(double));
    double* h      = (double*)calloc(N, sizeof(double));
    double* t      = (double*)calloc(N, sizeof(double));
    double* y      = (double*)calloc(N, sizeof(double));
    double* z      = (double*)calloc(N, sizeof(double));
    double* K2_inv = (double*)calloc(N, sizeof(double));

    double rho_old = 1, alpha = 1, omega = 1;
    double rho = 1, beta = 1;
    double tol_squared = tol * tol;

    precondition(A, K2_inv, N);

    gemv(r, A, x, N);
+   #pragma omp parallel for
    for (int i = 0; i < N; i++) {
        r[i] = b[i] - r[i];
    }

+   #pragma omp parallel for
    for (int i = 0; i < N; i++) {
        r_hat[i] = r[i];
    }

    rho = dot_product(r_hat, r, N);

+   #pragma omp parallel for
    for (int i = 0; i < N; i++) {
        p[i] = r[i];
    }

    int iter;
    for (iter = 1; iter <= max_iter; iter++) {
        if (iter % 1000 == 0) {
            printf("Iteration %d, residul = %e\n", iter, sqrt(dot_product(r, r, N)));
        }

        precondition_apply(y, K2_inv, p, N);
        gemv(v, A, y, N);
        alpha = rho / dot_product(r_hat, v, N);

+       #pragma omp parallel for
        for (int i = 0; i < N; i++) {
            h[i] = x[i] + alpha * y[i];
        }

+       #pragma omp parallel for
        for (int i = 0; i < N; i++) {
            s[i] = r[i] - alpha * v[i];
        }

        if (dot_product(s, s, N) < tol_squared) {
+           #pragma omp parallel for
            for (int i = 0; i < N; i++) {
                x[i] = h[i];
            }
            break;
        }

        precondition_apply(z, K2_inv, s, N);
        gemv(t, A, z, N);
        omega = dot_product(t, s, N) / dot_product(t, t, N);

+       #pragma omp parallel for
        for (int i = 0; i < N; i++) {
            x[i] = h[i] + omega * z[i];
        }

+       #pragma omp parallel for
        for (int i = 0; i < N; i++) {
            r[i] = s[i] - omega * t[i];
        }

        if (dot_product(r, r, N) < tol_squared) break;

        rho_old = rho;
        rho = dot_product(r_hat, r, N);
        beta = (rho / rho_old) * (alpha / omega);

+       #pragma omp parallel for
        for (int i = 0; i < N; i++) {
            p[i] = r[i] + beta * (p[i] - omega * v[i]);
        }
    }

    free(r);
    free(r_hat);
    free(p);
    free(v);
    free(s);
    free(h);
    free(t);
    free(y);
    free(z);
    free(K2_inv);

    if (iter >= max_iter)
        return -1;
    else
        return iter;
}
```

直接在所有for循环前添加`#pragma omp parallel for`指令，以实现并行化。

特别的，对于
```c
#pragma omp parallel for reduction(+:result)
for (int i = 0; i < N; i++) {
    result += x[i] * y[i];
}
```

由于存在数据竞争，必须使用`reduction`子句来确保正确性。

除此以外，编译时需要添加`-fopenmp`选项。（懂不懂忘了加选项然后努力半天都是~200s的痛感）


import Cast1 from "./casts/lab4-ooamad8-a1.cast?raw"

<Asciinema cast={Cast1} />

<Card type="task" title="思考题">

在使用 OpenMP 线程级并行时，请在报告中记录使用的**线程数**以及**加速比**，思考: **加速比能随着线程数的增加而线性增长吗？**

</Card>

运行程序前通过环境变量 `OMP_NUM_THREADS` 来设置使用的线程数

例如

```bash
export OMP_NUM_THREADS=4
```

实际测试中，我们配置了`OMP_NUM_THREADS`分别为4,8,16,52,104，以下为结果：

| 线程数 | 运行时间（秒） | 加速比 |
|--------|----------------|--------|
| *baseline* |    427.23     |  1   |
|   4    |        63.1132       |   6.77    |
|   8    |       26.3015       |  16.24  |
|  16    |        18.7415       |   22.80  |
|  52    |        8.68367       |   49.20 |
|  104   |       12.0822       |   35.36 |

加速比能随着线程数的增加而线性增长吗？

**答案是：不能。**

import Image4 from "./assets/lab4-dp1.svg?url"

<NImage src={Image4} />

使用 Profiler 的结果进行分析，可以看出，随着线程数的增加，

  * 看到 OpenMP 运行时库 (`libgomp`) 相关的函数开销（用于线程同步和调度）在总运行时间中的占比显著增加，异军突起。
  * 随着线程数的增加，在并行化的 `for` 循环（如 `gemv`, `dot_product`）上花费的总时间（Wall Time）理论上会显著减少 ~~但实际上没有显著减少啊\流汗~~。
  * 但是，程序中串行部分所花费的时间基本保持不变。
  * 综合这些因素，总的加速比曲线会开始趋于平缓，甚至在核心数非常多时可能出现性能下降的情况。

### 探究调度方式和调度粒度对并行效果的影响

<Card type="task" title="任务">

探究调度方式和调度粒度对并行效果的影响

比如，从访存连续性的角度考虑，一个线程执行 for 循环的连续数次迭代可能会比跳跃执行同样次数的迭代要更高效，这与我们在课程中提到的“伪共享”问题是一致的。 可以使用 OpenMP 的 schedule 子句和 size 参数来调整调度方式和调度粒度，也可以手动分配任务。

</Card>


`schedule` 子句用于指定如何将循环的迭代次数分配给并行区域中的线程。这对于负载均衡和减少开销至关重要。

其基本语法为：`#pragma omp parallel for schedule(kind, [chunk_size])`

主要有三种 `kind`：

1.  **`static`**:

      * **工作方式**: 在循环开始前，迭代任务就被静态地分配给线程。
      * **`schedule(static)`**: 如果不指定 `chunk_size`，循环迭代会大致被均等地分成N块（N为线程数），每个线程分到一块。
      * **`schedule(static, chunk_size)`**: 迭代任务被划分为大小为 `chunk_size` 的连续块，然后以轮询(round-robin)的方式静态分配给线程。
      * **优点**: 调度开销最小，因为分配在运行时之前就已确定。
      * **缺点**: 如果每次迭代的计算量差异巨大，容易导致负载不均。
      * **适用场景**: 每次迭代计算量都相近的循环。

2.  **`dynamic`**:

      * **工作方式**: 任务是动态分配的。线程完成当前块后，会向运行时系统请求下一个块。
      * **`schedule(dynamic)`**: 默认 `chunk_size` 为 1。
      * **`schedule(dynamic, chunk_size)`**: 线程每次请求 `chunk_size` 个迭代任务。
      * **优点**: 天然支持负载均衡，非常适合迭代计算量不均的场景。
      * **缺点**: 调度开销较大，因为线程需要频繁地与主线程或运行时同步以获取新任务。
      * **适用场景**: 每次迭代计算量未知或差异很大的循环（例如，循环中包含 `if` 分支，导致路径不同）。

3.  **`guided`**:

      * **工作方式**: `dynamic` 的一种变体和优化。开始时分配较大的任务块，随着剩余任务的减少，分配的任务块也逐渐变小。
      * **优点**: 结合了 `static` 的低开销（初始大块）和 `dynamic` 的负载均衡（后期小块）的优点。
      * **缺点**: 仍然比 `static` 有更高的开销。
      * **适用场景**: 迭代计算量不均，但又希望减少 `dynamic` 频繁调度开销的场景。

由于 BiCGSTAB 的运算时间实在是有点过于玄学（波动太大（倒也正常）），总之我们把 BiCGSTAB 中对性能影响最大的`gemv`函数单独拿出来测试。（控制变量（确信）

```c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <omp.h>
#include <math.h>

typedef enum {
    STATIC,
    DYNAMIC,
    GUIDED
} schedule_type;

void gemv_scheduled(double* y, double* A, double* x, int N, schedule_type sched_type, int chunk_size) {
    switch (sched_type) {
        case STATIC:
            #pragma omp parallel for schedule(static, chunk_size)
            for (int i = 0; i < N; i++) {
                y[i] = 0.0;
                for (int j = 0; j < N; j++) {
                    y[i] += A[i * N + j] * x[j];
                }
            }
            break;
        case DYNAMIC:
            #pragma omp parallel for schedule(dynamic, chunk_size)
            for (int i = 0; i < N; i++) {
                y[i] = 0.0;
                for (int j = 0; j < N; j++) {
                    y[i] += A[i * N + j] * x[j];
                }
            }
            break;
        case GUIDED:
            #pragma omp parallel for schedule(guided, chunk_size)
            for (int i = 0; i < N; i++) {
                y[i] = 0.0;
                for (int j = 0; j < N; j++) {
                    y[i] += A[i * N + j] * x[j];
                }
            }
            break;
    }
}


int main() {
    const int N = 32768; 
    const int N_TRIALS = 10; // 多次运行取平均值

    // 分配内存
    double* A = (double*)malloc(N * N * sizeof(double));
    double* x = (double*)malloc(N * sizeof(double));
    double* y = (double*)malloc(N * sizeof(double));

    if (!A || !x || !y) {
        fprintf(stderr, "Memory allocation failed\n");
        return 1;
    }

    // 初始化数据
    for (int i = 0; i < N * N; i++) A[i] = (double)rand() / RAND_MAX;
    for (int i = 0; i < N; i++) x[i] = (double)rand() / RAND_MAX;

    int num_threads = omp_get_max_threads();
    printf("Matrix size N = %d, Using %d threads\n", N, num_threads);
    printf("=================================================================\n");
    printf("%-10s | %-12s | %-15s | %-10s\n", "Schedule", "Chunk Size", "Avg Time (ms)", "Note");
    printf("-----------------------------------------------------------------\n");

    // 定义要测试的调度策略和 chunk size
    schedule_type schedules[] = {STATIC, DYNAMIC, GUIDED};
    const char* schedule_names[] = {"static", "dynamic", "guided"};
    int chunk_sizes[] = {1, 4, 16, 32, 64, 128, 256, 512};

    for (int s = 0; s < 3; s++) {
        for (int c = 0; c < 8; c++) {
            int chunk = chunk_sizes[c];
            double total_time = 0.0;

            // 预热
            gemv_scheduled(y, A, x, N, schedules[s], chunk);

            for (int t = 0; t < N_TRIALS; t++) {
                double start_time = omp_get_wtime();
                gemv_scheduled(y, A, x, N, schedules[s], chunk);
                double end_time = omp_get_wtime();
                total_time += (end_time - start_time);
            }

            double avg_time_ms = (total_time / N_TRIALS) * 1000.0;
        }
        printf("-----------------------------------------------------------------\n");
    }

    free(A);
    free(x);
    free(y);

    return 0;
}
```

在 M602 节点上运行结果为：

```
Matrix size N = 32768, Using 104 threads
=================================================================
Schedule   | Chunk Size   | Avg Time (ms)   | Note
-----------------------------------------------------------------
static     | 1            | 85.1242         |
static     | 4            | 80.7229         |
static     | 16           | 78.7635         |
static     | 32           | 78.0869         |
static     | 64           | 75.2856         |
static     | 128          | 73.9751         |
static     | 256          | 76.6910         |
static     | 512          | 69.3671         |
-----------------------------------------------------------------
dynamic    | 1            | 81.8968         |
dynamic    | 4            | 68.8556         |
dynamic    | 16           | 63.6352         |
dynamic    | 32           | 59.4023         |
dynamic    | 64           | 57.0717         |
dynamic    | 128          | 58.0172         |
dynamic    | 256          | 67.0884         |
dynamic    | 512          | 65.6648         |
-----------------------------------------------------------------
guided     | 1            | 61.7485         |
guided     | 4            | 60.6535         |
guided     | 16           | 57.2916         |
guided     | 32           | 64.5815         |
guided     | 64           | 56.8399         |
guided     | 128          | 60.1174         |
guided     | 256          | 62.5511         |
guided     | 512          | 61.3749         |
-----------------------------------------------------------------
```

**分析：**

对于 `gemv`这种任务：

1.  **`static` 调度性能最好**: 对于 `gemv` 这种负载完全均衡的循环，`static` 调度的性能是最好的。因为它几乎没有运行时调度开销，所有任务分配在循环开始前一次性完成。

2.  **`dynamic` 调度性能最差**: `dynamic` 在这里表现最差，因为它带来了巨大的运行时开销。每个线程每完成一个（或一批）迭代，就要去请求新任务，这在负载均衡的情况下是完全没有必要的开销。

3.  **`guided` 调度居中**: `guided` 的性能介于两者之间。它比 `dynamic` 好，因为它初始分配的任务块较大，减少了调度次数。但它仍然不如 `static`，因为它依然存在不必要的动态调度开销。

4.  **`chunk_size` 的影响**:

      * 对于 `static`：
          * 当 `chunk_size` 很小（例如 1）时，性能可能不是最优的。虽然任务分配是静态的，但一个线程处理的迭代任务在内存上不连续（线程0处理i=0, 8, 16...），形成了实验文档中*从访存连续性的角度考虑，一个线程执行 `for` 循环的连续数次迭代可能会比跳跃执行同样次数的迭代要更高效，这与我们在课程中提到的“伪共享”问题是一致的*。
          * 随着 `chunk_size` 增大，性能会提升并趋于稳定。因为每个线程现在处理一个连续的内存块（例如线程0处理i=0到63），这最大化了缓存的利用率。
      * 对于 `dynamic` 和 `guided`：`chunk_size`较小时，增大 `chunk_size` 对性能有显著的积极影响。因为它减少了调度的次数，降低了线程在请求新任务时的开销。


## 编译优化

此处编译优化在*OpenMP 线程级并行*实现后进行。

此处优化在M600集群上运行。

测试使用的数据为：`data/case_2001.bin`

### 对比不同编译器编译出来的程序的运行时间

测试中的
- `g++`版本为：`g++ (Debian 12.2.0-14+deb12u1) 12.2.0`
- `icpx`版本为：`Intel(R) oneAPI DPC++/C++ Compiler 2025.0.4 (2025.0.4.20241205)`
- `clang++`版本为：`Debian clang version 14.0.6`

| 编译器  | 运行时间 |
| ----- | ------ |
| g++    | 10.3849 s |
| icpx   | 6.9736 s |
| clang++ | 20.5139 s |

**分析**：`icpx` 编译器的性能优于 `g++` 和 `clang++`，这可能与其对现代硬件的优化有关。`icpx` 在 SIMD 和多线程方面的优化使其在处理大规模并行计算时表现更佳。而 `clang++` 的性能最差，可能是由于其对某些优化的支持不如前两者。

### 对比不添加额外选项，与添加了指导优化的编译选项

将使用`icpx`和`icx`作为编译器。

| 编译选项  | 运行时间 | 迭代次数 | 最终结果的误差 |
| --- | --- | --- | --- |
| `-O0` | 33.2252 s | 22208 | 6.339233e-15 |
| `-Og` | 8.78027 s | 17424 | 9.172063e-15 |
| `-O2` | 7.78544 s | 20386 | 6.749136e-15 |
| `-O3` | 7.25592 s | 20386 | 6.749136e-15 |
| `-Ofast` | 5.95681 s | 20386 | 6.749136e-15 |
| `-Ofast -mavx512f -ffast-math` | 5.30115 s | 16848 | 5.164212e-16 |

运行时间方面，`-O0`&lt;`-Og`&lt;`-O2`&lt;`-O3`&lt;`-Ofast`。这是符合预期的，因为随着优化级别的提高，编译器会进行更多的代码优化，从而提高程序的运行效率。




### 优化结果

import Cast5 from "./casts/lab4-running-fuck2.cast?url"

<Asciinema url={Cast5} />

## MPI 进程级并行

### ……你主要需要明确下面这三个问题，并进行合理实现

1. **任务划分：分析计算任务的特征，将大的计算任务拆分成多个互不依赖的子任务，并分配给不同的进程进行计算。**

    `BiCGSTAB` 算法的计算瓶颈主要在于大规模的矩阵-向量乘法 (GEMV) 和向量点积 (Dot Product)。这些操作都具有良好的数据并行特性。参考实验文档，该实现中使用矩阵分块的形式进行任务划分，具体是按行分块。

    我们将 $N \times N$ 的主矩阵 $A$ 在行维度上进行切分，将 $N$ 行数据尽可能均匀地分配给 $P$ 个 MPI 进程。每个进程获得一个大小为 $N_{\text{local}} \times N$ 的子矩阵 $A_{\text{local}}$，其中 $N_{\text{local}} \approx N/P$。相应地，所有 $N$ 维的向量（如解向量 $x$、残差 $r$ 等）也按照相同的逻辑进行划分，每个进程只负责计算和存储其对应 $A_{\text{local}}$ 的那部分向量分量。

    通过这种划分，原先对整个矩阵和向量的操作被分解成了多个在子矩阵和子向量上的、大部分情况下相互独立的计算子任务。例如，原先的 GEMV 操作 $y = Ax$ 就被分解为多个并行的 $y_{\text{local}} = A_{\text{local}} * x$ 子任务。

    **实现思路**

    -  在并行逻辑的开始，我们首先计算每个进程应分配的行数 (`counts`) 和在全局数据中的起始偏移量 (`displs`)。这里考虑了 `N` 无法被进程数 `size` 整除的边界情况，确保任务分配的均衡性。

        ```c
        int* counts = (int*)malloc(size * sizeof(int)); // 每个进程的行数
        int* displs = (int*)malloc(size * sizeof(int)); // 每个进程的行偏移
        int rem = N % size;
        int sum = 0;
        for (int i = 0; i < size; i++) {
            counts[i] = N / size;
            if (rem > 0) {
                counts[i]++;
                rem--;
            }
            displs[i] = sum;
            sum += counts[i];
        }
        int local_n = counts[rank]; // 当前进程的任务量
        ```

    -  根据计算出的 `local_n`，每个进程只为自己负责的子任务分配内存。例如 `A_local`、`r_local` 等。

        ```c
        double* A_local = (double*)malloc(local_n * N * sizeof(double));
        double* r_local = (double*)calloc(local_n, sizeof(double));
        double* r_hat_local = (double*)calloc(local_n, sizeof(double));
        double* p_local = (double*)calloc(local_n, sizeof(double));
        double* v_local = (double*)calloc(local_n, sizeof(double));
        double* s_local = (double*)calloc(local_n, sizeof(double));
        double* h_local = (double*)calloc(local_n, sizeof(double));
        double* t_local = (double*)calloc(local_n, sizeof(double));
        double* y_local = (double*)calloc(local_n, sizeof(double));
        double* z_local = (double*)calloc(local_n, sizeof(double));
        double* x_local = (double*)calloc(local_n, sizeof(double));
        double* K2_inv_local = (double*)calloc(local_n, sizeof(double));
        ```

2. **数据分发与结果收集：在并行计算开始前，需要把数据分发给每个进程。同理，完成计算后，需要将结果收集起来。**

   

    在非共享内存的 MPI 模型中，数据通信是并行化的生命线。我们主要采用了**集合通信 (Collective Communication)**，因为它能更高效地处理这种结构化的、全局性的数据同步需求。点对点通信虽然灵活，但在需要广播或全局规约的场景下，会导致主进程通信负载过重，可扩展性较差。
    
      * 主循环开始前的数据分发: 在计算开始前，主进程 (rank 0) 负责读取完整数据。随后：
          * 使用 `MPI_Bcast` 将所有进程都需要知道的全局信息（如矩阵维度 `N`）广播出去。
          * 使用 `MPI_Scatterv` 将按行划分好的矩阵 `A_local` 和向量 `x_local` 等分发给对应的进程。使用 `v` 后缀的版本（`Scatterv`）是为了灵活处理行数分配不均的情况。

              ```cpp
              // --- 在调用求解器前，将 N 广播给所有进程 ---
              MPI_Bcast(&N, 1, MPI_INT, 0, MPI_COMM_WORLD);
              ...
              // 分发矩阵 A
              MPI_Scatterv(A, mat_counts, mat_displs, MPI_DOUBLE, A_local, 
                           local_n * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);
              // 分发初始向量 x 到各自的 x_local
              MPI_Scatterv(x_full, counts, displs, MPI_DOUBLE, x_local, 
                           local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);
              ```

      * **迭代中通信**: 在 `BiCGSTAB` 的迭代过程中，也需要通信来同步数据：
          * `MPI_Allgatherv`: 在执行 GEMV (`v = Ay`) 之前，输入向量 `y` 必须是完整的。由于 `y` 是以 `y_local` 的形式分布在各个进程上的，我们使用 `MPI_Allgatherv` 将所有 `y_local` 片段收集起来，在**每个进程**上都重构出完整的 `y_full` 向量。
          * `MPI_Allreduce`: 在计算点积时，每个进程先计算一个局部和，然后通过 `MPI_Allreduce` 将所有局部和相加，并将最终的全局总和返回给**所有进程**。这使得每个进程都能独立计算下一步所需的标量（如 `alpha`, `omega`），避免了“主进程计算-再广播”的模式。

            ```cpp
            // 为 GEMV 准备完整的输入向量
            MPI_Allgatherv(y_local, local_n, MPI_DOUBLE, y_full, counts, 
                           displs, MPI_DOUBLE, MPI_COMM_WORLD);

            // 计算全局点积
            local_dot = 0.0;
            for (int i = 0; i < local_n; i++) 
                local_dot += r_hat_local[i] * v_local[i];
            MPI_Allreduce(&local_dot, &dot_rhat_v, 1, MPI_DOUBLE, 
                          MPI_SUM, MPI_COMM_WORLD);
            ```
      * **结果收集**: 计算结束后，使用 `MPI_Gatherv` 将分布在各进程的解向量片段 `x_local` 收集到主进程的完整向量 `x` 中，以便进行最终的验证和输出。

        ```cpp
        MPI_Gatherv(x_local, local_n, MPI_DOUBLE, x, counts, 
                    displs, MPI_DOUBLE, 0, MPI_COMM_WORLD);
        ```

3. **并行计算：你需要分析 BiCGSTAB 算法的迭代过程中，哪些部分可以充分并行计算，哪些部分必须串行计算。**

    `BiCGSTAB` 算法的大部分计算步骤都可以高效并行化，但也包含必须sync执行的部分。

    * **可并行部分**:  
        1.  `GEMV`: 如 `v = Ay`，在每个进程都拥有完整的向量 `y` 后，可以完全独立地计算 `v_local = A_local * y`。
        2.  向量线性组合: 如 `s = r - alpha * v`，这类操作是元素级别的，各进程可以在自己的 `s_local`, `r_local`, `v_local` 上独立完成，无需任何通信。
        3.  `precondition`: `y = K_inv * p` 同样是元素级别的操作，可在本地独立完成。

    * **必须串行计算部分**:  
        主要列出的是需要依赖所有线程计算结果的步骤，这些步骤需要全局同步才能继续。  
        1.  **标量计算**: `alpha`, `omega`, `beta`, `rho` 的计算依赖于全局点积的结果。例如，`alpha = rho / dot_product(r_hat, v)`，必须等待所有进程完成 `v_local` 的计算，并通过 `MPI_Allreduce` 得到全局点积后，才能进行除法运算。
        2.  **收敛判断**: 判断 `dot_product(s, s) < tol` 也需要一次全局点积的同步。

### 代码与优化结果

```c 
#include <math.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <mpi.h>

void gemv(double* y, double* A, double* x, int rows, int cols) {
#pragma omp parallel for
    for (int i = 0; i < rows; i++) {
        double sum = 0.0;
        for (int j = 0; j < cols; j++) {
            sum += A[i * cols + j] * x[j];
        }
        y[i] = sum;
    }
}

double dot_product(double* x, double* y, int n) {
    double result = 0.0;
#pragma omp parallel for reduction(+:result)
    for (int i = 0; i < n; i++) {
        result += x[i] * y[i];
    }
    return result;
}

void precondition(double* A, double* K2_inv, int rows, int cols, int start_row) {
#pragma omp parallel for
    for (int i = 0; i < rows; i++) {
        int global_row = start_row + i;
        K2_inv[i] = 1.0 / A[i * cols + global_row];
    }
}

void precondition_apply(double* z, double* K2_inv, double* r, int n) {
#pragma omp parallel for
    for (int i = 0; i < n; i++) {
        z[i] = K2_inv[i] * r[i];
    }
}

int bicgstab(int N, double* A, double* b, double* x, int max_iter, double tol) {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    if (size <= 1) {
        return bicgstab_serial(N, A, b, x, max_iter, tol);
    }

    int* counts = (int*)malloc(size * sizeof(int));
    int* displs = (int*)malloc(size * sizeof(int));
    int rem = N % size;
    int sum = 0;
    for (int i = 0; i < size; i++) {
        counts[i] = N / size;
        if (rem > 0) {
            counts[i]++;
            rem--;
        }
        displs[i] = sum;
        sum += counts[i];
    }
    int local_n = counts[rank];
    int start_row = displs[rank];

    double* A_local = (double*)malloc(local_n * N * sizeof(double));
    double* r_local = (double*)calloc(local_n, sizeof(double));
    double* r_hat_local = (double*)calloc(local_n, sizeof(double));
    double* p_local = (double*)calloc(local_n, sizeof(double));
    double* v_local = (double*)calloc(local_n, sizeof(double));
    double* s_local = (double*)calloc(local_n, sizeof(double));
    double* h_local = (double*)calloc(local_n, sizeof(double));
    double* t_local = (double*)calloc(local_n, sizeof(double));
    double* y_local = (double*)calloc(local_n, sizeof(double));
    double* z_local = (double*)calloc(local_n, sizeof(double));
    double* x_local = (double*)calloc(local_n, sizeof(double));
    double* K2_inv_local = (double*)calloc(local_n, sizeof(double));
    
    double* y_full = (double*)calloc(N, sizeof(double));
    double* z_full = (double*)calloc(N, sizeof(double));
    double* x_full = (double*)calloc(N, sizeof(double));
    double* b_full = (double*)malloc(N * sizeof(double));

    int* mat_counts = (int*)malloc(size * sizeof(int));
    int* mat_displs = (int*)malloc(size * sizeof(int));
    for (int i = 0; i < size; ++i) {
        mat_counts[i] = counts[i] * N;
        mat_displs[i] = displs[i] * N;
    }
    MPI_Scatterv(A, mat_counts, mat_displs, MPI_DOUBLE, A_local, 
                 local_n * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);
    free(mat_counts);
    free(mat_displs);

    precondition(A_local, K2_inv_local, local_n, N, start_row);

    if (rank == 0) {
        memcpy(x_full, x, N * sizeof(double));
        memcpy(b_full, b, N * sizeof(double));
    }
    MPI_Bcast(x_full, N, MPI_DOUBLE, 0, MPI_COMM_WORLD);
    MPI_Bcast(b_full, N, MPI_DOUBLE, 0, MPI_COMM_WORLD);
    MPI_Scatterv(x_full, counts, displs, MPI_DOUBLE, x_local, 
                 local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);

    double rho = 1.0, rho_old = 1.0, alpha = 1.0, omega = 1.0;
    double tol_squared = tol * tol;

    double* Ax_local = (double*)calloc(local_n, sizeof(double));
    gemv(Ax_local, A_local, x_full, local_n, N);
    for (int i = 0; i < local_n; i++) {
        r_local[i] = b_full[start_row + i] - Ax_local[i];
    }
    free(Ax_local);
    free(b_full);

    memcpy(r_hat_local, r_local, local_n * sizeof(double));
    
    double local_dot = dot_product(r_hat_local, r_local, local_n);
    MPI_Allreduce(&local_dot, &rho, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
    memcpy(p_local, r_local, local_n * sizeof(double));

    int iter;
    for (iter = 1; iter <= max_iter; iter++) {
        rho_old = rho;

        precondition_apply(y_local, K2_inv_local, p_local, local_n);

        MPI_Allgatherv(y_local, local_n, MPI_DOUBLE, y_full, 
                       counts, displs, MPI_DOUBLE, MPI_COMM_WORLD);
        gemv(v_local, A_local, y_full, local_n, N);

        local_dot = dot_product(r_hat_local, v_local, local_n);
        double dot_rhat_v;
        MPI_Allreduce(&local_dot, &dot_rhat_v, 1, MPI_DOUBLE, MPI_SUM, 
                      MPI_COMM_WORLD);
        alpha = rho / dot_rhat_v;

        for (int i = 0; i < local_n; i++) {
            h_local[i] = x_local[i] + alpha * y_local[i];
            s_local[i] = r_local[i] - alpha * v_local[i];
        }

        local_dot = dot_product(s_local, s_local, local_n);
        double s_norm_sq;
        MPI_Allreduce(&local_dot, &s_norm_sq, 1, 
                      MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
        if (s_norm_sq < tol_squared) {
            memcpy(x_local, h_local, local_n * sizeof(double));
            break;
        }

        precondition_apply(z_local, K2_inv_local, s_local, local_n);

        MPI_Allgatherv(z_local, local_n, MPI_DOUBLE, z_full, counts, 
                       displs, MPI_DOUBLE, MPI_COMM_WORLD);
        gemv(t_local, A_local, z_full, local_n, N);

        // omega = (t·s)/(t·t)
        double dot_t_s = dot_product(t_local, s_local, local_n);
        double dot_t_t = dot_product(t_local, t_local, local_n);
        double global_t_s, global_t_t;
        MPI_Allreduce(&dot_t_s, &global_t_s, 1, 
                      MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
        MPI_Allreduce(&dot_t_t, &global_t_t, 1, 
                      MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
        omega = global_t_s / global_t_t;

        // x = h + omega*z
        // r = s - omega*t
        for (int i = 0; i < local_n; i++) {
            x_local[i] = h_local[i] + omega * z_local[i];
            r_local[i] = s_local[i] - omega * t_local[i];
        }

        local_dot = dot_product(r_local, r_local, local_n);
        double r_norm_sq;
        MPI_Allreduce(&local_dot, &r_norm_sq, 1, MPI_DOUBLE, 
                      MPI_SUM, MPI_COMM_WORLD);
        if (r_norm_sq < tol_squared) break;

        local_dot = dot_product(r_hat_local, r_local, local_n);
        MPI_Allreduce(&local_dot, &rho, 1, 
                      MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
        
        // beta = (rho/rho_old)*(alpha/omega)
        double beta = (rho / rho_old) * (alpha / omega);

        // p = r + beta*(p - omega*v)
        for (int i = 0; i < local_n; i++) {
            p_local[i] = r_local[i] + beta * (p_local[i] - omega * v_local[i]);
        }
    }

    MPI_Gatherv(x_local, local_n, MPI_DOUBLE, x, counts, displs, 
                MPI_DOUBLE, 0, MPI_COMM_WORLD);

    free(counts); free(displs);
    free(A_local); free(r_local); free(r_hat_local); free(p_local); free(v_local);
    free(s_local); free(h_local); free(t_local); free(y_local); free(z_local);
    free(x_local); free(K2_inv_local); free(y_full); free(z_full); free(x_full);

    return (iter <= max_iter) ? iter : -1;
}
```


import CastMPI1 from "./casts/lab4-mpi111.cast?url"

<Asciinema url={CastMPI1} />


## Profile 性能分析（优化程序之后）

<Card title="环境说明" >
使用`mpirun -trace -n 4 ./build/bicgstab ./data/case_2001.bin`在M600节点上运行，使用的核数、线程数均少于在solver2分区上运行时的数量，因此性能相对于上一节有所下降。
</Card>

使用` traceanalyzer --cli ./bicgstab.stf --summary`得到：

```

>>>>> Welcome to the Intel(R) Trace Analyzer command line interface. <<<<

Application runtime [s]: 5.10609
Time for cache [s]: 0
Time for analysis [s]: 0.403
4:4
20.4087:7.25727:0
512:32768
MPI_Allreduce:4
5.40816:0:0
N/A:N/A:N/A
415164:0:0
3321312:0:0
MPI_Allgatherv:4
0:1.74278:0
N/A:N/A:N/A
0:138388:0
0:553828776:0
MPI_Scatterv:4
0.049829:6.9e-05:0.011874
N/A:N/A:N/A
9:2:1
0:32016:32032008
MPI_Bcast:4
0.042636:3e-05:0
N/A:N/A:N/A
10:2:0
4:32016:0
MPI_Finalize:1
0.001699:0:0
N/A:N/A:N/A
4:0:0
0:0:0
MPI_Gatherv:4
0:0.000177:0
N/A:N/A:N/A
0:4:0
0:16008:0
MPI_Get_processor_name:1
6e-06:0:0
N/A:N/A:N/A
4:0:0
0:0:0
MPI_Comm_size:1
5e-06:0:0
N/A:N/A:N/A
8:0:0
0:0:0
MPI_Comm_rank:1
4e-06:0:0
N/A:N/A:N/A
8:0:0
0:0:0
```


参考文档：[Intel® Trace Analyzer Command Line Interface Reference](https://www.intel.com/content/www/us/en/docs/trace-analyzer-collector/user-guide-reference/2021-10/traceanalyzer-command-line-interface.html)

> The application summary sheet consists of a three-line header:
> ```
>          <# processes>:<# processes per node>
> <application time>:<MPI time>:<IIS time>
> <first message size of middle bucket (2)>: \
> <first message size of highest bucket (3)>
> ```
> The header is followed by these sets of lines, for each of the top ten  functions, sorted by descending total time:
> 
> ```
>          <Name of MPI_group>:<# involved processes> 
>          
> <total time in above func for bucket 1>:<for bucket 2>:<for bucket 3>
> <total IIS time in above func for bucket 1>:<for bucket 2>:<for bucket 3>
> <count in above func for bucket 1>:<for bucket 2>:<for bucket 3>
> <total # bytes in above func for bucket 1>:<for bucket 2>:<for bucket 3>
> ```
> In the application summary sheet, IIS stands for Ideal Interconnect Simulator, which predicts MPI behavior on an ideal interconnect.
> 
> You can import the application summary sheet to spreadsheet applications such as Microsoft* Office Excel*. Fields are separated by colons. Unknown values are indicated by N/A. 

**耗时最多的三个 MPI 函数** (请区分 MPI 与 OpenMP)：
  - MPI_Allreduce 5.40816s
  - MPI_Allgatherv 1.74278s
  - MPI_Scatterv 0.061772s

**程序在 MPI 上消耗的总时间**：7.25727s